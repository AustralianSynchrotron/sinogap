{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Header</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread, imsave\n",
    "import h5py\n",
    "import tifffile\n",
    "import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "\n",
    "def plotData(dataY, rangeY=None, dataYR=None, rangeYR=None,\n",
    "             dataX=None, rangeX=None, rangeP=None,\n",
    "             figsize=(16,8), saveTo=None, show=True):\n",
    "\n",
    "    if type(dataY) is np.ndarray :\n",
    "        plotData((dataY,), rangeY=rangeY, dataYR=dataYR, rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataYR) is np.ndarray :\n",
    "        plotData(dataY, rangeY=rangeY, dataYR=(dataYR,), rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataY) is not tuple :\n",
    "        eprint(f\"Unknown data type to plot: {type(dataY)}.\")\n",
    "        return\n",
    "    if type(dataYR) is not tuple and dataYR is not None:\n",
    "        eprint(f\"Unknown data type to plot: {type(dataYR)}.\")\n",
    "        return\n",
    "\n",
    "    last = min( len(data) for data in dataY )\n",
    "    if dataYR is not None:\n",
    "        last = min( last,  min( len(data) for data in dataYR ) )\n",
    "    if dataX is not None:\n",
    "        last = min(last, len(dataX))\n",
    "    if rangeP is None :\n",
    "        rangeP = (0,last)\n",
    "    elif type(rangeP) is int :\n",
    "        rangeP = (0,rangeP) if rangeP > 0 else (-rangeP,last)\n",
    "    elif type(rangeP) is tuple :\n",
    "        rangeP = ( 0    if rangeP[0] is None else rangeP[0],\n",
    "                   last if rangeP[1] is None else rangeP[1],)\n",
    "    else :\n",
    "        eprint(f\"Bad data type on plotData input rangeP: {type(rangeP)}\")\n",
    "        raise Exception(f\"Bug in the code.\")\n",
    "    rangeP = np.s_[ max(0, rangeP[0]) : min(last, rangeP[1]) ]\n",
    "    if dataX is None :\n",
    "        dataX = np.arange(rangeP.start, rangeP.stop)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax1.xaxis.grid(True, 'both', linestyle='dotted')\n",
    "    if rangeX is not None :\n",
    "        ax1.set_xlim(rangeX)\n",
    "    else :\n",
    "        ax1.set_xlim(rangeP.start,rangeP.stop-1)\n",
    "\n",
    "    ax1.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "    nofPlots = len(dataY)\n",
    "    if rangeY is not None:\n",
    "        ax1.set_ylim(rangeY)\n",
    "    colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "    for idx , data in enumerate(dataY):\n",
    "        ax1.plot(dataX, data[rangeP], linestyle='-',  color=colors[idx])\n",
    "\n",
    "    if dataYR is not None : # right Y axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "        nofPlots = len(dataYR)\n",
    "        if rangeYR is not None:\n",
    "            ax2.set_ylim(rangeYR)\n",
    "        colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "        for idx , data in enumerate(dataYR):\n",
    "            ax2.plot(dataX, data[rangeP], linestyle='dashed',  color=colors[idx])\n",
    "\n",
    "    if saveTo:\n",
    "        fig.savefig(saveTo)\n",
    "    if not show:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def plotImage(image) :\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotImages(images) :\n",
    "    for i, img in enumerate(images) :\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sliceShape(shape, sl) :\n",
    "    if type(shape) is int :\n",
    "        shape = torch.Size([shape])\n",
    "    if type(sl) is tuple :\n",
    "        if len(shape) != len(sl) :\n",
    "            raise Exception(f\"Different sizes of shape {shape} and sl {sl}\")\n",
    "        out = []\n",
    "        for i in range(0, len(shape)) :\n",
    "            indeces = sl[i].indices(shape[i])\n",
    "            out.append(indeces[1]-indeces[0])\n",
    "        return out\n",
    "    elif type(sl) is slice :\n",
    "        indeces = sl.indices(shape[0])\n",
    "        return indeces[1]-indeces[0]\n",
    "    else :\n",
    "        raise Exception(f\"Incompatible object {sl}\")\n",
    "\n",
    "\n",
    "def tensorStat(stat) :\n",
    "    print(stat.mean().item(), stat.std().item(), stat.min().item(), stat.max().item())\n",
    "\n",
    "\n",
    "def fillWheights(seq) :\n",
    "    for wh in seq :\n",
    "        if hasattr(wh, 'weight') :\n",
    "            torch.nn.init.xavier_uniform_(wh.weight)\n",
    "\n",
    "\n",
    "def unsqeeze4dim(tens):\n",
    "    orgDims = tens.dim()\n",
    "    if tens.dim() == 2 :\n",
    "        tens = tens.unsqueeze(0)\n",
    "    if tens.dim() == 3 :\n",
    "        tens = tens.unsqueeze(1)\n",
    "    return tens, orgDims\n",
    "\n",
    "\n",
    "def squeezeOrg(tens, orgDims):\n",
    "    if orgDims == tens.dim():\n",
    "        return tens\n",
    "    if tens.dim() != 4 or orgDims > 4 or orgDims < 2:\n",
    "        raise Exception(f\"Unexpected dimensions to squeeze: {tens.dim()} {orgDims}.\")\n",
    "    if orgDims < 4 :\n",
    "        if tens.shape[1] > 1:\n",
    "            raise Exception(f\"Cant squeeze dimension 1 in: {tens.shape}.\")\n",
    "        tens = tens.squeeze(1)\n",
    "    if orgDims < 3 :\n",
    "        if tens.shape[0] > 1:\n",
    "            raise Exception(f\"Cant squeeze dimension 0 in: {tens.shape}.\")\n",
    "        tens = tens.squeeze(0)\n",
    "    return tens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Configs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED_VALUE):\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "\n",
    "seed = 7\n",
    "set_seed(seed)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TCfg:\n",
    "    exec = 0\n",
    "    device: torch.device = f\"cuda:{exec}\"\n",
    "    nofEpochs: int = 2048\n",
    "    latentDim: int = 64\n",
    "    batchSize: int = 16384\n",
    "    labelSmoothFac: float = 1.0 # For Real labels (or set to 1.0 for no smoothing).\n",
    "    learningRateD: float = 0.002\n",
    "    learningRateG: float = 0.002\n",
    "    historyHDF = f\"train_{exec}.hdf\"\n",
    "\n",
    "class DCfg:\n",
    "    gapW = 4\n",
    "    sinoSh = (5*gapW,5*gapW) # 20,20\n",
    "    readSh = (8*sinoSh[0], 8*sinoSh[0]) # 80,80\n",
    "    sinoSize = math.prod(sinoSh)\n",
    "    gapSh = (sinoSh[0],gapW)\n",
    "    gapSize = math.prod(gapSh)\n",
    "    gapRngX = np.s_[ sinoSh[1]//2 - gapW//2 : sinoSh[1]//2 + gapW//2 ]\n",
    "    gapRng = np.s_[ : , gapRngX ]\n",
    "    disRng = np.s_[ gapW:-gapW , gapRngX ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Save/Load model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_path):\n",
    "    if not device == 'cpu':\n",
    "        model.to('cpu')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    if not device == 'cpu':\n",
    "        model.to(device)\n",
    "    return\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def addToHDF(filename, containername, data) :\n",
    "    if len(data.shape) == 2 :\n",
    "        data=np.expand_dims(data, 0)\n",
    "    if len(data.shape) != 3 :\n",
    "        raise Exception(f\"Not appropriate input array size {data.shape}.\")\n",
    "\n",
    "    with h5py.File(filename,'a') as file :\n",
    "\n",
    "        if  containername not in file.keys():\n",
    "            dset = file.create_dataset(containername, data.shape,\n",
    "                                       maxshape=(None,data.shape[1],data.shape[2]),\n",
    "                                       dtype='f')\n",
    "            dset[()] = data\n",
    "            return\n",
    "\n",
    "        dset = file[containername]\n",
    "        csh = dset.shape\n",
    "        if csh[1] != data.shape[1] or csh[2] != data.shape[2] :\n",
    "            raise Exception(f\"Shape mismatch: input {data.shape}, file {dset.shape}.\")\n",
    "        msh = dset.maxshape\n",
    "        newLen = csh[0] + data.shape[0]\n",
    "        if msh[0] is None or msh[0] >= newLen :\n",
    "            dset.resize(newLen, axis=0)\n",
    "        else :\n",
    "            raise Exception(f\"Insufficient maximum shape {msh} to add data\"\n",
    "                            f\" {data.shape} to current volume {dset.shape}.\")\n",
    "        dset[csh[0]:newLen,...] = data\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Raw Read</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StripesFromHDF :\n",
    "\n",
    "    def __init__(self, sampleName, maskName, bgName=None, dfName=None, loadToMem=True):\n",
    "\n",
    "        sampleHDF = sampleName.split(':')\n",
    "        if len(sampleHDF) != 2 :\n",
    "            raise Exception(f\"String \\\"{sampleName}\\\" does not represent an HDF5 format.\")\n",
    "        with h5py.File(sampleHDF[0],'r') as trgH5F:\n",
    "            if  sampleHDF[1] not in trgH5F.keys():\n",
    "                raise Exception(f\"No dataset '{sampleHDF[1]}' in input file {sampleHDF[0]}.\")\n",
    "            self.data = trgH5F[sampleHDF[1]]\n",
    "            if not self.data.size :\n",
    "                raise Exception(f\"Container \\\"{sampleName}\\\" is zero size.\")\n",
    "            self.sh = self.data.shape\n",
    "            if len(self.sh) != 3 :\n",
    "                raise Exception(f\"Dimensions of the container \\\"{sampleName}\\\" is not 3 {self.sh}.\")\n",
    "            self.fsh = self.sh[1:3]\n",
    "            self.volume = None\n",
    "            if loadToMem :\n",
    "                self.volume = np.empty(self.sh, dtype=np.float32)\n",
    "                self.data.read_direct(self.volume)\n",
    "                trgH5F.close()\n",
    "\n",
    "            def loadImage(imageName) :\n",
    "                if not imageName:\n",
    "                    return None\n",
    "                imdata = imread(imageName).astype(np.float32)\n",
    "                if len(imdata.shape) == 3 :\n",
    "                    imdata = np.mean(imdata[:,:,0:3], 2)\n",
    "                if imdata.shape != self.fsh :\n",
    "                    raise Exception(f\"Dimensions of the input image \\\"{imageName}\\\" {imdata.shape} \"\n",
    "                                    f\"do not match the face of the container \\\"{sampleName}\\\" {self.fsh}.\")\n",
    "                return imdata\n",
    "\n",
    "            self.mask = loadImage(maskName)\n",
    "            if self.mask is None :\n",
    "                self.mask = np.ones(self.fsh, dtype=np.uint8)\n",
    "            self.mask = self.mask.astype(bool)\n",
    "            self.bg = loadImage(bgName)\n",
    "            self.df = loadImage(dfName)\n",
    "            if self.bg is not None :\n",
    "                if self.df is not None:\n",
    "                    self.bg -= self.df\n",
    "                self.mask  &=  self.bg > 0.0\n",
    "\n",
    "            self.allIndices = []\n",
    "            for yCr in range(0,self.fsh[0]) :\n",
    "                for xCr in range(0,self.fsh[1]) :\n",
    "                    idx = np.s_[yCr,xCr]\n",
    "                    if self.mask[idx] :\n",
    "                        if self.volume is not None :\n",
    "                            if self.df is not None :\n",
    "                                self.volume[:,*idx] -= self.df[idx]\n",
    "                            if self.bg is not None :\n",
    "                                self.volume[:,*idx] /= self.bg[idx]\n",
    "                        if  xCr + DCfg.readSh[1] < self.fsh[1] \\\n",
    "                        and np.all( self.mask[yCr,xCr+1:xCr+DCfg.readSh[1]] ) :\n",
    "                            self.allIndices.append(idx)\n",
    "\n",
    "    def get_dataset(self, transform=None) :\n",
    "\n",
    "        class Sinos(torch.utils.data.Dataset) :\n",
    "\n",
    "            def __init__(self, root, transform=None):\n",
    "                self.container = root\n",
    "                self.transform = transforms.Compose([transforms.ToTensor(), transform]) \\\n",
    "                    if transform else transforms.ToTensor()\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.container.allIndices)\n",
    "\n",
    "            def __getitem__(self, index, idxs=None):\n",
    "                idx = self.container.allIndices[index]\n",
    "                if idxs is None :\n",
    "                    idxs = random.randint(0,self.container.sh[0]-DCfg.readSh[0]-1)\n",
    "                xyrng=np.s_[ idx[0], idx[1]:idx[1]+DCfg.readSh[1] ]\n",
    "                if self.container.volume is not None :\n",
    "                    data = self.container.volume[idxs:idxs+DCfg.readSh[0], *xyrng]\n",
    "                else :\n",
    "                    data = self.container.data[idxs:idxs+DCfg.readSh[0], *xyrng]\n",
    "                    if self.container.df is not None :\n",
    "                        data -= self.container.df[None,*xyrng]\n",
    "                    if self.container.bg is not None :\n",
    "                        data /= self.container.bg[None,*xyrng]\n",
    "                if self.transform :\n",
    "                    data = self.transform(data)\n",
    "                return data\n",
    "\n",
    "        return Sinos(self, transform)\n",
    "\n",
    "sinoRoot = StripesFromHDF(\"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/SAMPLE_Y0_BG.hdf:/data\",\n",
    "                          \"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/maskc.tif\",\n",
    "                          None, None)  # 229247,541 @ gap2\n",
    "\n",
    "#sinoRoot = StripesFromHDF(\"/mnt/hddData/Linda_18515/output/Lamb1_Eiger_7m_45keV_360Scan/for_sinogap.hdf:/data\",\n",
    "#                          \"/mnt/hddData/Linda_18515/output/Lamb1_Eiger_7m_45keV_360Scan/for_sinogap_mask.tif\",\n",
    "#                          None, None )  # 34008,1620 @ gap2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Transform</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransform =  transforms.Compose([\n",
    "    transforms.Resize(DCfg.sinoSh),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=(0.5), std=(1))\n",
    "])\n",
    "trainSet = sinoRoot.get_dataset(dataTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Show</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06777197122573853 0.20532475411891937 -0.2713082432746887 0.7447687387466431\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "refImages = None\n",
    "refNoises = None\n",
    "\n",
    "def showMe() :\n",
    "    global refImages, refNoises\n",
    "    image = None\n",
    "    while True:\n",
    "        idx, idxs = (96000, 180)\n",
    "        #idx, idxs = (random.randint(0,len(trainSet)),\n",
    "        #             random.randint(0,trainSet.container.sh[0]-DCfg.readSh[0]-1) )\n",
    "        image = trainSet.__getitem__(idx,idxs).squeeze()\n",
    "        break\n",
    "        if image.mean() > 0  and image.std() > 0.1 :\n",
    "            print (f\"({idx}, {idxs})\")\n",
    "            break\n",
    "    tensorStat(image)\n",
    "    image = image.to(TCfg.device)\n",
    "    plotImage(image.cpu())\n",
    "    refImages = torch.stack((image,image)).to(TCfg.device)\n",
    "    refNoises = torch.randn((2,TCfg.latentDim)).to(TCfg.device)\n",
    "\n",
    "#plotImage(trainSet.container.volume[:, random.randint(0,trainSet.container.sh[1]-1), : ])\n",
    "showMe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## <font style=\"color:lightblue\">Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Lower resolution generators</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192532/3525890942.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator2(\n",
       "  (noise2latent): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=700, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Unflatten(dim=1, unflattened_size=(7, 10, 10))\n",
       "  )\n",
       "  (encode): Sequential(\n",
       "    (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (link): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Unflatten(dim=1, unflattened_size=(64, 4, 4))\n",
       "  )\n",
       "  (decode): Sequential(\n",
       "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Conv2d(64, 1, kernel_size=(1, 3), stride=(1, 1))\n",
       "    (7): Tanh()\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): LeakyReLU(negative_slope=0.2)\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "      (3): Unflatten(dim=1, unflattened_size=(64, 4, 4))\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "      (3): LeakyReLU(negative_slope=0.2)\n",
       "      (4): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.2)\n",
       "      (6): Conv2d(64, 1, kernel_size=(1, 3), stride=(1, 1))\n",
       "      (7): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator2, self).__init__()\n",
    "\n",
    "        self.gapW = 2\n",
    "        self.sinoSh = (5*self.gapW,5*self.gapW) # 10,10\n",
    "        self.sinoSize = math.prod(self.sinoSh)\n",
    "        self.gapSh = (self.sinoSh[0],self.gapW)\n",
    "        self.gapSize = math.prod(self.gapSh)\n",
    "        self.gapRngX = np.s_[ self.sinoSh[1]//2 - self.gapW//2 : self.sinoSh[1]//2 + self.gapW//2 ]\n",
    "        self.gapRng = np.s_[ : , self.gapRngX ]\n",
    "\n",
    "        latentChannels = 7\n",
    "        self.noise2latent = nn.Sequential(\n",
    "            nn.Linear(TCfg.latentDim, self.sinoSize*latentChannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten( 1, (latentChannels,) + self.sinoSh )\n",
    "        )\n",
    "        fillWheights(self.noise2latent)\n",
    "\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(latentChannels+1, 64, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.encode)\n",
    "\n",
    "\n",
    "        self.link = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (64, 4, 4)),\n",
    "        )\n",
    "        fillWheights(self.link)\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(64, 64, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 64, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 64, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 1, (1,3)),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "        fillWheights(self.decode)\n",
    "\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            self.encode,\n",
    "            self.link,\n",
    "            self.decode\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        images, noises = input\n",
    "        images, orgDims = unsqeeze4dim(images)\n",
    "        latent = self.noise2latent(noises)\n",
    "        modelIn = torch.cat((images,latent),dim=1)\n",
    "        mIn = modelIn[:,0,*self.gapRng]\n",
    "        mIn[()] = self.preProc(images[:,0,:,:])\n",
    "        patches = self.body(modelIn) - 0.5\n",
    "        mIn = mIn.unsqueeze(1)\n",
    "        patches = mIn + torch.where( patches < 0 , patches * (mIn+0.5) , patches )\n",
    "        return squeezeOrg(patches, orgDims)\n",
    "\n",
    "\n",
    "    def preProc(self, images) :\n",
    "        res = torch.zeros(images[...,*self.gapRng].shape, device=images.device)\n",
    "        res[...,0] += 2*images[...,self.gapRngX.start-1] + images[...,self.gapRngX.stop]\n",
    "        res[...,1] += 2*images[...,self.gapRngX.stop] + images[...,self.gapRngX.start-1]\n",
    "        return res/3\n",
    "\n",
    "\n",
    "    def generatePatches(self, images, noises=None) :\n",
    "        if noises is None :\n",
    "            noises = torch.randn(images.shape[0], TCfg.latentDim).to(TCfg.device)\n",
    "        return self.forward((images,noises))\n",
    "\n",
    "\n",
    "    def fillImages(self, images, noises=None) :\n",
    "        images[...,*self.gapRng] = self.generatePatches(images, noises)\n",
    "        return images\n",
    "\n",
    "\n",
    "    def generateImages(self, images, noises=None) :\n",
    "        clone = images.clone()\n",
    "        return self.fillImages(clone)\n",
    "\n",
    "\n",
    "\n",
    "generator2 = Generator2()\n",
    "generator2 = load_model(generator2, model_path=f\"model_1_gen.pt\" )\n",
    "generator2 = generator2.to(TCfg.device)\n",
    "generator2.eval()\n",
    "#model_summary = summary(generator2, input_data=[ [refImages, refNoises] ] ).__str__()\n",
    "#print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Generator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Generator                                [2, 20, 4]                --\n",
      "├─Sequential: 1-1                        [2, 7, 20, 20]            --\n",
      "│    └─Linear: 2-1                       [2, 2800]                 182,000\n",
      "│    └─ReLU: 2-2                         [2, 2800]                 --\n",
      "│    └─Unflatten: 2-3                    [2, 7, 20, 20]            --\n",
      "├─Sequential: 1-2                        [2, 1, 20, 4]             --\n",
      "│    └─Sequential: 2-4                   [2, 128, 4, 4]            --\n",
      "│    │    └─Conv2d: 3-1                  [2, 128, 18, 18]          9,344\n",
      "│    │    └─LeakyReLU: 3-2               [2, 128, 18, 18]          --\n",
      "│    │    └─Conv2d: 3-3                  [2, 128, 8, 8]            147,584\n",
      "│    │    └─LeakyReLU: 3-4               [2, 128, 8, 8]            --\n",
      "│    │    └─Conv2d: 3-5                  [2, 128, 6, 6]            147,584\n",
      "│    │    └─LeakyReLU: 3-6               [2, 128, 6, 6]            --\n",
      "│    │    └─Conv2d: 3-7                  [2, 128, 4, 4]            147,584\n",
      "│    │    └─LeakyReLU: 3-8               [2, 128, 4, 4]            --\n",
      "│    └─Sequential: 2-5                   [2, 128, 4, 4]            --\n",
      "│    │    └─Flatten: 3-9                 [2, 2048]                 --\n",
      "│    │    └─Linear: 3-10                 [2, 2048]                 4,196,352\n",
      "│    │    └─LeakyReLU: 3-11              [2, 2048]                 --\n",
      "│    │    └─Unflatten: 3-12              [2, 128, 4, 4]            --\n",
      "│    └─Sequential: 2-6                   [2, 1, 20, 4]             --\n",
      "│    │    └─ConvTranspose2d: 3-13        [2, 128, 6, 4]            49,280\n",
      "│    │    └─LeakyReLU: 3-14              [2, 128, 6, 4]            --\n",
      "│    │    └─ConvTranspose2d: 3-15        [2, 128, 8, 4]            49,280\n",
      "│    │    └─LeakyReLU: 3-16              [2, 128, 8, 4]            --\n",
      "│    │    └─ConvTranspose2d: 3-17        [2, 128, 18, 4]           65,664\n",
      "│    │    └─LeakyReLU: 3-18              [2, 128, 18, 4]           --\n",
      "│    │    └─ConvTranspose2d: 3-19        [2, 128, 20, 4]           49,280\n",
      "│    │    └─LeakyReLU: 3-20              [2, 128, 20, 4]           --\n",
      "│    │    └─Conv2d: 3-21                 [2, 1, 20, 4]             129\n",
      "│    │    └─Tanh: 3-22                   [2, 1, 20, 4]             --\n",
      "==========================================================================================\n",
      "Total params: 5,044,081\n",
      "Trainable params: 5,044,081\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 71.93\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.41\n",
      "Params size (MB): 20.18\n",
      "Estimated Total Size (MB): 21.59\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.gapW = DCfg.gapW\n",
    "        self.sinoSh = (5*self.gapW,5*self.gapW) # 20,20\n",
    "        self.sinoSize = math.prod(self.sinoSh)\n",
    "        self.gapSh = (self.sinoSh[0],self.gapW)\n",
    "        self.gapSize = math.prod(self.gapSh)\n",
    "        self.gapRngX = np.s_[ self.sinoSh[1]//2 - self.gapW//2 : self.sinoSh[1]//2 + self.gapW//2 ]\n",
    "        self.gapRng = np.s_[ : , self.gapRngX ]\n",
    "\n",
    "        #self.preGenerator = generator2\n",
    "        #self.preGenerator.eval()\n",
    "\n",
    "        latentChannels = 7\n",
    "        self.noise2latent = nn.Sequential(\n",
    "            nn.Linear(TCfg.latentDim, self.sinoSize*latentChannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten( 1, (latentChannels,) + self.sinoSh )\n",
    "        )\n",
    "        fillWheights(self.noise2latent)\n",
    "\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(latentChannels+1, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, stride=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.encode)\n",
    "\n",
    "\n",
    "        self.link = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (128, 4, 4)),\n",
    "        )\n",
    "        fillWheights(self.link)\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, (4,1), stride=(2,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 1, 1),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "        fillWheights(self.decode)\n",
    "\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            self.encode,\n",
    "            self.link,\n",
    "            self.decode\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        images, noises = input\n",
    "        images, orgDims = unsqeeze4dim(images)\n",
    "        latent = self.noise2latent(noises)\n",
    "        modelIn = torch.cat((images,latent),dim=1)\n",
    "        mIn = modelIn[:,0,*self.gapRng]\n",
    "        mIn[()] = self.preProc(images[:,0,:,:])\n",
    "        patches = self.body(modelIn) - 0.5\n",
    "        #return patches\n",
    "        mIn = mIn.unsqueeze(1)\n",
    "        patches = mIn + torch.where( patches < 0 , patches * (mIn+0.5) , patches )\n",
    "        return squeezeOrg(patches, orgDims)\n",
    "\n",
    "\n",
    "    def preProc(self, images) :\n",
    "        images, orgDims = unsqeeze4dim(images)\n",
    "        preImages = torch.nn.functional.interpolate(images, scale_factor=0.5, mode='area')\n",
    "        prePatches = generator2.generatePatches(preImages)\n",
    "        prePatches = torch.nn.functional.interpolate(prePatches, scale_factor=2, mode='bilinear')\n",
    "        return squeezeOrg(prePatches, orgDims)\n",
    "\n",
    "\n",
    "    def generatePatches(self, images, noises=None) :\n",
    "        if noises is None :\n",
    "            noises = torch.randn(images.shape[0], TCfg.latentDim).to(TCfg.device)\n",
    "        return self.forward((images,noises))\n",
    "\n",
    "\n",
    "    def fillImages(self, images, noises=None) :\n",
    "        images[...,*self.gapRng] = self.generatePatches(images, noises)\n",
    "        return images\n",
    "\n",
    "\n",
    "    def generateImages(self, images, noises=None) :\n",
    "        clone = images.clone()\n",
    "        return self.fillImages(clone)\n",
    "\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "if False :\n",
    "    generator = load_model(generator, model_path=f\"model_{TCfg.exec}_gen.pt\" )\n",
    "else :\n",
    "    try : os.remove(TCfg.historyHDF)\n",
    "    except : pass\n",
    "generator = generator.to(TCfg.device)\n",
    "model_summary = summary(generator, input_data=[ [refImages, refNoises] ] ).__str__()\n",
    "print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Discriminator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Discriminator                            [2, 1]                    --\n",
      "├─Sequential: 1-1                        [2, 128, 4, 4]            --\n",
      "│    └─Conv2d: 2-1                       [2, 128, 18, 18]          1,280\n",
      "│    └─LeakyReLU: 2-2                    [2, 128, 18, 18]          --\n",
      "│    └─Conv2d: 2-3                       [2, 128, 8, 8]            147,584\n",
      "│    └─LeakyReLU: 2-4                    [2, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-5                       [2, 128, 6, 6]            147,584\n",
      "│    └─LeakyReLU: 2-6                    [2, 128, 6, 6]            --\n",
      "│    └─Conv2d: 2-7                       [2, 128, 4, 4]            147,584\n",
      "│    └─LeakyReLU: 2-8                    [2, 128, 4, 4]            --\n",
      "├─Sequential: 1-2                        [2, 1]                    --\n",
      "│    └─Flatten: 2-9                      [2, 2048]                 --\n",
      "│    └─Dropout: 2-10                     [2, 2048]                 --\n",
      "│    └─Linear: 2-11                      [2, 128]                  262,272\n",
      "│    └─LeakyReLU: 2-12                   [2, 128]                  --\n",
      "│    └─Linear: 2-13                      [2, 1]                    129\n",
      "│    └─Sigmoid: 2-14                     [2, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 706,433\n",
      "Trainable params: 706,433\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 35.59\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.90\n",
      "Params size (MB): 2.83\n",
      "Estimated Total Size (MB): 3.73\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, stride=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.body)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        fillWheights(self.head)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        modelIn = images.clone()\n",
    "        convRes = self.body(modelIn)\n",
    "        res = self.head(convRes)\n",
    "        return res\n",
    "\n",
    "discriminator = Discriminator()\n",
    "#discriminator = load_model(discriminator, model_path=f\"model_{TCfg.exec}_dis.pt\")\n",
    "discriminator = discriminator.to(TCfg.device)\n",
    "model_summary = summary(discriminator, input_data=refImages ).__str__()\n",
    "print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46289483 0.46290964]\n",
      "-0.21509817242622375 0.02938493713736534 -0.24984320998191833 -0.13241493701934814\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.eval()\n",
    "discriminator.eval()\n",
    "refImages[1,...,DCfg.gapRngX] = generator.preProc(refImages[1,...])\n",
    "mgens = generator.generateImages(refImages,refNoises)\n",
    "liks = discriminator(mgens)\n",
    "print(liks.detach().cpu().squeeze().numpy())\n",
    "tensorStat( mgens[1,*DCfg.disRng] - refImages[1,*DCfg.disRng] )\n",
    "plotImages((refImages[0,...].detach().cpu().squeeze(),\n",
    "            refImages[1,...].detach().cpu().squeeze()))\n",
    "plotImages((mgens[0,...].detach().cpu().squeeze(),\n",
    "            mgens[1,...].detach().cpu().squeeze()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Train</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE = nn.BCELoss(reduction='none')\n",
    "MSE = nn.MSELoss(reduction='none')\n",
    "\n",
    "\n",
    "def applyWeights(inp, weights):\n",
    "    inp = inp.squeeze()\n",
    "    sum = len(inp)\n",
    "    if not weights is None :\n",
    "        inp *= weights\n",
    "        sum = weights.sum()\n",
    "    return inp.sum()/sum\n",
    "\n",
    "def loss_Adv(y_true, y_pred, weights=None):\n",
    "    loss = BCE(y_pred, y_true)\n",
    "    return applyWeights(loss,weights)\n",
    "\n",
    "def loss_Gen(y_true, y_pred, p_true, p_pred, weights=None):\n",
    "    lossAdv = loss_Adv(y_true, y_pred, weights)\n",
    "    lossesDif = MSE(p_pred, p_true).mean(dim=(1,2)) / (0.5+p_true.mean(dim=(1,2)))\n",
    "    lossDif = applyWeights(lossesDif, weights)\n",
    "    return lossAdv + 1000 * lossDif, lossAdv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Optimizers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_G = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=TCfg.learningRateG,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=TCfg.learningRateD,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Train step</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class TrainInfo:\n",
    "    bestRealImage = None\n",
    "    bestRealPrediction = 0\n",
    "    bestRealWght = 0\n",
    "    worstRealImage = None\n",
    "    worstRealPrediction = 0\n",
    "    worstRealWght = 0\n",
    "    bestFakeImage = None\n",
    "    bestFakePrediction = 0\n",
    "    bestFakeWght = 0\n",
    "    worstFakeImage = None\n",
    "    worstFakePrediction = 0\n",
    "    worstFakeWght = 0\n",
    "    ratReal = 0.0\n",
    "    ratFake = 0.0\n",
    "    totalImages = 0\n",
    "    iterations = 0\n",
    "    disPerformed = 0\n",
    "    genPerformed = 0\n",
    "    gaLoss = None\n",
    "\n",
    "\n",
    "trainDis = True\n",
    "trainGen = True\n",
    "def train_step(images):\n",
    "    global trainDis, trainGen\n",
    "    TrainInfo.iterations += 1\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    nofIm = images.shape[0]\n",
    "    images = images.squeeze(1).to(TCfg.device)\n",
    "    means = images.mean(dim=(1,2)).squeeze() + 0.5\n",
    "    #stdds = images.std(dim=(1,2)).squeeze()\n",
    "    absWeights = means + 1\n",
    "    absWeights /= absWeights.sum()\n",
    "    weights = absWeights.to(TCfg.device)\n",
    "    #disWeights = stdds / means\n",
    "    #disWeights /= disWeights.sum()\n",
    "    #weights = absWeights + disWeights\n",
    "    #weights /= weights.sum()\n",
    "\n",
    "    fakeImages = generator.generateImages(images)\n",
    "    y_pred_real = None\n",
    "    y_pred_fake = None\n",
    "    ratReal = 0\n",
    "    ratFake = 0\n",
    "\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "    counter = 0\n",
    "    while trainDis:\n",
    "        counter += 1\n",
    "        TrainInfo.disPerformed += 1\n",
    "        optimizer_D.zero_grad()\n",
    "        y_pred_real = discriminator(images)\n",
    "        y_pred_fake = discriminator(fakeImages)\n",
    "        y_pred_both = torch.cat((y_pred_real, y_pred_fake), dim=0)\n",
    "        labels = torch.cat( (\n",
    "            torch.full((nofIm, 1),  TCfg.labelSmoothFac),\n",
    "            torch.zeros(nofIm, 1) ),\n",
    "            dim=0\n",
    "        ).to(TCfg.device)\n",
    "        D_loss = loss_Adv(labels, y_pred_both, torch.cat( (weights, weights) ) )\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        if ratReal > ratFake:\n",
    "            trainGen = True\n",
    "            break\n",
    "        elif counter > 1:\n",
    "            trainGen = False #  ratFake < 0.1 or ratReal > 0.6\n",
    "            break\n",
    "        else:\n",
    "            images = images.detach()\n",
    "            fakeImages = fakeImages.detach()\n",
    "    if not trainDis :\n",
    "        with torch.no_grad():\n",
    "            y_pred_real = discriminator(images)\n",
    "            labels = torch.full((nofIm, 1),  TCfg.labelSmoothFac,\n",
    "                                dtype=torch.float, device=TCfg.device)\n",
    "            D_loss = loss_Adv(labels, y_pred_real, weights)\n",
    "            ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.train()\n",
    "    counter = 0\n",
    "    while trainGen :\n",
    "        counter += 1\n",
    "        TrainInfo.genPerformed += 1\n",
    "        optimizer_G.zero_grad()\n",
    "        fakeImages = generator.generateImages(images)\n",
    "        y_pred_fake = discriminator(fakeImages)\n",
    "        labels = torch.ones(nofIm, 1).to(TCfg.device)\n",
    "        G_loss, GA_loss = loss_Gen(labels, y_pred_fake,\n",
    "                                   images[...,DCfg.gapRngX], fakeImages[...,DCfg.gapRngX],\n",
    "                                   weights)\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        if ratFake > 0.1 :\n",
    "            trainDis = True\n",
    "            break\n",
    "        elif counter > 1:\n",
    "            trainDis = ratReal < 0.1 or ratFake > 0.6\n",
    "            break\n",
    "        else :\n",
    "            images = images.detach()\n",
    "            fakeImages = fakeImages.detach()\n",
    "    if not trainGen :\n",
    "        with torch.no_grad():\n",
    "            labels = torch.ones(nofIm, 1, dtype=torch.float, device=TCfg.device)\n",
    "            G_loss, GA_loss = loss_Gen(labels, y_pred_fake,\n",
    "                                       images[...,DCfg.gapRngX], fakeImages[...,DCfg.gapRngX],\n",
    "                                       weights)\n",
    "            ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "\n",
    "    trainDis = trainGen = True\n",
    "\n",
    "\n",
    "    idx = y_pred_real.argmax()\n",
    "    TrainInfo.bestRealImage = images[idx,...]\n",
    "    TrainInfo.bestRealPrediction = y_pred_real[idx].item()\n",
    "    TrainInfo.bestRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_real.argmin()\n",
    "    TrainInfo.worstRealImage = images[idx,...]\n",
    "    TrainInfo.worstRealPrediction =  y_pred_real[idx].item()\n",
    "    TrainInfo.worstRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmax()\n",
    "    TrainInfo.bestFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.bestFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.bestFakeWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmin()\n",
    "    TrainInfo.worstFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.worstFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.worstFakeWght = nofIm*weights[idx]\n",
    "    TrainInfo.ratReal += ratReal * nofIm\n",
    "    TrainInfo.ratFake += ratFake * nofIm\n",
    "    TrainInfo.totalImages += nofIm\n",
    "    TrainInfo.gaLoss = GA_loss\n",
    "\n",
    "    return D_loss, G_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Trainer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "G_LOSS = []\n",
    "D_LOSS = []\n",
    "\n",
    "def train(dataloader):\n",
    "\n",
    "    discriminator.to(TCfg.device)\n",
    "    generator.to(TCfg.device)\n",
    "    refImages.to(TCfg.device)\n",
    "    refNoises.to(TCfg.device)\n",
    "    lastUpdateTime = time.time()\n",
    "\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        epoch += 1\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        start = time.time()\n",
    "        D_loss_list, G_loss_list = [], []\n",
    "\n",
    "        for it , images in tqdm.tqdm(enumerate(dataloader), total=int(len(dataloader))):\n",
    "            images = images.to(TCfg.device)\n",
    "            D_loss, G_loss = train_step(images)\n",
    "            D_loss_list.append(D_loss)\n",
    "            G_loss_list.append(G_loss)\n",
    "\n",
    "            #if not it or it > len(dataloader)-2 or time.time() - lastUpdateTime > 60 :\n",
    "            if time.time() - lastUpdateTime > 60 :\n",
    "                lastUpdateTime = time.time()\n",
    "                IPython.display.clear_output(wait=True)\n",
    "                print(f\"Epoch: {epoch}. Losses: \"\n",
    "                      f\" Dis: {D_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratReal/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.disPerformed/TrainInfo.iterations:.3f}),\"\n",
    "                      f\" Gen: {G_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratFake/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.genPerformed/TrainInfo.iterations:.3f}),\"\n",
    "                      f\" Adv: {TrainInfo.gaLoss:.3f}\"\n",
    "                      )\n",
    "\n",
    "                TrainInfo.iterations = 0\n",
    "                TrainInfo.totalImages = 0\n",
    "                TrainInfo.ratReal = 0\n",
    "                TrainInfo.ratFake = 0\n",
    "                TrainInfo.genPerformed = 0\n",
    "                TrainInfo.disPerformed = 0\n",
    "                fourImages = np.ones( (2*DCfg.sinoSh[1] + DCfg.gapW ,\n",
    "                                       4*DCfg.sinoSh[0] + 3*DCfg.gapW), dtype=np.float32  )\n",
    "                def addImage(clmn, row, img) :\n",
    "                    fourImages[ row * ( DCfg.sinoSh[1]+DCfg.gapW) : (row+1) * DCfg.sinoSh[1] + row*DCfg.gapW ,\n",
    "                                clmn * ( DCfg.sinoSh[0]+DCfg.gapW) : (clmn+1) * DCfg.sinoSh[0] + clmn*DCfg.gapW ] = \\\n",
    "                        img.squeeze().detach().cpu().numpy()\n",
    "                refNoises[0,:] = torch.randn(TCfg.latentDim)\n",
    "                gens = generator.generateImages(refImages,refNoises)\n",
    "                dif = gens[0,...] - refImages[1,...]\n",
    "                tensorStat(dif[DCfg.disRng])\n",
    "                hGap = DCfg.gapW // 2\n",
    "                dif[:,hGap:hGap+DCfg.gapW] = (refImages[0,*DCfg.gapRng] - refImages[1,*DCfg.gapRng])\n",
    "                dif[:,-DCfg.gapW-hGap:-hGap] = (gens[0,*DCfg.gapRng] - refImages[0,*DCfg.gapRng])\n",
    "                if ( cof := max(-dif.min(),dif.max()) ) != 0 :\n",
    "                    dif /= 2*cof\n",
    "                disIn = torch.cat(( gens[0,...].unsqueeze(0), refImages), dim=0)\n",
    "                liks = discriminator(disIn)\n",
    "                addImage(0,0,TrainInfo.bestRealImage)\n",
    "                addImage(0,1,TrainInfo.worstRealImage)\n",
    "                addImage(1,0,TrainInfo.bestFakeImage)\n",
    "                addImage(1,1,TrainInfo.worstFakeImage)\n",
    "                addImage(2,0,gens[0,...])\n",
    "                addImage(2,1,refImages[0,...])\n",
    "                addImage(3,0,refImages[1,...])\n",
    "                addImage(3,1,dif)\n",
    "\n",
    "                print (f\"TT: {TrainInfo.bestRealPrediction:.4e} ({TrainInfo.bestRealWght:.3f}),  \"\n",
    "                       f\"FT: {TrainInfo.bestFakePrediction:.4e} ({TrainInfo.bestFakeWght:.3f}),  \"\n",
    "                       f\"GI: {liks[0].item():.5f}, {liks[2].item():.5f} \" )\n",
    "                print (f\"TF: {TrainInfo.worstRealPrediction:.4e} ({TrainInfo.worstRealWght:.3f}),  \"\n",
    "                       f\"FF: {TrainInfo.worstFakePrediction:.4e} ({TrainInfo.worstFakeWght:.3f}),  \"\n",
    "                       f\"RP: {liks[1].item():.5f}\" )\n",
    "                try :\n",
    "                    #tifffile.imwrite(f\"tmp_{TCfg.exec}.tif\", fourImages)\n",
    "                    addToHDF(TCfg.historyHDF, \"data\", fourImages)\n",
    "                except :\n",
    "                    eprint(\"Failed to save.\")\n",
    "                plt.imshow(fourImages, cmap='gray')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "        epoch_D_loss = sum(D_loss_list)/len(D_loss_list)\n",
    "        epoch_G_loss = sum(G_loss_list)/len(G_loss_list)\n",
    "        D_LOSS.append(epoch_D_loss.detach().cpu())\n",
    "        G_LOSS.append(epoch_G_loss.detach().cpu())\n",
    "        print('\\n')\n",
    "        print(f\"Time for epoch {epoch} is {time.time()-start:.3f} sec\")\n",
    "        print(f\"Discriminator loss: {epoch_D_loss:.3f}, Generator loss: {epoch_G_loss:.3f}.\")\n",
    "        print('\\n')\n",
    "        save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "        save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Execute</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Dis loss: 0.693 (0.515 / 1.000),  Gen loss: 1.704 (0.393 / 1.000)  GA loss: 0.694\n",
      "0.040373168885707855 0.033986497670412064 -0.044824302196502686 0.10645686089992523\n",
      "TT: 5.2595e-01 (0.991),  FT: 5.1151e-01 (1.160),  GI: 0.49758, 0.49771 \n",
      "TF: 4.7299e-01 (0.808),  FF: 4.9123e-01 (1.212),  RP: 0.50026\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [02:21<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for epoch 11 is 145.09333753585815 sec\n",
      "Discriminator loss: 0.693, Generator loss: 1.671.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    dataset=trainSet,\n",
    "    batch_size=TCfg.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "trainGen=True\n",
    "trainDis=True\n",
    "train(trainLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Post</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randIdx = random.randint(0,len(testSet)-1)\n",
    "image = testSet[randIdx]\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#image = image.to(TCfg.device)\n",
    "#fake_image = generate_images(image)\n",
    "#plt.imshow(fake_image.detach().squeeze(), cmap='gray')\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "addToHDF(f\"test_{TCfg.exec}.hdf\", \"data\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n",
    "#save_model(generator, TCfg.device, model_path=\"saves/work_1.GEN.pt\")\n",
    "#save_model(discriminator, TCfg.device, model_path=\"saves/work_1.DIS.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
