{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Header</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread, imsave\n",
    "import h5py\n",
    "import tifffile\n",
    "import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "\n",
    "def plotData(dataY, rangeY=None, dataYR=None, rangeYR=None,\n",
    "             dataX=None, rangeX=None, rangeP=None,\n",
    "             figsize=(16,8), saveTo=None, show=True):\n",
    "\n",
    "    if type(dataY) is np.ndarray :\n",
    "        plotData((dataY,), rangeY=rangeY, dataYR=dataYR, rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataYR) is np.ndarray :\n",
    "        plotData(dataY, rangeY=rangeY, dataYR=(dataYR,), rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataY) is not tuple :\n",
    "        eprint(f\"Unknown data type to plot: {type(dataY)}.\")\n",
    "        return\n",
    "    if type(dataYR) is not tuple and dataYR is not None:\n",
    "        eprint(f\"Unknown data type to plot: {type(dataYR)}.\")\n",
    "        return\n",
    "\n",
    "    last = min( len(data) for data in dataY )\n",
    "    if dataYR is not None:\n",
    "        last = min( last,  min( len(data) for data in dataYR ) )\n",
    "    if dataX is not None:\n",
    "        last = min(last, len(dataX))\n",
    "    if rangeP is None :\n",
    "        rangeP = (0,last)\n",
    "    elif type(rangeP) is int :\n",
    "        rangeP = (0,rangeP) if rangeP > 0 else (-rangeP,last)\n",
    "    elif type(rangeP) is tuple :\n",
    "        rangeP = ( 0    if rangeP[0] is None else rangeP[0],\n",
    "                   last if rangeP[1] is None else rangeP[1],)\n",
    "    else :\n",
    "        eprint(f\"Bad data type on plotData input rangeP: {type(rangeP)}\")\n",
    "        raise Exception(f\"Bug in the code.\")\n",
    "    rangeP = np.s_[ max(0, rangeP[0]) : min(last, rangeP[1]) ]\n",
    "    if dataX is None :\n",
    "        dataX = np.arange(rangeP.start, rangeP.stop)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax1.xaxis.grid(True, 'both', linestyle='dotted')\n",
    "    if rangeX is not None :\n",
    "        ax1.set_xlim(rangeX)\n",
    "    else :\n",
    "        ax1.set_xlim(rangeP.start,rangeP.stop-1)\n",
    "\n",
    "    ax1.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "    nofPlots = len(dataY)\n",
    "    if rangeY is not None:\n",
    "        ax1.set_ylim(rangeY)\n",
    "    colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "    for idx , data in enumerate(dataY):\n",
    "        ax1.plot(dataX, data[rangeP], linestyle='-',  color=colors[idx])\n",
    "\n",
    "    if dataYR is not None : # right Y axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "        nofPlots = len(dataYR)\n",
    "        if rangeYR is not None:\n",
    "            ax2.set_ylim(rangeYR)\n",
    "        colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "        for idx , data in enumerate(dataYR):\n",
    "            ax2.plot(dataX, data[rangeP], linestyle='dashed',  color=colors[idx])\n",
    "\n",
    "    if saveTo:\n",
    "        fig.savefig(saveTo)\n",
    "    if not show:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def plotImage(image) :\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotImages(images) :\n",
    "    for i, img in enumerate(images) :\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sliceShape(shape, sl) :\n",
    "    if type(shape) is int :\n",
    "        shape = torch.Size([shape])\n",
    "    if type(sl) is tuple :\n",
    "        if len(shape) != len(sl) :\n",
    "            raise Exception(f\"Different sizes of shape {shape} and sl {sl}\")\n",
    "        out = []\n",
    "        for i in range(0, len(shape)) :\n",
    "            indeces = sl[i].indices(shape[i])\n",
    "            out.append(indeces[1]-indeces[0])\n",
    "        return out\n",
    "    elif type(sl) is slice :\n",
    "        indeces = sl.indices(shape[0])\n",
    "        return indeces[1]-indeces[0]\n",
    "    else :\n",
    "        raise Exception(f\"Incompatible object {sl}\")\n",
    "\n",
    "\n",
    "def tensorStat(stat) :\n",
    "    print(stat.mean().item(), stat.std().item(), (stat.std()/stat.mean()).item(), stat.min().item(), stat.max().item())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Configs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED_VALUE):\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "\n",
    "seed = 7\n",
    "set_seed(seed)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TCfg:\n",
    "    exec = 0\n",
    "    device: torch.device = f\"cuda:{exec}\"\n",
    "    nofEpochs: int = 2048\n",
    "    latentDim: int = 64\n",
    "    batchSize: int = 4096\n",
    "    labelSmoothFac: float = 1 # For Real labels (or set to 1.0 for no smoothing).\n",
    "    learningRateD: float = 0.00002\n",
    "    learningRateG: float = 0.00002\n",
    "    historyHDF = f\"train_{exec}.hdf\"\n",
    "\n",
    "class DCfg:\n",
    "    readSh = (80,80)\n",
    "    sinoSh = (10,10)\n",
    "    sinoSize = math.prod(sinoSh)\n",
    "    gapSh = (sinoSh[0],sinoSh[0]//5)\n",
    "    gapSize = math.prod(gapSh)\n",
    "    gapRngX = np.s_[ sinoSh[1]//2 - gapSh[1]//2 : sinoSh[1]//2 + gapSh[1]//2 ]\n",
    "    gapRng = np.s_[ : , gapRngX ]\n",
    "    disRng = np.s_[ gapSh[1]:-gapSh[1] , gapRngX ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Raw Read</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StripesFromHDF :\n",
    "\n",
    "    def __init__(self, sampleName, maskName, bgName=None, dfName=None, loadToMem=True):\n",
    "\n",
    "        sampleHDF = sampleName.split(':')\n",
    "        if len(sampleHDF) != 2 :\n",
    "            raise Exception(f\"String \\\"{sampleName}\\\" does not represent an HDF5 format.\")\n",
    "        with h5py.File(sampleHDF[0],'r') as trgH5F:\n",
    "            if  sampleHDF[1] not in trgH5F.keys():\n",
    "                raise Exception(f\"No dataset '{sampleHDF[1]}' in input file {sampleHDF[0]}.\")\n",
    "            self.data = trgH5F[sampleHDF[1]]\n",
    "            if not self.data.size :\n",
    "                raise Exception(f\"Container \\\"{sampleName}\\\" is zero size.\")\n",
    "            self.sh = self.data.shape\n",
    "            if len(self.sh) != 3 :\n",
    "                raise Exception(f\"Dimensions of the container \\\"{sampleName}\\\" is not 3 {self.sh}.\")\n",
    "            self.fsh = self.sh[1:3]\n",
    "            self.volume = None\n",
    "            if loadToMem :\n",
    "                self.volume = np.empty(self.sh, dtype=np.float32)\n",
    "                self.data.read_direct(self.volume)\n",
    "                trgH5F.close()\n",
    "\n",
    "            def loadImage(imageName) :\n",
    "                if not imageName:\n",
    "                    return None\n",
    "                imdata = imread(imageName).astype(np.float32)\n",
    "                if len(imdata.shape) == 3 :\n",
    "                    imdata = np.mean(imdata[:,:,0:3], 2)\n",
    "                #imdata = imdata.transpose()\n",
    "                if imdata.shape != self.fsh :\n",
    "                    raise Exception(f\"Dimensions of the input image \\\"{imageName}\\\" {imdata.shape} \"\n",
    "                                    f\"do not match the face of the container \\\"{sampleName}\\\" {self.fsh}.\")\n",
    "                return imdata\n",
    "\n",
    "\n",
    "            self.mask = loadImage(maskName)\n",
    "            if self.mask is None :\n",
    "                self.mask = np.ones(self.fsh, dtype=np.uint8)\n",
    "            self.mask = self.mask.astype(bool)\n",
    "            self.bg = loadImage(bgName)\n",
    "            self.df = loadImage(dfName)\n",
    "            if self.bg is not None :\n",
    "                if self.df is not None:\n",
    "                    self.bg -= self.df\n",
    "                self.mask  &=  self.bg > 0.0\n",
    "\n",
    "            self.allIndices = []\n",
    "            for yCr in range(0,self.fsh[0]) :\n",
    "                for xCr in range(0,self.fsh[1]) :\n",
    "                    idx = np.s_[yCr,xCr]\n",
    "                    if self.mask[idx] :\n",
    "                        if self.volume is not None :\n",
    "                            if self.df is not None :\n",
    "                                self.volume[:,*idx] -= self.df[idx]\n",
    "                            if self.bg is not None :\n",
    "                                self.volume[:,*idx] /= self.bg[idx]\n",
    "                        if  xCr + DCfg.readSh[1] < self.fsh[1] \\\n",
    "                        and np.all( self.mask[yCr,xCr+1:xCr+DCfg.readSh[1]] ) :\n",
    "                            self.allIndices.append(idx)\n",
    "\n",
    "    def get_dataset(self, transform=None) :\n",
    "        class Sinos(torch.utils.data.Dataset) :\n",
    "            def __init__(self, root, transform=None):\n",
    "                self.container = root\n",
    "                self.transform = transforms.Compose([transforms.ToTensor(), transform]) \\\n",
    "                    if transform else transforms.ToTensor()\n",
    "            def __len__(self):\n",
    "                return len(self.container.allIndices)\n",
    "            def __getitem__(self, index):\n",
    "                idx = self.container.allIndices[index]\n",
    "                xyrng=np.s_[ idx[0], idx[1]:idx[1]+DCfg.readSh[1] ]\n",
    "                if self.container.volume is not None :\n",
    "                    data = self.container.volume[:, *xyrng]\n",
    "                else :\n",
    "                    data = self.container.data[:, *xyrng]\n",
    "                    if self.container.df is not None :\n",
    "                        data -= self.container.df[None,*xyrng]\n",
    "                    if self.container.bg is not None :\n",
    "                        data /= self.container.bg[None,*xyrng]\n",
    "                theta = random.randint(0,data.shape[0]-DCfg.readSh[0]-1)\n",
    "                data = data[theta:theta+DCfg.readSh[0],:]\n",
    "                if self.transform :\n",
    "                    data = self.transform(data)\n",
    "                return data\n",
    "        return Sinos(self, transform)\n",
    "\n",
    "sinoRoot = StripesFromHDF(\"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/SAMPLE_Y0_BG.hdf:/data\",\n",
    "                          \"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/maskc.tif\",\n",
    "                          None, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Transform</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransform =  transforms.Compose([\n",
    "    transforms.Resize(DCfg.sinoSh),\n",
    "    transforms.Normalize(mean=(0.5), std=(1))\n",
    "])\n",
    "trainSet = sinoRoot.get_dataset(dataTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Predict</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preKernel = None\n",
    "#preWeights = None\n",
    "#def preConv(images) :\n",
    "#    global preKernel, preWeights\n",
    "#    input = images[...,DCfg.gapSh[1]:-DCfg.gapSh[1]].clone()\n",
    "#    input.to(images.device)\n",
    "#    if len(input.shape) == 2 :\n",
    "#        input = input.unsqueeze(0)\n",
    "#    if len(input.shape) == 3 :\n",
    "#        input = input.unsqueeze(1)\n",
    "#    input[...,DCfg.gapSh[1]:-DCfg.gapSh[1]] = 0\n",
    "#    if preKernel is None :\n",
    "#        preKernel = torch.ones((2*DCfg.gapSh[1]+1,2*DCfg.gapSh[1]+1), device=images.device)\n",
    "#        prof = torch.zeros(DCfg.gapSh[1], device=images.device)\n",
    "#        for pp in range(DCfg.gapSh[1]) :\n",
    "#            prof[pp] = math.exp( - 6 * (pp+1)**2 / DCfg.gapSh[1]**2 )\n",
    "#        preKernel[:,0:DCfg.gapSh[1]] *= prof\n",
    "#        preKernel[:,DCfg.gapSh[1]+1:]  *= prof\n",
    "#        preKernel[0:DCfg.gapSh[1],:] *= prof.unsqueeze(1)\n",
    "#        preKernel[DCfg.gapSh[1]+1:,:]  *= prof.unsqueeze(1)\n",
    "#        preKernel = preKernel.unsqueeze(0).unsqueeze(0)\n",
    "#        img101 = torch.ones(DCfg.sinoSh,device=images.device)\n",
    "#        preWeights = preConv(img101).to(images.device)\n",
    "#    else :\n",
    "#        preKernel = preKernel.to(images.device)\n",
    "#        if not preWeights is None :\n",
    "#            preWeights = preWeights.to(images.device)\n",
    "#\n",
    "#    res = torch.nn.functional.conv2d(input[...,],preKernel,padding=(DCfg.gapSh[1],0))\n",
    "#    if len(images.shape) < 4 :\n",
    "#        res = res.squeeze(1)\n",
    "#    if len(images.shape) < 3 :\n",
    "#        res = res.squeeze(0)\n",
    "#    if not preWeights is None :\n",
    "#        res /= preWeights\n",
    "#    return res\n",
    "#\n",
    "\n",
    "\n",
    "preKernels = {}\n",
    "preWeights = None\n",
    "def preConv(images) :\n",
    "    global preKernels, preWeights\n",
    "    input = images[...,DCfg.gapSh[1]:-DCfg.gapSh[1]].clone()\n",
    "    input.to(images.device)\n",
    "    if len(input.shape) == 2 :\n",
    "        input = input.unsqueeze(0)\n",
    "    if len(input.shape) == 3 :\n",
    "        input = input.unsqueeze(1)\n",
    "    input[...,DCfg.gapSh[1]:-DCfg.gapSh[1]] = 0\n",
    "    if not preKernels :\n",
    "        for ln in range( 0, DCfg.gapSh[1]//2 + DCfg.gapSh[1]%2) :\n",
    "            rd = 1 + 2*ln\n",
    "            sz = 1 + 2*rd\n",
    "            preKernel = torch.ones((sz,sz), device=images.device)\n",
    "            prof = torch.zeros(rd, device=images.device)\n",
    "            for pp in range(rd) :\n",
    "                prof[pp] = math.exp( - (pp+1)**2 / rd**2 )\n",
    "            preKernel[:,0:rd] *= prof\n",
    "            preKernel[:,rd+1:]  *= prof\n",
    "            preKernel[0:rd,:] *= prof.unsqueeze(1)\n",
    "            preKernel[rd+1:,:]  *= prof.unsqueeze(1)\n",
    "            preKernels[ln] = preKernel.unsqueeze(0).unsqueeze(0)\n",
    "        #preKernel = torch.sqrt(preKernel).unsqueeze(0).unsqueeze(0)\n",
    "        img101 = torch.ones(DCfg.sinoSh,device=images.device)\n",
    "        preWeights = preConv(img101).to(images.device)\n",
    "    else :\n",
    "        for sz in preKernels :\n",
    "            preKernels[sz] = preKernels[sz].to(images.device)\n",
    "        if not preWeights is None :\n",
    "            preWeights = preWeights.to(images.device)\n",
    "\n",
    "    res = torch.empty((input.shape[0], input.shape[1], *DCfg.gapSh), device=images.device)\n",
    "    for lc in range(0,DCfg.gapSh[1]) :\n",
    "        ln = min(lc, DCfg.gapSh[1]-lc-1)\n",
    "        rd = 1 + 2*ln\n",
    "        preKernel = preKernels[ln]\n",
    "        inputln = input[...,DCfg.gapSh[1]+lc-rd:DCfg.gapSh[1]+lc+rd+1]\n",
    "        resln = torch.nn.functional.conv2d(inputln,preKernel,padding=(rd,0))\n",
    "        res[...,lc] = resln.squeeze(3)\n",
    "    #res = torch.nn.functional.conv2d(input,preKernel,padding=(DCfg.gapSh[1],0))\n",
    "    if len(images.shape) < 4 :\n",
    "        res = res.squeeze(1)\n",
    "    if len(images.shape) < 3 :\n",
    "        res = res.squeeze(0)\n",
    "    if not preWeights is None :\n",
    "        res /= preWeights\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Show</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3832629323005676 0.11751554906368256 0.3066186010837555 0.1504685878753662 0.5261924266815186\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "refImages = None\n",
    "refNoises = None\n",
    "\n",
    "def showMe() :\n",
    "    global refImages, refNoises\n",
    "    randIdx = random.randint(0,len(trainSet)-1) # 835526 735307\n",
    "    image = trainSet[randIdx].squeeze()\n",
    "    tensorStat(image)\n",
    "    image = image.to(TCfg.device)\n",
    "\n",
    "    #start = time.time()\n",
    "    patch = preConv(image)\n",
    "    #print(start - time.time())\n",
    "    pimage = image.clone()\n",
    "    pimage[...,*DCfg.gapRng] = patch\n",
    "    plotImages([image.transpose(0,1).cpu(),pimage.transpose(0,1).cpu()])\n",
    "\n",
    "    #tifffile.imwrite(f\"tmp_{TCfg.exec}.tif\", image.numpy())\n",
    "    refImages = torch.stack((image,pimage)).to(TCfg.device)\n",
    "    refNoises = torch.randn((2,TCfg.latentDim)).to(TCfg.device)\n",
    "\n",
    "showMe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## <font style=\"color:lightblue\">Models</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillWheights(seq) :\n",
    "    for wh in seq :\n",
    "        if hasattr(wh, 'weight') :\n",
    "            torch.nn.init.xavier_uniform_(wh.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Save/Load model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_path):\n",
    "    if not device == 'cpu':\n",
    "        model.to('cpu')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    if not device == 'cpu':\n",
    "        model.to(device)\n",
    "    return\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def addToHDF(filename, containername, data) :\n",
    "    if len(data.shape) == 2 :\n",
    "        data=np.expand_dims(data, 0)\n",
    "    if len(data.shape) != 3 :\n",
    "        raise Exception(f\"Not appropriate input array size {data.shape}.\")\n",
    "\n",
    "    with h5py.File(filename,'a') as file :\n",
    "\n",
    "        if  containername not in file.keys():\n",
    "            dset = file.create_dataset(containername, data.shape,\n",
    "                                       maxshape=(None,data.shape[1],data.shape[2]),\n",
    "                                       dtype='f')\n",
    "            dset[()] = data\n",
    "            return\n",
    "\n",
    "        dset = file[containername]\n",
    "        csh = dset.shape\n",
    "        if csh[1] != data.shape[1] or csh[2] != data.shape[2] :\n",
    "            raise Exception(f\"Shape mismatch: input {data.shape}, file {dset.shape}.\")\n",
    "        msh = dset.maxshape\n",
    "        newLen = csh[0] + data.shape[0]\n",
    "        if msh[0] is None or msh[0] >= newLen :\n",
    "            dset.resize(newLen, axis=0)\n",
    "        else :\n",
    "            raise Exception(f\"Insufficient maximum shape {msh} to add data\"\n",
    "                            f\" {data.shape} to current volume {dset.shape}.\")\n",
    "        dset[csh[0]:newLen,...] = data\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Generator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Generator                                [2, 10, 10]               --\n",
      "├─Sequential: 1-1                        [2, 63, 10, 10]           --\n",
      "│    └─Linear: 2-1                       [2, 6300]                 409,500\n",
      "│    └─ReLU: 2-2                         [2, 6300]                 --\n",
      "│    └─Unflatten: 2-3                    [2, 63, 10, 10]           --\n",
      "├─Sequential: 1-2                        [2, 1, 10, 2]             --\n",
      "│    └─Sequential: 2-4                   [2, 512, 4, 4]            --\n",
      "│    │    └─Conv2d: 3-1                  [2, 128, 8, 8]            73,856\n",
      "│    │    └─LeakyReLU: 3-2               [2, 128, 8, 8]            --\n",
      "│    │    └─Conv2d: 3-3                  [2, 256, 6, 6]            295,168\n",
      "│    │    └─LeakyReLU: 3-4               [2, 256, 6, 6]            --\n",
      "│    │    └─Conv2d: 3-5                  [2, 512, 4, 4]            1,180,160\n",
      "│    │    └─LeakyReLU: 3-6               [2, 512, 4, 4]            --\n",
      "│    └─Sequential: 2-5                   [2, 512, 4, 4]            --\n",
      "│    │    └─Flatten: 3-7                 [2, 8192]                 --\n",
      "│    │    └─Linear: 3-8                  [2, 8192]                 67,117,056\n",
      "│    │    └─LeakyReLU: 3-9               [2, 8192]                 --\n",
      "│    │    └─Unflatten: 3-10              [2, 512, 4, 4]            --\n",
      "│    └─Sequential: 2-6                   [2, 1, 10, 2]             --\n",
      "│    │    └─ConvTranspose2d: 3-11        [2, 256, 6, 4]            393,472\n",
      "│    │    └─LeakyReLU: 3-12              [2, 256, 6, 4]            --\n",
      "│    │    └─ConvTranspose2d: 3-13        [2, 128, 8, 4]            98,432\n",
      "│    │    └─LeakyReLU: 3-14              [2, 128, 8, 4]            --\n",
      "│    │    └─ConvTranspose2d: 3-15        [2, 64, 10, 4]            24,640\n",
      "│    │    └─LeakyReLU: 3-16              [2, 64, 10, 4]            --\n",
      "│    │    └─Conv2d: 3-17                 [2, 1, 10, 2]             193\n",
      "│    │    └─LeakyReLU: 3-18              [2, 1, 10, 2]             --\n",
      "==========================================================================================\n",
      "Total params: 69,592,477\n",
      "Trainable params: 69,592,477\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 230.69\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.85\n",
      "Params size (MB): 278.37\n",
      "Estimated Total Size (MB): 279.22\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.noise2latent = nn.Sequential(\n",
    "            nn.Linear(TCfg.latentDim, DCfg.sinoSize*63),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten( 1, (63,) + DCfg.sinoSh )\n",
    "        )\n",
    "        fillWheights(self.noise2latent)\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "\n",
    "        )\n",
    "        fillWheights(self.encode)\n",
    "\n",
    "\n",
    "        self.link = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8192, 8192),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (512, 4, 4)),\n",
    "        )\n",
    "        fillWheights(self.link)\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 1, (1,3)),\n",
    "#            nn.Tanh()\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.decode)\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            self.encode,\n",
    "            self.link,\n",
    "            self.decode\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        images, noises = input\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        latent = self.noise2latent(noises)\n",
    "        modelIn = torch.cat((images,latent),dim=1)\n",
    "        modelIn[:,0,*DCfg.gapRng] = preConv(images[:,0,:,:])\n",
    "        patches = self.body(modelIn) - 0.5\n",
    "        #return patches\n",
    "        #mIn = modelIn[:,0,*DCfg.gapRng].unsqueeze(1)\n",
    "        #tensorStat(mIn[:,DCfg.gapSh[1]:-DCfg.gapSh[1]])\n",
    "        #tensorStat(patches[:,DCfg.gapSh[1]:-DCfg.gapSh[1]])\n",
    "        #patches = mIn + torch.where( patches < 0 , patches * (mIn+0.5) , patches )\n",
    "        #tensorStat(patches[:,DCfg.gapSh[1]:-DCfg.gapSh[1]])\n",
    "        modelOut = torch.cat((images[:,:,:,:DCfg.gapRngX.start], patches, images[:,:,:,DCfg.gapRngX.stop:]),dim=3)\n",
    "        return modelOut.squeeze(1)\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "#generator = load_model(generator, model_path=f\"model_{TCfg.exec}_gen.pt\" )\n",
    "generator.to(TCfg.device)\n",
    "model_summary = summary(generator, input_data=[ [refImages, refNoises] ] ).__str__()\n",
    "print(model_summary)\n",
    "try : os.remove(TCfg.historyHDF)\n",
    "except : pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Discriminator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Discriminator                            [2, 1]                    --\n",
      "├─Sequential: 1-1                        [2, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-1                       [2, 128, 8, 8]            1,280\n",
      "│    └─LeakyReLU: 2-2                    [2, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-3                       [2, 256, 6, 6]            295,168\n",
      "│    └─LeakyReLU: 2-4                    [2, 256, 6, 6]            --\n",
      "│    └─Conv2d: 2-5                       [2, 512, 4, 4]            1,180,160\n",
      "│    └─LeakyReLU: 2-6                    [2, 512, 4, 4]            --\n",
      "├─Sequential: 1-2                        [2, 1]                    --\n",
      "│    └─Flatten: 2-7                      [2, 8192]                 --\n",
      "│    └─Dropout: 2-8                      [2, 8192]                 --\n",
      "│    └─Linear: 2-9                       [2, 128]                  1,048,704\n",
      "│    └─Linear: 2-10                      [2, 1]                    129\n",
      "│    └─Sigmoid: 2-11                     [2, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 2,525,441\n",
      "Trainable params: 2,525,441\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 61.28\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.41\n",
      "Params size (MB): 10.10\n",
      "Estimated Total Size (MB): 10.51\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.body)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(8192, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        fillWheights(self.head)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        modelIn = images.clone()\n",
    "        modelIn[...,0:DCfg.gapSh[1],DCfg.gapRngX] = 0 # torch.randn(images.shape[0],1,DCfg.gapSh[1],DCfg.gapSh[1])\n",
    "        modelIn[...,-DCfg.gapSh[1]:,DCfg.gapRngX] = 0 # torch.randn(images.shape[0],1,DCfg.gapSh[1],DCfg.gapSh[1])\n",
    "        convRes = self.body(modelIn)\n",
    "        res = self.head(convRes)\n",
    "        return res\n",
    "\n",
    "discriminator = Discriminator()\n",
    "#discriminator = load_model(discriminator, model_path=f\"model_{TCfg.exec}_dis.pt\" )\n",
    "discriminator.to(TCfg.device)\n",
    "model_summary = summary(discriminator, input_data=refImages ).__str__()\n",
    "print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4386716 0.4386335]\n",
      "-0.7925593852996826 0.1018279567360878 -0.12847991287708282 -0.955499529838562 -0.6678276062011719\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.eval()\n",
    "discriminator.eval()\n",
    "mgens = generator([refImages,refNoises])\n",
    "liks = discriminator(mgens)\n",
    "print(liks.detach().cpu().squeeze().numpy())\n",
    "tensorStat( mgens[1,*DCfg.disRng] - refImages[1,*DCfg.disRng] )\n",
    "plotImages((mgens[0,...].detach().cpu().squeeze().transpose(0,1),\n",
    "            mgens[1,...].detach().cpu().squeeze().transpose(0,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Train</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE = nn.BCELoss(reduction='none')\n",
    "\n",
    "def loss_func(y_true, y_pred, weights=None):\n",
    "    loss = BCE(y_pred, y_true)\n",
    "    summ = len(loss)\n",
    "    if not weights is None :\n",
    "        loss *= weights.unsqueeze(1)\n",
    "        summ = weights.sum()\n",
    "    return loss.sum() / summ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Optimizers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_G = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=TCfg.learningRateG,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=TCfg.learningRateD,\n",
    "    betas=(0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Train step</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images(images) :\n",
    "    noise = torch.randn(images.shape[0], TCfg.latentDim).to(TCfg.device)\n",
    "    outImages = generator((images, noise))\n",
    "    return outImages\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class TrainInfo:\n",
    "    bestRealImage = None\n",
    "    bestRealPrediction = 0\n",
    "    bestRealWght = 0\n",
    "    worstRealImage = None\n",
    "    worstRealPrediction = 0\n",
    "    worstRealWght = 0\n",
    "    bestFakeImage = None\n",
    "    bestFakePrediction = 0\n",
    "    bestFakeWght = 0\n",
    "    worstFakeImage = None\n",
    "    worstFakePrediction = 0\n",
    "    worstFakeWght = 0\n",
    "    ratReal = 0.0\n",
    "    ratFake = 0.0\n",
    "    totalImages = 0\n",
    "    iterations = 0\n",
    "    disPerformed = 0\n",
    "    genPerformed = 0\n",
    "\n",
    "\n",
    "trainDis = True\n",
    "trainGen = True\n",
    "def train_step(images):\n",
    "    global trainDis, trainGen\n",
    "    TrainInfo.iterations += 1\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    nofIm = images.shape[0]\n",
    "    images = images.squeeze(1).to(TCfg.device)\n",
    "    means = images.mean(dim=(1,2)).squeeze() + 0.5\n",
    "    stdds = images.std(dim=(1,2)).squeeze()\n",
    "    absWeights = means + 1\n",
    "    absWeights /= absWeights.sum()\n",
    "    disWeights = stdds / means\n",
    "    disWeights /= disWeights.sum()\n",
    "    weights = absWeights + disWeights\n",
    "    weights /= weights.sum()\n",
    "    fakeImages = generate_images(images)\n",
    "\n",
    "    generator.eval()\n",
    "    if trainDis :\n",
    "        discriminator.train()\n",
    "        optimizer_D.zero_grad()\n",
    "    y_pred_real = discriminator(images)\n",
    "    y_pred_fake = discriminator(fakeImages)\n",
    "    y_pred_both = torch.cat((y_pred_real, y_pred_fake), dim=0)\n",
    "    labels = torch.cat( (\n",
    "        torch.full((nofIm, 1),  TCfg.labelSmoothFac),\n",
    "        torch.zeros(nofIm, 1) ),\n",
    "        dim=0\n",
    "    ).to(TCfg.device)\n",
    "    D_loss = None\n",
    "    dweights = torch.cat( (weights, weights) )\n",
    "    if trainDis :\n",
    "        TrainInfo.disPerformed += 1\n",
    "        D_loss = loss_func(labels, y_pred_both, dweights )\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    else :\n",
    "        with torch.no_grad():\n",
    "            D_loss = loss_func(labels, y_pred_both, torch.cat( (weights, weights) ))\n",
    "\n",
    "    ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "    ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "    trainGen = ratReal > 0.5 and ratFake < 0.5\n",
    "\n",
    "    G_loss = None\n",
    "    if trainGen:\n",
    "        discriminator.eval()\n",
    "        if trainGen :\n",
    "            generator.train()\n",
    "            optimizer_G.zero_grad()\n",
    "        fakeImages = generate_images(images)\n",
    "        y_pred_fake = discriminator(fakeImages)\n",
    "        labels = torch.ones(nofIm, 1).to(TCfg.device)\n",
    "        if trainGen :\n",
    "            TrainInfo.genPerformed += 1\n",
    "            G_loss = loss_func(labels, y_pred_fake, weights)\n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        trainDis = ratFake > 0.1\n",
    "    else :\n",
    "        labels = torch.ones(nofIm, 1).to(TCfg.device)\n",
    "        with torch.no_grad() :\n",
    "            G_loss = loss_func(labels, y_pred_fake, weights)\n",
    "        optimizer_D.zero_grad()\n",
    "        trainDis = True\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    idx = y_pred_real.argmax()\n",
    "    TrainInfo.bestRealImage = images[idx,...]\n",
    "    TrainInfo.bestRealPrediction = y_pred_real[idx].item()\n",
    "    TrainInfo.bestRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_real.argmin()\n",
    "    TrainInfo.worstRealImage = images[idx,...]\n",
    "    TrainInfo.worstRealPrediction =  y_pred_real[idx].item()\n",
    "    TrainInfo.worstRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmax()\n",
    "    TrainInfo.bestFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.bestFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.bestFakeWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmin()\n",
    "    TrainInfo.worstFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.worstFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.worstFakeWght = nofIm*weights[idx]\n",
    "    TrainInfo.ratReal += ratReal * nofIm\n",
    "    TrainInfo.ratFake += ratFake * nofIm\n",
    "    TrainInfo.totalImages += nofIm\n",
    "\n",
    "    return D_loss, G_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Trainer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "G_LOSS = []\n",
    "D_LOSS = []\n",
    "\n",
    "def train(dataloader, epochs):\n",
    "\n",
    "    discriminator.to(TCfg.device)\n",
    "    generator.to(TCfg.device)\n",
    "    refImages.to(TCfg.device)\n",
    "    refNoises.to(TCfg.device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        start = time.time()\n",
    "        lastUpdateTime = start\n",
    "        D_loss_list, G_loss_list = [], []\n",
    "\n",
    "        pit = -1\n",
    "        for it , images in tqdm.tqdm(enumerate(dataloader), total=int(len(dataloader))):\n",
    "\n",
    "            images = images.to(TCfg.device)\n",
    "            D_loss, G_loss = train_step(images)\n",
    "            D_loss_list.append(D_loss)\n",
    "            G_loss_list.append(G_loss)\n",
    "\n",
    "            #if not it or it == len(dataloader)-1 or time.time() - lastUpdateTime > 60 :\n",
    "            if  it == len(dataloader)-1:\n",
    "                lastUpdateTime = time.time()\n",
    "                its = it-pit\n",
    "                print(f\"Epoch: {epoch}.\"\n",
    "                      f\" Discriminator loss: {D_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratReal/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.disPerformed/TrainInfo.iterations:.3f}), \"\n",
    "                      f\" Generator loss: {G_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratFake/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.genPerformed/TrainInfo.iterations:.3f}).\")\n",
    "                TrainInfo.iterations = 0\n",
    "                TrainInfo.totalImages = 0\n",
    "                TrainInfo.ratReal = 0\n",
    "                TrainInfo.ratFake = 0\n",
    "                TrainInfo.genPerformed = 0\n",
    "                TrainInfo.disPerformed = 0\n",
    "                pit = it\n",
    "                fourImages = np.ones( (2*DCfg.sinoSh[1] + DCfg.gapSh[1] ,\n",
    "                                       3*DCfg.sinoSh[0] + 2*DCfg.gapSh[1]), dtype=np.float32  )\n",
    "                def addImage(clmn, row, img) :\n",
    "                    fourImages[ row * ( DCfg.sinoSh[1]+DCfg.gapSh[1]) : (row+1) * DCfg.sinoSh[1] + row*DCfg.gapSh[1] ,\n",
    "                                clmn * ( DCfg.sinoSh[0]+DCfg.gapSh[1]) : (clmn+1) * DCfg.sinoSh[0] + clmn*DCfg.gapSh[1] ] = \\\n",
    "                        img.squeeze().transpose(0,1).detach().cpu().numpy()\n",
    "                refNoises[0,:] = torch.randn(TCfg.latentDim)\n",
    "                gens = generator([refImages,refNoises])\n",
    "                tensorStat( gens[0,*DCfg.disRng] - refImages[0,*DCfg.disRng] )\n",
    "                disIn = torch.cat(( gens[0,...].unsqueeze(0), refImages), dim=0)\n",
    "                liks = discriminator(disIn)\n",
    "                addImage(0,0,TrainInfo.bestRealImage)\n",
    "                addImage(0,1,TrainInfo.worstRealImage)\n",
    "                addImage(1,0,TrainInfo.bestFakeImage)\n",
    "                addImage(1,1,TrainInfo.worstFakeImage)\n",
    "                addImage(2,0,gens[0,...])\n",
    "                addImage(2,1,refImages[0,...])\n",
    "                print (f\"TT: {TrainInfo.bestRealPrediction:.4e} ({TrainInfo.bestRealWght:.3f}),  \"\n",
    "                       f\"FT: {TrainInfo.bestFakePrediction:.4e} ({TrainInfo.bestFakeWght:.3f}),  \"\n",
    "                       f\"GI: {liks[0].item():.5f},  \" )\n",
    "                print (f\"TF: {TrainInfo.worstRealPrediction:.4e} ({TrainInfo.worstRealWght:.3f}),  \"\n",
    "                       f\"FF: {TrainInfo.worstFakePrediction:.4e} ({TrainInfo.worstFakeWght:.3f}),  \"\n",
    "                       f\"RP: {liks[1].item():.5f}, {liks[2].item():.5f} \" )\n",
    "                try :\n",
    "                    #tifffile.imwrite(f\"tmp_{TCfg.exec}.tif\", fourImages)\n",
    "                    addToHDF(TCfg.historyHDF, \"data\", fourImages)\n",
    "                except :\n",
    "                    eprint(\"Failed to save.\")\n",
    "                plt.imshow(fourImages, cmap='gray')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "        epoch_D_loss = sum(D_loss_list)/len(D_loss_list)\n",
    "        epoch_G_loss = sum(G_loss_list)/len(G_loss_list)\n",
    "        D_LOSS.append(epoch_D_loss.detach().cpu())\n",
    "        G_LOSS.append(epoch_G_loss.detach().cpu())\n",
    "        print('\\n')\n",
    "        print(f\"Time for epoch {epoch + 1} is {time.time()-start} sec\")\n",
    "        print(f\"Discriminator loss: {epoch_D_loss:.3f}, Generator loss: {epoch_G_loss:.3f}.\")\n",
    "        print('\\n')\n",
    "        save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "        save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Execute</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 314/315 [03:48<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180. Discriminator loss: 0.667 (0.551 / 1.000),  Generator loss: 0.738 (0.407 / 0.537).\n",
      "-0.002672384260222316 0.011623401194810867 -4.349449634552002 -0.019812464714050293 0.016743957996368408\n",
      "TT: 9.1753e-01 (2.052),  FT: 5.6545e-01 (0.808),  GI: 0.51611,  \n",
      "TF: 2.4057e-01 (0.852),  FF: 3.4909e-01 (1.331),  RP: 0.52652, 0.37279 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [03:49<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for epoch 181 is 234.32289934158325 sec\n",
      "Discriminator loss: 0.686, Generator loss: 0.729.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/315 [00:03<05:01,  1.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m trainDis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#fillWheights(generator.body)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#fillWheights(discriminator.body)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#fillWheights(discriminator.head)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTCfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnofEpochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it , images \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataloader))):\n\u001b[1;32m     22\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(TCfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m     D_loss, G_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     D_loss_list\u001b[38;5;241m.\u001b[39mappend(D_loss)\n\u001b[1;32m     25\u001b[0m     G_loss_list\u001b[38;5;241m.\u001b[39mappend(G_loss)\n",
      "Cell \u001b[0;32mIn[37], line 62\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     56\u001b[0m y_pred_fake \u001b[38;5;241m=\u001b[39m discriminator(fakeImages)\n\u001b[1;32m     57\u001b[0m y_pred_both \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((y_pred_real, y_pred_fake), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnofIm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mTCfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabelSmoothFac\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnofIm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m---> 62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTCfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m D_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m dweights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat( (weights, weights) )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    dataset=trainSet,\n",
    "    batch_size=TCfg.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "trainGen=True\n",
    "trainDis=True\n",
    "#fillWheights(generator.body)\n",
    "#fillWheights(discriminator.body)\n",
    "#fillWheights(discriminator.head)\n",
    "train(trainLoader, TCfg.nofEpochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Post</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randIdx = random.randint(0,len(testSet)-1)\n",
    "image = testSet[randIdx]\n",
    "plt.imshow(image.squeeze().transpose(0,1), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#image = image.to(TCfg.device)\n",
    "#fake_image = generate_images(image)\n",
    "#plt.imshow(fake_image.detach().squeeze().transpose(0,1), cmap='gray')\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "addToHDF(f\"test_{TCfg.exec}.hdf\", \"data\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n",
    "#save_model(generator, TCfg.device, model_path=\"saves/work_1.GEN.pt\")\n",
    "#save_model(discriminator, TCfg.device, model_path=\"saves/work_1.DIS.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
