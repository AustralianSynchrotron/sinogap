{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Header</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread, imsave\n",
    "import h5py\n",
    "import tifffile\n",
    "import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "\n",
    "def plotData(dataY, rangeY=None, dataYR=None, rangeYR=None,\n",
    "             dataX=None, rangeX=None, rangeP=None,\n",
    "             figsize=(16,8), saveTo=None, show=True):\n",
    "\n",
    "    if type(dataY) is np.ndarray :\n",
    "        plotData((dataY,), rangeY=rangeY, dataYR=dataYR, rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataYR) is np.ndarray :\n",
    "        plotData(dataY, rangeY=rangeY, dataYR=(dataYR,), rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataY) is not tuple :\n",
    "        eprint(f\"Unknown data type to plot: {type(dataY)}.\")\n",
    "        return\n",
    "    if type(dataYR) is not tuple and dataYR is not None:\n",
    "        eprint(f\"Unknown data type to plot: {type(dataYR)}.\")\n",
    "        return\n",
    "\n",
    "    last = min( len(data) for data in dataY )\n",
    "    if dataYR is not None:\n",
    "        last = min( last,  min( len(data) for data in dataYR ) )\n",
    "    if dataX is not None:\n",
    "        last = min(last, len(dataX))\n",
    "    if rangeP is None :\n",
    "        rangeP = (0,last)\n",
    "    elif type(rangeP) is int :\n",
    "        rangeP = (0,rangeP) if rangeP > 0 else (-rangeP,last)\n",
    "    elif type(rangeP) is tuple :\n",
    "        rangeP = ( 0    if rangeP[0] is None else rangeP[0],\n",
    "                   last if rangeP[1] is None else rangeP[1],)\n",
    "    else :\n",
    "        eprint(f\"Bad data type on plotData input rangeP: {type(rangeP)}\")\n",
    "        raise Exception(f\"Bug in the code.\")\n",
    "    rangeP = np.s_[ max(0, rangeP[0]) : min(last, rangeP[1]) ]\n",
    "    if dataX is None :\n",
    "        dataX = np.arange(rangeP.start, rangeP.stop)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax1.xaxis.grid(True, 'both', linestyle='dotted')\n",
    "    if rangeX is not None :\n",
    "        ax1.set_xlim(rangeX)\n",
    "    else :\n",
    "        ax1.set_xlim(rangeP.start,rangeP.stop-1)\n",
    "\n",
    "    ax1.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "    nofPlots = len(dataY)\n",
    "    if rangeY is not None:\n",
    "        ax1.set_ylim(rangeY)\n",
    "    colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "    for idx , data in enumerate(dataY):\n",
    "        ax1.plot(dataX, data[rangeP], linestyle='-',  color=colors[idx])\n",
    "\n",
    "    if dataYR is not None : # right Y axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "        nofPlots = len(dataYR)\n",
    "        if rangeYR is not None:\n",
    "            ax2.set_ylim(rangeYR)\n",
    "        colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "        for idx , data in enumerate(dataYR):\n",
    "            ax2.plot(dataX, data[rangeP], linestyle='dashed',  color=colors[idx])\n",
    "\n",
    "    if saveTo:\n",
    "        fig.savefig(saveTo)\n",
    "    if not show:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def plotImage(image) :\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotImages(images) :\n",
    "    for i, img in enumerate(images) :\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sliceShape(shape, sl) :\n",
    "    if type(shape) is int :\n",
    "        shape = torch.Size([shape])\n",
    "    if type(sl) is tuple :\n",
    "        if len(shape) != len(sl) :\n",
    "            raise Exception(f\"Different sizes of shape {shape} and sl {sl}\")\n",
    "        out = []\n",
    "        for i in range(0, len(shape)) :\n",
    "            indeces = sl[i].indices(shape[i])\n",
    "            out.append(indeces[1]-indeces[0])\n",
    "        return out\n",
    "    elif type(sl) is slice :\n",
    "        indeces = sl.indices(shape[0])\n",
    "        return indeces[1]-indeces[0]\n",
    "    else :\n",
    "        raise Exception(f\"Incompatible object {sl}\")\n",
    "\n",
    "\n",
    "def tensorStat(stat) :\n",
    "    print(stat.mean().item(), stat.std().item(), (stat.std()/stat.mean()).item(), stat.min().item(), stat.max().item())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Configs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED_VALUE):\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "\n",
    "seed = 7\n",
    "set_seed(seed)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TCfg:\n",
    "    exec = 1\n",
    "    device: torch.device = f\"cuda:{exec}\"\n",
    "    nofEpochs: int = 2048\n",
    "    latentDim: int = 64\n",
    "    batchSize: int = 4096\n",
    "    labelSmoothFac: float = 1 # For Real labels (or set to 1.0 for no smoothing).\n",
    "    learningRateD: float = 0.00002\n",
    "    learningRateG: float = 0.00002\n",
    "    historyHDF = f\"train_{exec}.hdf\"\n",
    "\n",
    "class DCfg:\n",
    "    readSh = (80,80)\n",
    "    sinoSh = (10,10)\n",
    "    sinoSize = math.prod(sinoSh)\n",
    "    gapSh = (sinoSh[0],sinoSh[0]//5)\n",
    "    gapSize = math.prod(gapSh)\n",
    "    gapRngX = np.s_[ sinoSh[1]//2 - gapSh[1]//2 : sinoSh[1]//2 + gapSh[1]//2 ]\n",
    "    gapRng = np.s_[ : , gapRngX ]\n",
    "    disRng = np.s_[ gapSh[1]:-gapSh[1] , gapRngX ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Raw Read</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StripesFromHDF :\n",
    "\n",
    "    def __init__(self, sampleName, maskName, bgName=None, dfName=None, loadToMem=True):\n",
    "\n",
    "        sampleHDF = sampleName.split(':')\n",
    "        if len(sampleHDF) != 2 :\n",
    "            raise Exception(f\"String \\\"{sampleName}\\\" does not represent an HDF5 format.\")\n",
    "        with h5py.File(sampleHDF[0],'r') as trgH5F:\n",
    "            if  sampleHDF[1] not in trgH5F.keys():\n",
    "                raise Exception(f\"No dataset '{sampleHDF[1]}' in input file {sampleHDF[0]}.\")\n",
    "            self.data = trgH5F[sampleHDF[1]]\n",
    "            if not self.data.size :\n",
    "                raise Exception(f\"Container \\\"{sampleName}\\\" is zero size.\")\n",
    "            self.sh = self.data.shape\n",
    "            if len(self.sh) != 3 :\n",
    "                raise Exception(f\"Dimensions of the container \\\"{sampleName}\\\" is not 3 {self.sh}.\")\n",
    "            self.fsh = self.sh[1:3]\n",
    "            self.volume = None\n",
    "            if loadToMem :\n",
    "                self.volume = np.empty(self.sh, dtype=np.float32)\n",
    "                self.data.read_direct(self.volume)\n",
    "                trgH5F.close()\n",
    "\n",
    "            def loadImage(imageName) :\n",
    "                if not imageName:\n",
    "                    return None\n",
    "                imdata = imread(imageName).astype(np.float32)\n",
    "                if len(imdata.shape) == 3 :\n",
    "                    imdata = np.mean(imdata[:,:,0:3], 2)\n",
    "                #imdata = imdata.transpose()\n",
    "                if imdata.shape != self.fsh :\n",
    "                    raise Exception(f\"Dimensions of the input image \\\"{imageName}\\\" {imdata.shape} \"\n",
    "                                    f\"do not match the face of the container \\\"{sampleName}\\\" {self.fsh}.\")\n",
    "                return imdata\n",
    "\n",
    "\n",
    "            self.mask = loadImage(maskName)\n",
    "            if self.mask is None :\n",
    "                self.mask = np.ones(self.fsh, dtype=np.uint8)\n",
    "            self.mask = self.mask.astype(bool)\n",
    "            self.bg = loadImage(bgName)\n",
    "            self.df = loadImage(dfName)\n",
    "            if self.bg is not None :\n",
    "                if self.df is not None:\n",
    "                    self.bg -= self.df\n",
    "                self.mask  &=  self.bg > 0.0\n",
    "\n",
    "            self.allIndices = []\n",
    "            for yCr in range(0,self.fsh[0]) :\n",
    "                for xCr in range(0,self.fsh[1]) :\n",
    "                    idx = np.s_[yCr,xCr]\n",
    "                    if self.mask[idx] :\n",
    "                        if self.volume is not None :\n",
    "                            if self.df is not None :\n",
    "                                self.volume[:,*idx] -= self.df[idx]\n",
    "                            if self.bg is not None :\n",
    "                                self.volume[:,*idx] /= self.bg[idx]\n",
    "                        if  xCr + DCfg.readSh[1] < self.fsh[1] \\\n",
    "                        and np.all( self.mask[yCr,xCr+1:xCr+DCfg.readSh[1]] ) :\n",
    "                            self.allIndices.append(idx)\n",
    "\n",
    "    def get_dataset(self, transform=None) :\n",
    "        class Sinos(torch.utils.data.Dataset) :\n",
    "            def __init__(self, root, transform=None):\n",
    "                self.container = root\n",
    "                self.transform = transforms.Compose([transforms.ToTensor(), transform]) \\\n",
    "                    if transform else transforms.ToTensor()\n",
    "            def __len__(self):\n",
    "                return len(self.container.allIndices)\n",
    "            def __getitem__(self, index):\n",
    "                idx = self.container.allIndices[index]\n",
    "                xyrng=np.s_[ idx[0], idx[1]:idx[1]+DCfg.readSh[1] ]\n",
    "                if self.container.volume is not None :\n",
    "                    data = self.container.volume[:, *xyrng]\n",
    "                else :\n",
    "                    data = self.container.data[:, *xyrng]\n",
    "                    if self.container.df is not None :\n",
    "                        data -= self.container.df[None,*xyrng]\n",
    "                    if self.container.bg is not None :\n",
    "                        data /= self.container.bg[None,*xyrng]\n",
    "                theta = random.randint(0,data.shape[0]-DCfg.readSh[0]-1)\n",
    "                data = data[theta:theta+DCfg.readSh[0],:]\n",
    "                if self.transform :\n",
    "                    data = self.transform(data)\n",
    "                return data\n",
    "        return Sinos(self, transform)\n",
    "\n",
    "sinoRoot = StripesFromHDF(\"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/SAMPLE_Y0_BG.hdf:/data\",\n",
    "                          \"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/maskc.tif\",\n",
    "                          None, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Transform</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransform =  transforms.Compose([\n",
    "    transforms.Resize(DCfg.sinoSh),\n",
    "    transforms.Normalize(mean=(0.5), std=(1))\n",
    "])\n",
    "trainSet = sinoRoot.get_dataset(dataTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Predict</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#preKernel = None\n",
    "#preWeights = None\n",
    "#def preConv(images) :\n",
    "#    global preKernel, preWeights\n",
    "#    input = images[...,DCfg.gapSh[1]:-DCfg.gapSh[1]].clone()\n",
    "#    input.to(images.device)\n",
    "#    if len(input.shape) == 2 :\n",
    "#        input = input.unsqueeze(0)\n",
    "#    if len(input.shape) == 3 :\n",
    "#        input = input.unsqueeze(1)\n",
    "#    input[...,DCfg.gapSh[1]:-DCfg.gapSh[1]] = 0\n",
    "#    if preKernel is None :\n",
    "#        preKernel = torch.ones((2*DCfg.gapSh[1]+1,2*DCfg.gapSh[1]+1), device=images.device)\n",
    "#        prof = torch.zeros(DCfg.gapSh[1], device=images.device)\n",
    "#        for pp in range(DCfg.gapSh[1]) :\n",
    "#            prof[pp] = math.exp( - 6 * (pp+1)**2 / DCfg.gapSh[1]**2 )\n",
    "#        preKernel[:,0:DCfg.gapSh[1]] *= prof\n",
    "#        preKernel[:,DCfg.gapSh[1]+1:]  *= prof\n",
    "#        preKernel[0:DCfg.gapSh[1],:] *= prof.unsqueeze(1)\n",
    "#        preKernel[DCfg.gapSh[1]+1:,:]  *= prof.unsqueeze(1)\n",
    "#        preKernel = preKernel.unsqueeze(0).unsqueeze(0)\n",
    "#        img101 = torch.ones(DCfg.sinoSh,device=images.device)\n",
    "#        preWeights = preConv(img101).to(images.device)\n",
    "#    else :\n",
    "#        preKernel = preKernel.to(images.device)\n",
    "#        if not preWeights is None :\n",
    "#            preWeights = preWeights.to(images.device)\n",
    "#\n",
    "#    res = torch.nn.functional.conv2d(input[...,],preKernel,padding=(DCfg.gapSh[1],0))\n",
    "#    if len(images.shape) < 4 :\n",
    "#        res = res.squeeze(1)\n",
    "#    if len(images.shape) < 3 :\n",
    "#        res = res.squeeze(0)\n",
    "#    if not preWeights is None :\n",
    "#        res /= preWeights\n",
    "#    return res\n",
    "#\n",
    "\n",
    "\n",
    "preKernels = {}\n",
    "preWeights = None\n",
    "def preConv(images) :\n",
    "    global preKernels, preWeights\n",
    "    input = images[...,DCfg.gapSh[1]:-DCfg.gapSh[1]].clone()\n",
    "    input.to(images.device)\n",
    "    if len(input.shape) == 2 :\n",
    "        input = input.unsqueeze(0)\n",
    "    if len(input.shape) == 3 :\n",
    "        input = input.unsqueeze(1)\n",
    "    input[...,DCfg.gapSh[1]:-DCfg.gapSh[1]] = 0\n",
    "    if not preKernels :\n",
    "        for ln in range( 0, DCfg.gapSh[1]//2 + DCfg.gapSh[1]%2) :\n",
    "            rd = 1 + 2*ln\n",
    "            sz = 1 + 2*rd\n",
    "            preKernel = torch.ones((sz,sz), device=images.device)\n",
    "            prof = torch.zeros(rd, device=images.device)\n",
    "            for pp in range(rd) :\n",
    "                prof[pp] = math.exp( - (pp+1)**2 / rd**2 )\n",
    "            preKernel[:,0:rd] *= prof\n",
    "            preKernel[:,rd+1:]  *= prof\n",
    "            preKernel[0:rd,:] *= prof.unsqueeze(1)\n",
    "            preKernel[rd+1:,:]  *= prof.unsqueeze(1)\n",
    "            preKernels[ln] = preKernel.unsqueeze(0).unsqueeze(0)\n",
    "        #preKernel = torch.sqrt(preKernel).unsqueeze(0).unsqueeze(0)\n",
    "        img101 = torch.ones(DCfg.sinoSh,device=images.device)\n",
    "        preWeights = preConv(img101).to(images.device)\n",
    "    else :\n",
    "        for sz in preKernels :\n",
    "            preKernels[sz] = preKernels[sz].to(images.device)\n",
    "        if not preWeights is None :\n",
    "            preWeights = preWeights.to(images.device)\n",
    "\n",
    "    res = torch.empty((input.shape[0], input.shape[1], *DCfg.gapSh), device=images.device)\n",
    "    for lc in range(0,DCfg.gapSh[1]) :\n",
    "        ln = min(lc, DCfg.gapSh[1]-lc-1)\n",
    "        rd = 1 + 2*ln\n",
    "        preKernel = preKernels[ln]\n",
    "        inputln = input[...,DCfg.gapSh[1]+lc-rd:DCfg.gapSh[1]+lc+rd+1]\n",
    "        resln = torch.nn.functional.conv2d(inputln,preKernel,padding=(rd,0))\n",
    "        res[...,lc] = resln.squeeze(3)\n",
    "    #res = torch.nn.functional.conv2d(input,preKernel,padding=(DCfg.gapSh[1],0))\n",
    "    if len(images.shape) < 4 :\n",
    "        res = res.squeeze(1)\n",
    "    if len(images.shape) < 3 :\n",
    "        res = res.squeeze(0)\n",
    "    if not preWeights is None :\n",
    "        res /= preWeights\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Show</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3025965392589569 0.23866863548755646 0.7887355089187622 -0.16866731643676758 0.5188634395599365\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "refImages = None\n",
    "refNoises = None\n",
    "\n",
    "def showMe() :\n",
    "    global refImages, refNoises\n",
    "    randIdx = random.randint(0,len(trainSet)-1) # 835526 735307\n",
    "    image = trainSet[randIdx].squeeze()\n",
    "    tensorStat(image)\n",
    "    image = image.to(TCfg.device)\n",
    "\n",
    "    #start = time.time()\n",
    "    patch = preConv(image)\n",
    "    #print(start - time.time())\n",
    "    pimage = image.clone()\n",
    "    pimage[...,*DCfg.gapRng] = patch\n",
    "    plotImages([image.transpose(0,1).cpu(),pimage.transpose(0,1).cpu()])\n",
    "\n",
    "    #tifffile.imwrite(f\"tmp_{TCfg.exec}.tif\", image.numpy())\n",
    "    refImages = torch.stack((image,pimage)).to(TCfg.device)\n",
    "    refNoises = torch.randn((2,TCfg.latentDim)).to(TCfg.device)\n",
    "\n",
    "showMe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## <font style=\"color:lightblue\">Models</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillWheights(seq) :\n",
    "    for wh in seq :\n",
    "        if hasattr(wh, 'weight') :\n",
    "            torch.nn.init.xavier_uniform_(wh.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Save/Load model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_path):\n",
    "    if not device == 'cpu':\n",
    "        model.to('cpu')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    if not device == 'cpu':\n",
    "        model.to(device)\n",
    "    return\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def addToHDF(filename, containername, data) :\n",
    "    if len(data.shape) == 2 :\n",
    "        data=np.expand_dims(data, 0)\n",
    "    if len(data.shape) != 3 :\n",
    "        raise Exception(f\"Not appropriate input array size {data.shape}.\")\n",
    "\n",
    "    with h5py.File(filename,'a') as file :\n",
    "\n",
    "        if  containername not in file.keys():\n",
    "            dset = file.create_dataset(containername, data.shape,\n",
    "                                       maxshape=(None,data.shape[1],data.shape[2]),\n",
    "                                       dtype='f')\n",
    "            dset[()] = data\n",
    "            return\n",
    "\n",
    "        dset = file[containername]\n",
    "        csh = dset.shape\n",
    "        if csh[1] != data.shape[1] or csh[2] != data.shape[2] :\n",
    "            raise Exception(f\"Shape mismatch: input {data.shape}, file {dset.shape}.\")\n",
    "        msh = dset.maxshape\n",
    "        newLen = csh[0] + data.shape[0]\n",
    "        if msh[0] is None or msh[0] >= newLen :\n",
    "            dset.resize(newLen, axis=0)\n",
    "        else :\n",
    "            raise Exception(f\"Insufficient maximum shape {msh} to add data\"\n",
    "                            f\" {data.shape} to current volume {dset.shape}.\")\n",
    "        dset[csh[0]:newLen,...] = data\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Generator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Generator                                [2, 10, 10]               --\n",
      "├─Sequential: 1-1                        [2, 63, 10, 10]           --\n",
      "│    └─Linear: 2-1                       [2, 6300]                 409,500\n",
      "│    └─ReLU: 2-2                         [2, 6300]                 --\n",
      "│    └─Unflatten: 2-3                    [2, 63, 10, 10]           --\n",
      "├─Sequential: 1-2                        [2, 1, 10, 2]             --\n",
      "│    └─Sequential: 2-4                   [2, 512, 4, 4]            --\n",
      "│    │    └─Conv2d: 3-1                  [2, 128, 8, 8]            73,856\n",
      "│    │    └─LeakyReLU: 3-2               [2, 128, 8, 8]            --\n",
      "│    │    └─Conv2d: 3-3                  [2, 256, 6, 6]            295,168\n",
      "│    │    └─LeakyReLU: 3-4               [2, 256, 6, 6]            --\n",
      "│    │    └─Conv2d: 3-5                  [2, 512, 4, 4]            1,180,160\n",
      "│    │    └─LeakyReLU: 3-6               [2, 512, 4, 4]            --\n",
      "│    └─Sequential: 2-5                   [2, 512, 4, 4]            --\n",
      "│    │    └─Flatten: 3-7                 [2, 8192]                 --\n",
      "│    │    └─Linear: 3-8                  [2, 8192]                 67,117,056\n",
      "│    │    └─LeakyReLU: 3-9               [2, 8192]                 --\n",
      "│    │    └─Unflatten: 3-10              [2, 512, 4, 4]            --\n",
      "│    └─Sequential: 2-6                   [2, 1, 10, 2]             --\n",
      "│    │    └─ConvTranspose2d: 3-11        [2, 256, 6, 4]            393,472\n",
      "│    │    └─LeakyReLU: 3-12              [2, 256, 6, 4]            --\n",
      "│    │    └─ConvTranspose2d: 3-13        [2, 128, 8, 4]            98,432\n",
      "│    │    └─LeakyReLU: 3-14              [2, 128, 8, 4]            --\n",
      "│    │    └─ConvTranspose2d: 3-15        [2, 64, 10, 4]            24,640\n",
      "│    │    └─LeakyReLU: 3-16              [2, 64, 10, 4]            --\n",
      "│    │    └─Conv2d: 3-17                 [2, 1, 10, 2]             193\n",
      "│    │    └─Tanh: 3-18                   [2, 1, 10, 2]             --\n",
      "==========================================================================================\n",
      "Total params: 69,592,477\n",
      "Trainable params: 69,592,477\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 230.69\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.85\n",
      "Params size (MB): 278.37\n",
      "Estimated Total Size (MB): 279.22\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.noise2latent = nn.Sequential(\n",
    "            nn.Linear(TCfg.latentDim, DCfg.sinoSize*63),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten( 1, (63,) + DCfg.sinoSh )\n",
    "        )\n",
    "        fillWheights(self.noise2latent)\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "\n",
    "        )\n",
    "        fillWheights(self.encode)\n",
    "\n",
    "\n",
    "        self.link = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8192, 8192),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (512, 4, 4)),\n",
    "        )\n",
    "        fillWheights(self.link)\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 1, (1,3)),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "        fillWheights(self.decode)\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            self.encode,\n",
    "            self.link,\n",
    "            self.decode\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        images, noises = input\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        latent = self.noise2latent(noises)\n",
    "        modelIn = torch.cat((images,latent),dim=1)\n",
    "        modelIn[:,0,*DCfg.gapRng] = preConv(images[:,0,:,:])\n",
    "        patches = self.body(modelIn)\n",
    "        #return patches\n",
    "        mIn = modelIn[:,0,*DCfg.gapRng].unsqueeze(1)\n",
    "        #tensorStat(mIn[:,DCfg.gapSh[1]:-DCfg.gapSh[1]])\n",
    "        #tensorStat(patches[:,DCfg.gapSh[1]:-DCfg.gapSh[1]])\n",
    "        patches = mIn + torch.where( patches < 0 , patches * (mIn+0.5) , patches )\n",
    "        #tensorStat(patches[:,DCfg.gapSh[1]:-DCfg.gapSh[1]])\n",
    "        modelOut = torch.cat((images[:,:,:,:DCfg.gapRngX.start], patches, images[:,:,:,DCfg.gapRngX.stop:]),dim=3)\n",
    "        return modelOut.squeeze(1)\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "#generator = load_model(generator, model_path=f\"model_{TCfg.exec}_gen.pt\" )\n",
    "generator.to(TCfg.device)\n",
    "model_summary = summary(generator, input_data=[ [refImages, refNoises] ] ).__str__()\n",
    "print(model_summary)\n",
    "try : os.remove(TCfg.historyHDF)\n",
    "except : pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Discriminator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Discriminator                            [2, 1]                    --\n",
      "├─Sequential: 1-1                        [2, 512, 4, 4]            --\n",
      "│    └─Conv2d: 2-1                       [2, 128, 8, 8]            1,280\n",
      "│    └─LeakyReLU: 2-2                    [2, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-3                       [2, 256, 6, 6]            295,168\n",
      "│    └─LeakyReLU: 2-4                    [2, 256, 6, 6]            --\n",
      "│    └─Conv2d: 2-5                       [2, 512, 4, 4]            1,180,160\n",
      "│    └─LeakyReLU: 2-6                    [2, 512, 4, 4]            --\n",
      "├─Sequential: 1-2                        [2, 1]                    --\n",
      "│    └─Flatten: 2-7                      [2, 8192]                 --\n",
      "│    └─Dropout: 2-8                      [2, 8192]                 --\n",
      "│    └─Linear: 2-9                       [2, 128]                  1,048,704\n",
      "│    └─Linear: 2-10                      [2, 1]                    129\n",
      "│    └─Sigmoid: 2-11                     [2, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 2,525,441\n",
      "Trainable params: 2,525,441\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 61.28\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.41\n",
      "Params size (MB): 10.10\n",
      "Estimated Total Size (MB): 10.51\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, 128, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.body)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(8192, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        fillWheights(self.head)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        modelIn = images.clone()\n",
    "        modelIn[...,0:DCfg.gapSh[1],DCfg.gapRngX] = 0 # torch.randn(images.shape[0],1,DCfg.gapSh[1],DCfg.gapSh[1])\n",
    "        modelIn[...,-DCfg.gapSh[1]:,DCfg.gapRngX] = 0 # torch.randn(images.shape[0],1,DCfg.gapSh[1],DCfg.gapSh[1])\n",
    "        convRes = self.body(modelIn)\n",
    "        res = self.head(convRes)\n",
    "        return res\n",
    "\n",
    "discriminator = Discriminator()\n",
    "#discriminator = load_model(discriminator, model_path=f\"model_{TCfg.exec}_dis.pt\" )\n",
    "discriminator.to(TCfg.device)\n",
    "model_summary = summary(discriminator, input_data=refImages ).__str__()\n",
    "print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5358107 0.5358412]\n",
      "-0.04043395072221756 0.011804810725152493 -0.2919529378414154 -0.056030094623565674 -0.022628791630268097\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.eval()\n",
    "discriminator.eval()\n",
    "mgens = generator([refImages,refNoises])\n",
    "liks = discriminator(mgens)\n",
    "print(liks.detach().cpu().squeeze().numpy())\n",
    "tensorStat( mgens[1,*DCfg.disRng] - refImages[1,*DCfg.disRng] )\n",
    "plotImages((mgens[0,...].detach().cpu().squeeze().transpose(0,1),\n",
    "            mgens[1,...].detach().cpu().squeeze().transpose(0,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Train</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE = nn.BCELoss(reduction='none')\n",
    "\n",
    "def loss_func(y_true, y_pred, weights=None):\n",
    "    loss = BCE(y_pred, y_true)\n",
    "    summ = len(loss)\n",
    "    if not weights is None :\n",
    "        loss *= weights.unsqueeze(1)\n",
    "        summ = weights.sum()\n",
    "    return loss.sum() / summ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Optimizers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_G = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=TCfg.learningRateG,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=TCfg.learningRateD,\n",
    "    betas=(0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Train step</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images(images) :\n",
    "    noise = torch.randn(images.shape[0], TCfg.latentDim).to(TCfg.device)\n",
    "    outImages = generator((images, noise))\n",
    "    return outImages\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class TrainInfo:\n",
    "    bestRealImage = None\n",
    "    bestRealPrediction = 0\n",
    "    bestRealWght = 0\n",
    "    worstRealImage = None\n",
    "    worstRealPrediction = 0\n",
    "    worstRealWght = 0\n",
    "    bestFakeImage = None\n",
    "    bestFakePrediction = 0\n",
    "    bestFakeWght = 0\n",
    "    worstFakeImage = None\n",
    "    worstFakePrediction = 0\n",
    "    worstFakeWght = 0\n",
    "    ratReal = 0.0\n",
    "    ratFake = 0.0\n",
    "    totalImages = 0\n",
    "    iterations = 0\n",
    "    disPerformed = 0\n",
    "    genPerformed = 0\n",
    "\n",
    "\n",
    "trainDis = True\n",
    "trainGen = True\n",
    "def train_step(images):\n",
    "    global trainDis, trainGen\n",
    "    TrainInfo.iterations += 1\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    nofIm = images.shape[0]\n",
    "    images = images.squeeze(1).to(TCfg.device)\n",
    "    means = images.mean(dim=(1,2)).squeeze() + 0.5\n",
    "    stdds = images.std(dim=(1,2)).squeeze()\n",
    "    absWeights = means + 1\n",
    "    absWeights /= absWeights.sum()\n",
    "    disWeights = stdds / means\n",
    "    disWeights /= disWeights.sum()\n",
    "    weights = absWeights + disWeights\n",
    "    weights /= weights.sum()\n",
    "    fakeImages = generate_images(images)\n",
    "\n",
    "    generator.eval()\n",
    "    if trainDis :\n",
    "        discriminator.train()\n",
    "        optimizer_D.zero_grad()\n",
    "    y_pred_real = discriminator(images)\n",
    "    y_pred_fake = discriminator(fakeImages)\n",
    "    y_pred_both = torch.cat((y_pred_real, y_pred_fake), dim=0)\n",
    "    labels = torch.cat( (\n",
    "        torch.full((nofIm, 1),  TCfg.labelSmoothFac),\n",
    "        torch.zeros(nofIm, 1) ),\n",
    "        dim=0\n",
    "    ).to(TCfg.device)\n",
    "    D_loss = None\n",
    "    dweights = torch.cat( (weights, weights) )\n",
    "    if trainDis :\n",
    "        TrainInfo.disPerformed += 1\n",
    "        D_loss = loss_func(labels, y_pred_both, dweights )\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "    else :\n",
    "        with torch.no_grad():\n",
    "            D_loss = loss_func(labels, y_pred_both, torch.cat( (weights, weights) ))\n",
    "\n",
    "    ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "    ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "    trainGen = ratReal > 0.5 and ratFake < 0.5\n",
    "\n",
    "    G_loss = None\n",
    "    if trainGen:\n",
    "        discriminator.eval()\n",
    "        if trainGen :\n",
    "            generator.train()\n",
    "            optimizer_G.zero_grad()\n",
    "        fakeImages = generate_images(images)\n",
    "        y_pred_fake = discriminator(fakeImages)\n",
    "        labels = torch.ones(nofIm, 1).to(TCfg.device)\n",
    "        if trainGen :\n",
    "            TrainInfo.genPerformed += 1\n",
    "            G_loss = loss_func(labels, y_pred_fake, weights)\n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        trainDis = ratFake > 0.1\n",
    "    else :\n",
    "        labels = torch.ones(nofIm, 1).to(TCfg.device)\n",
    "        with torch.no_grad() :\n",
    "            G_loss = loss_func(labels, y_pred_fake, weights)\n",
    "        optimizer_D.zero_grad()\n",
    "        trainDis = True\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    idx = y_pred_real.argmax()\n",
    "    TrainInfo.bestRealImage = images[idx,...]\n",
    "    TrainInfo.bestRealPrediction = y_pred_real[idx].item()\n",
    "    TrainInfo.bestRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_real.argmin()\n",
    "    TrainInfo.worstRealImage = images[idx,...]\n",
    "    TrainInfo.worstRealPrediction =  y_pred_real[idx].item()\n",
    "    TrainInfo.worstRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmax()\n",
    "    TrainInfo.bestFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.bestFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.bestFakeWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmin()\n",
    "    TrainInfo.worstFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.worstFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.worstFakeWght = nofIm*weights[idx]\n",
    "    TrainInfo.ratReal += ratReal * nofIm\n",
    "    TrainInfo.ratFake += ratFake * nofIm\n",
    "    TrainInfo.totalImages += nofIm\n",
    "\n",
    "    return D_loss, G_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Trainer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "G_LOSS = []\n",
    "D_LOSS = []\n",
    "\n",
    "def train(dataloader, epochs):\n",
    "\n",
    "    discriminator.to(TCfg.device)\n",
    "    generator.to(TCfg.device)\n",
    "    refImages.to(TCfg.device)\n",
    "    refNoises.to(TCfg.device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        start = time.time()\n",
    "        lastUpdateTime = start\n",
    "        D_loss_list, G_loss_list = [], []\n",
    "\n",
    "        pit = -1\n",
    "        for it , images in tqdm.tqdm(enumerate(dataloader), total=int(len(dataloader))):\n",
    "\n",
    "            images = images.to(TCfg.device)\n",
    "            D_loss, G_loss = train_step(images)\n",
    "            D_loss_list.append(D_loss)\n",
    "            G_loss_list.append(G_loss)\n",
    "\n",
    "            #if not it or it == len(dataloader)-1 or time.time() - lastUpdateTime > 60 :\n",
    "            if it == len(dataloader)-1:\n",
    "                lastUpdateTime = time.time()\n",
    "                its = it-pit\n",
    "                print(f\"Epoch: {epoch}.\"\n",
    "                      f\" Discriminator loss: {D_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratReal/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.disPerformed/TrainInfo.iterations:.3f}), \"\n",
    "                      f\" Generator loss: {G_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratFake/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.genPerformed/TrainInfo.iterations:.3f}).\")\n",
    "                TrainInfo.iterations = 0\n",
    "                TrainInfo.totalImages = 0\n",
    "                TrainInfo.ratReal = 0\n",
    "                TrainInfo.ratFake = 0\n",
    "                TrainInfo.genPerformed = 0\n",
    "                TrainInfo.disPerformed = 0\n",
    "                pit = it\n",
    "                fourImages = np.ones( (2*DCfg.sinoSh[1] + DCfg.gapSh[1] ,\n",
    "                                       3*DCfg.sinoSh[0] + 2*DCfg.gapSh[1]), dtype=np.float32  )\n",
    "                def addImage(clmn, row, img) :\n",
    "                    fourImages[ row * ( DCfg.sinoSh[1]+DCfg.gapSh[1]) : (row+1) * DCfg.sinoSh[1] + row*DCfg.gapSh[1] ,\n",
    "                                clmn * ( DCfg.sinoSh[0]+DCfg.gapSh[1]) : (clmn+1) * DCfg.sinoSh[0] + clmn*DCfg.gapSh[1] ] = \\\n",
    "                        img.squeeze().transpose(0,1).detach().cpu().numpy()\n",
    "                refNoises[0,:] = torch.randn(TCfg.latentDim)\n",
    "                gens = generator([refImages,refNoises])\n",
    "                tensorStat( gens[0,*DCfg.disRng] - refImages[0,*DCfg.disRng] )\n",
    "                disIn = torch.cat(( gens[0,...].unsqueeze(0), refImages), dim=0)\n",
    "                liks = discriminator(disIn)\n",
    "                addImage(0,0,TrainInfo.bestRealImage)\n",
    "                addImage(0,1,TrainInfo.worstRealImage)\n",
    "                addImage(1,0,TrainInfo.bestFakeImage)\n",
    "                addImage(1,1,TrainInfo.worstFakeImage)\n",
    "                addImage(2,0,gens[0,...])\n",
    "                addImage(2,1,refImages[0,...])\n",
    "                print (f\"TT: {TrainInfo.bestRealPrediction:.4e} ({TrainInfo.bestRealWght:.3f}),  \"\n",
    "                       f\"FT: {TrainInfo.bestFakePrediction:.4e} ({TrainInfo.bestFakeWght:.3f}),  \"\n",
    "                       f\"GI: {liks[0].item():.5f},  \" )\n",
    "                print (f\"TF: {TrainInfo.worstRealPrediction:.4e} ({TrainInfo.worstRealWght:.3f}),  \"\n",
    "                       f\"FF: {TrainInfo.worstFakePrediction:.4e} ({TrainInfo.worstFakeWght:.3f}),  \"\n",
    "                       f\"RP: {liks[1].item():.5f}, {liks[2].item():.5f} \" )\n",
    "                try :\n",
    "                    #tifffile.imwrite(f\"tmp_{TCfg.exec}.tif\", fourImages)\n",
    "                    addToHDF(TCfg.historyHDF, \"data\", fourImages)\n",
    "                except :\n",
    "                    eprint(\"Failed to save.\")\n",
    "                plt.imshow(fourImages, cmap='gray')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "        epoch_D_loss = sum(D_loss_list)/len(D_loss_list)\n",
    "        epoch_G_loss = sum(G_loss_list)/len(G_loss_list)\n",
    "        D_LOSS.append(epoch_D_loss.detach().cpu())\n",
    "        G_LOSS.append(epoch_G_loss.detach().cpu())\n",
    "        print('\\n')\n",
    "        print(f\"Time for epoch {epoch + 1} is {time.time()-start} sec\")\n",
    "        print(f\"Discriminator loss: {epoch_D_loss:.3f}, Generator loss: {epoch_G_loss:.3f}.\")\n",
    "        print('\\n')\n",
    "        save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "        save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Execute</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 314/315 [03:13<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219. Discriminator loss: 0.689 (0.536 / 1.000),  Generator loss: 0.713 (0.465 / 0.289).\n",
      "-0.004643439315259457 0.011591440998017788 -2.496304988861084 -0.023614615201950073 0.0140647292137146\n",
      "TT: 5.9075e-01 (1.441),  FT: 5.8435e-01 (1.191),  GI: 0.50322,  \n",
      "TF: 4.0076e-01 (0.803),  FF: 3.4845e-01 (0.803),  RP: 0.49677, 0.83135 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [03:14<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for epoch 220 is 200.01395344734192 sec\n",
      "Discriminator loss: 0.688, Generator loss: 0.710.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 12/315 [00:08<03:35,  1.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m trainDis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#fillWheights(generator.body)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#fillWheights(discriminator.body)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#fillWheights(discriminator.head)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTCfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnofEpochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[86], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it , images \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataloader))):\n\u001b[1;32m     22\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(TCfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m     D_loss, G_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     D_loss_list\u001b[38;5;241m.\u001b[39mappend(D_loss)\n\u001b[1;32m     25\u001b[0m     G_loss_list\u001b[38;5;241m.\u001b[39mappend(G_loss)\n",
      "Cell \u001b[0;32mIn[85], line 67\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     65\u001b[0m     TrainInfo\u001b[38;5;241m.\u001b[39mdisPerformed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     66\u001b[0m     D_loss \u001b[38;5;241m=\u001b[39m loss_func(labels, y_pred_both, dweights )\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mD_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    dataset=trainSet,\n",
    "    batch_size=TCfg.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "trainGen=True\n",
    "trainDis=True\n",
    "#fillWheights(generator.body)\n",
    "#fillWheights(discriminator.body)\n",
    "#fillWheights(discriminator.head)\n",
    "train(trainLoader, TCfg.nofEpochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Post</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randIdx = random.randint(0,len(testSet)-1)\n",
    "image = testSet[randIdx]\n",
    "plt.imshow(image.squeeze().transpose(0,1), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#image = image.to(TCfg.device)\n",
    "#fake_image = generate_images(image)\n",
    "#plt.imshow(fake_image.detach().squeeze().transpose(0,1), cmap='gray')\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "addToHDF(f\"test_{TCfg.exec}.hdf\", \"data\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n",
    "#save_model(generator, TCfg.device, model_path=\"saves/work_1.GEN.pt\")\n",
    "#save_model(discriminator, TCfg.device, model_path=\"saves/work_1.DIS.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
