{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Header</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread, imsave\n",
    "import h5py\n",
    "import tifffile\n",
    "import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "\n",
    "def plotData(dataY, rangeY=None, dataYR=None, rangeYR=None,\n",
    "             dataX=None, rangeX=None, rangeP=None,\n",
    "             figsize=(16,8), saveTo=None, show=True):\n",
    "\n",
    "    if type(dataY) is np.ndarray :\n",
    "        plotData((dataY,), rangeY=rangeY, dataYR=dataYR, rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataYR) is np.ndarray :\n",
    "        plotData(dataY, rangeY=rangeY, dataYR=(dataYR,), rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataY) is not tuple :\n",
    "        eprint(f\"Unknown data type to plot: {type(dataY)}.\")\n",
    "        return\n",
    "    if type(dataYR) is not tuple and dataYR is not None:\n",
    "        eprint(f\"Unknown data type to plot: {type(dataYR)}.\")\n",
    "        return\n",
    "\n",
    "    last = min( len(data) for data in dataY )\n",
    "    if dataYR is not None:\n",
    "        last = min( last,  min( len(data) for data in dataYR ) )\n",
    "    if dataX is not None:\n",
    "        last = min(last, len(dataX))\n",
    "    if rangeP is None :\n",
    "        rangeP = (0,last)\n",
    "    elif type(rangeP) is int :\n",
    "        rangeP = (0,rangeP) if rangeP > 0 else (-rangeP,last)\n",
    "    elif type(rangeP) is tuple :\n",
    "        rangeP = ( 0    if rangeP[0] is None else rangeP[0],\n",
    "                   last if rangeP[1] is None else rangeP[1],)\n",
    "    else :\n",
    "        eprint(f\"Bad data type on plotData input rangeP: {type(rangeP)}\")\n",
    "        raise Exception(f\"Bug in the code.\")\n",
    "    rangeP = np.s_[ max(0, rangeP[0]) : min(last, rangeP[1]) ]\n",
    "    if dataX is None :\n",
    "        dataX = np.arange(rangeP.start, rangeP.stop)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax1.xaxis.grid(True, 'both', linestyle='dotted')\n",
    "    if rangeX is not None :\n",
    "        ax1.set_xlim(rangeX)\n",
    "    else :\n",
    "        ax1.set_xlim(rangeP.start,rangeP.stop-1)\n",
    "\n",
    "    ax1.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "    nofPlots = len(dataY)\n",
    "    if rangeY is not None:\n",
    "        ax1.set_ylim(rangeY)\n",
    "    colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "    for idx , data in enumerate(dataY):\n",
    "        ax1.plot(dataX, data[rangeP], linestyle='-',  color=colors[idx])\n",
    "\n",
    "    if dataYR is not None : # right Y axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "        nofPlots = len(dataYR)\n",
    "        if rangeYR is not None:\n",
    "            ax2.set_ylim(rangeYR)\n",
    "        colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "        for idx , data in enumerate(dataYR):\n",
    "            ax2.plot(dataX, data[rangeP], linestyle='dashed',  color=colors[idx])\n",
    "\n",
    "    if saveTo:\n",
    "        fig.savefig(saveTo)\n",
    "    if not show:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def plotImage(image) :\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotImages(images) :\n",
    "    for i, img in enumerate(images) :\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sliceShape(shape, sl) :\n",
    "    if type(shape) is int :\n",
    "        shape = torch.Size([shape])\n",
    "    if type(sl) is tuple :\n",
    "        if len(shape) != len(sl) :\n",
    "            raise Exception(f\"Different sizes of shape {shape} and sl {sl}\")\n",
    "        out = []\n",
    "        for i in range(0, len(shape)) :\n",
    "            indeces = sl[i].indices(shape[i])\n",
    "            out.append(indeces[1]-indeces[0])\n",
    "        return out\n",
    "    elif type(sl) is slice :\n",
    "        indeces = sl.indices(shape[0])\n",
    "        return indeces[1]-indeces[0]\n",
    "    else :\n",
    "        raise Exception(f\"Incompatible object {sl}\")\n",
    "\n",
    "\n",
    "def tensorStat(stat) :\n",
    "    print(f\"{stat.mean().item():.3e}, {stat.std().item():.3e}, \"\n",
    "          f\"{stat.min().item():.3e}, {stat.max().item():.3e}\")\n",
    "\n",
    "\n",
    "def fillWheights(seq) :\n",
    "    for wh in seq :\n",
    "        if hasattr(wh, 'weight') :\n",
    "            torch.nn.init.xavier_uniform_(wh.weight)\n",
    "\n",
    "\n",
    "def unsqeeze4dim(tens):\n",
    "    orgDims = tens.dim()\n",
    "    if tens.dim() == 2 :\n",
    "        tens = tens.unsqueeze(0)\n",
    "    if tens.dim() == 3 :\n",
    "        tens = tens.unsqueeze(1)\n",
    "    return tens, orgDims\n",
    "\n",
    "\n",
    "def squeezeOrg(tens, orgDims):\n",
    "    if orgDims == tens.dim():\n",
    "        return tens\n",
    "    if tens.dim() != 4 or orgDims > 4 or orgDims < 2:\n",
    "        raise Exception(f\"Unexpected dimensions to squeeze: {tens.dim()} {orgDims}.\")\n",
    "    if orgDims < 4 :\n",
    "        if tens.shape[1] > 1:\n",
    "            raise Exception(f\"Cant squeeze dimension 1 in: {tens.shape}.\")\n",
    "        tens = tens.squeeze(1)\n",
    "    if orgDims < 3 :\n",
    "        if tens.shape[0] > 1:\n",
    "            raise Exception(f\"Cant squeeze dimension 0 in: {tens.shape}.\")\n",
    "        tens = tens.squeeze(0)\n",
    "    return tens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Configs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED_VALUE):\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "\n",
    "seed = 7\n",
    "set_seed(seed)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TCfg:\n",
    "    exec = 1\n",
    "    device: torch.device = f\"cuda:{exec}\"\n",
    "    nofEpochs: int = 2048\n",
    "    latentDim: int = 64\n",
    "    batchSize: int = 16384\n",
    "    labelSmoothFac: float = 0.1 # For Fake labels (or set to 0.0 for no smoothing).\n",
    "    learningRateD: float = 0.00002\n",
    "    learningRateG: float = 0.00002\n",
    "    historyHDF = f\"train_{exec}.hdf\"\n",
    "\n",
    "class DCfg:\n",
    "    gapW = 4\n",
    "    sinoSh = (5*gapW,5*gapW) # 20,20\n",
    "    readSh = (8*sinoSh[0], 8*sinoSh[0]) # 80,80\n",
    "    sinoSize = math.prod(sinoSh)\n",
    "    gapSh = (sinoSh[0],gapW)\n",
    "    gapSize = math.prod(gapSh)\n",
    "    gapRngX = np.s_[ sinoSh[1]//2 - gapW//2 : sinoSh[1]//2 + gapW//2 ]\n",
    "    gapRng = np.s_[ : , gapRngX ]\n",
    "    disRng = np.s_[ gapW:-gapW , gapRngX ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Save/Load</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_path):\n",
    "    if not device == 'cpu':\n",
    "        model.to('cpu')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    if not device == 'cpu':\n",
    "        model.to(device)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def saveCheckPoint(path, epoch,\n",
    "                   generator, discriminator,\n",
    "                   optimizerGen=None, optimizerDis=None,\n",
    "                   schedulerGen=None, schedulerDis=None,\n",
    "                   device=None ) :\n",
    "    generator.to('cpu')\n",
    "    discriminator.to('cpu')\n",
    "    checkPoint = {}\n",
    "    checkPoint['epoch'] = epoch\n",
    "    checkPoint['generator'] = generator.state_dict()\n",
    "    checkPoint['discriminator'] = discriminator.state_dict()\n",
    "    if not optimizerGen is None :\n",
    "        checkPoint['optimizerGen'] = optimizerGen.state_dict()\n",
    "    if not schedulerGen is None :\n",
    "        checkPoint['schedulerGen'] = schedulerGen.state_dict()\n",
    "    if not optimizerDis is None :\n",
    "        checkPoint['optimizerDis'] = optimizerDis.state_dict()\n",
    "    if not schedulerDis is None :\n",
    "        checkPoint['schedulerDis'] = schedulerDis.state_dict()\n",
    "    torch.save(checkPoint, path)\n",
    "    if not device is None:\n",
    "        generator.to(device)\n",
    "        discriminator.to(device)\n",
    "\n",
    "\n",
    "def loadCheckPoint(path, generator, discriminator,\n",
    "                   optimizerGen=None, optimizerDis=None,\n",
    "                   schedulerGen=None, schedulerDis=None,\n",
    "                   device=None ) :\n",
    "    checkPoint = torch.load(path)\n",
    "    epoch = checkPoint['epoch']\n",
    "    generator.load_state_dict(checkPoint['generator'])\n",
    "    discriminator.load_state_dict(checkPoint['discriminator'])\n",
    "    if not device is None :\n",
    "        generator.to(device)\n",
    "        discriminator.to(device)\n",
    "    if not optimizerGen is None :\n",
    "        optimizerGen.load_state_dict(checkPoint['optimizerGen'])\n",
    "    if not schedulerGen is None :\n",
    "        schedulerGen.load_state_dict(checkPoint['schedulerGen'])\n",
    "    if not optimizerDis is None :\n",
    "        optimizerDis.load_state_dict(checkPoint['optimizerDis'])\n",
    "    if not schedulerDis is None :\n",
    "        schedulerDis.load_state_dict(checkPoint['schedulerDis'])\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def addToHDF(filename, containername, data) :\n",
    "    if len(data.shape) == 2 :\n",
    "        data=np.expand_dims(data, 0)\n",
    "    if len(data.shape) != 3 :\n",
    "        raise Exception(f\"Not appropriate input array size {data.shape}.\")\n",
    "\n",
    "    with h5py.File(filename,'a') as file :\n",
    "\n",
    "        if  containername not in file.keys():\n",
    "            dset = file.create_dataset(containername, data.shape,\n",
    "                                       maxshape=(None,data.shape[1],data.shape[2]),\n",
    "                                       dtype='f')\n",
    "            dset[()] = data\n",
    "            return\n",
    "\n",
    "        dset = file[containername]\n",
    "        csh = dset.shape\n",
    "        if csh[1] != data.shape[1] or csh[2] != data.shape[2] :\n",
    "            raise Exception(f\"Shape mismatch: input {data.shape}, file {dset.shape}.\")\n",
    "        msh = dset.maxshape\n",
    "        newLen = csh[0] + data.shape[0]\n",
    "        if msh[0] is None or msh[0] >= newLen :\n",
    "            dset.resize(newLen, axis=0)\n",
    "        else :\n",
    "            raise Exception(f\"Insufficient maximum shape {msh} to add data\"\n",
    "                            f\" {data.shape} to current volume {dset.shape}.\")\n",
    "        dset[csh[0]:newLen,...] = data\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Raw Read</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StripesFromHDF :\n",
    "\n",
    "    def __init__(self, sampleName, maskName, bgName=None, dfName=None, loadToMem=True):\n",
    "\n",
    "        sampleHDF = sampleName.split(':')\n",
    "        if len(sampleHDF) != 2 :\n",
    "            raise Exception(f\"String \\\"{sampleName}\\\" does not represent an HDF5 format.\")\n",
    "        with h5py.File(sampleHDF[0],'r') as trgH5F:\n",
    "            if  sampleHDF[1] not in trgH5F.keys():\n",
    "                raise Exception(f\"No dataset '{sampleHDF[1]}' in input file {sampleHDF[0]}.\")\n",
    "            self.data = trgH5F[sampleHDF[1]]\n",
    "            if not self.data.size :\n",
    "                raise Exception(f\"Container \\\"{sampleName}\\\" is zero size.\")\n",
    "            self.sh = self.data.shape\n",
    "            if len(self.sh) != 3 :\n",
    "                raise Exception(f\"Dimensions of the container \\\"{sampleName}\\\" is not 3 {self.sh}.\")\n",
    "            self.fsh = self.sh[1:3]\n",
    "            self.volume = None\n",
    "            if loadToMem :\n",
    "                self.volume = np.empty(self.sh, dtype=np.float32)\n",
    "                self.data.read_direct(self.volume)\n",
    "                trgH5F.close()\n",
    "\n",
    "            def loadImage(imageName) :\n",
    "                if not imageName:\n",
    "                    return None\n",
    "                imdata = imread(imageName).astype(np.float32)\n",
    "                if len(imdata.shape) == 3 :\n",
    "                    imdata = np.mean(imdata[:,:,0:3], 2)\n",
    "                if imdata.shape != self.fsh :\n",
    "                    raise Exception(f\"Dimensions of the input image \\\"{imageName}\\\" {imdata.shape} \"\n",
    "                                    f\"do not match the face of the container \\\"{sampleName}\\\" {self.fsh}.\")\n",
    "                return imdata\n",
    "\n",
    "            self.mask = loadImage(maskName)\n",
    "            if self.mask is None :\n",
    "                self.mask = np.ones(self.fsh, dtype=np.uint8)\n",
    "            self.mask = self.mask.astype(bool)\n",
    "            self.bg = loadImage(bgName)\n",
    "            self.df = loadImage(dfName)\n",
    "            if self.bg is not None :\n",
    "                if self.df is not None:\n",
    "                    self.bg -= self.df\n",
    "                self.mask  &=  self.bg > 0.0\n",
    "\n",
    "            self.allIndices = []\n",
    "            for yCr in range(0,self.fsh[0]) :\n",
    "                for xCr in range(0,self.fsh[1]) :\n",
    "                    idx = np.s_[yCr,xCr]\n",
    "                    if self.mask[idx] :\n",
    "                        if self.volume is not None :\n",
    "                            if self.df is not None :\n",
    "                                self.volume[:,*idx] -= self.df[idx]\n",
    "                            if self.bg is not None :\n",
    "                                self.volume[:,*idx] /= self.bg[idx]\n",
    "                        if  xCr + DCfg.readSh[1] < self.fsh[1] \\\n",
    "                        and np.all( self.mask[yCr,xCr+1:xCr+DCfg.readSh[1]] ) :\n",
    "                            self.allIndices.append(idx)\n",
    "\n",
    "    def get_dataset(self, transform=None) :\n",
    "\n",
    "        class Sinos(torch.utils.data.Dataset) :\n",
    "\n",
    "            def __init__(self, root, transform=None):\n",
    "                self.container = root\n",
    "                self.transform = transforms.Compose([transforms.ToTensor(), transform]) \\\n",
    "                    if transform else transforms.ToTensor()\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.container.allIndices)\n",
    "\n",
    "            def __getitem__(self, index, idxs=None):\n",
    "                if idxs is None :\n",
    "                    idxs = random.randint(0,self.container.sh[0]-DCfg.readSh[0]-1)\n",
    "                idx = self.container.allIndices[index]\n",
    "                xyrng=np.s_[ idx[0], idx[1]:idx[1]+DCfg.readSh[1] ]\n",
    "                if self.container.volume is not None :\n",
    "                    data = self.container.volume[idxs:idxs+DCfg.readSh[0], *xyrng]\n",
    "                else :\n",
    "                    data = self.container.data[idxs:idxs+DCfg.readSh[0], *xyrng]\n",
    "                    if self.container.df is not None :\n",
    "                        data -= self.container.df[None,*xyrng]\n",
    "                    if self.container.bg is not None :\n",
    "                        data /= self.container.bg[None,*xyrng]\n",
    "                if self.transform :\n",
    "                    data = self.transform(data)\n",
    "                return (data, index, idxs)\n",
    "\n",
    "        return Sinos(self, transform)\n",
    "\n",
    "examples = {}\n",
    "\n",
    "#baseName = \"4176862R_Eig_Threshold-4keV\"\n",
    "#examples[2] = (1271101, 570) # (229247,541)\n",
    "#examples[4] = (962537,512) # (112073,759)  (129198, 800)\n",
    "\n",
    "baseName = \"18515.Lamb1_Eiger_7m_45keV_360Scan\"\n",
    "examples[2] = (171308, 848) # (34008,1620)\n",
    "examples[4] = (27643, 240)\n",
    "\n",
    "#baseName = \"21987.70keV_7m_Eiger_Calf3\"\n",
    "#examples[2] =\n",
    "\n",
    "sinoRoot = StripesFromHDF(f\"storage/{baseName}.hdf:/data\", f\"storage/{baseName}.mask.tif\",None, None)\n",
    "dataTransform =  transforms.Compose([\n",
    "    transforms.Resize(DCfg.sinoSh),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=(0.5), std=(1))\n",
    "])\n",
    "trainSet = sinoRoot.get_dataset(dataTransform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Show</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.171e-01, 2.589e-01, -1.987e-01, 4.734e-01\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "refImages = None\n",
    "refNoises = None\n",
    "\n",
    "def showMe() :\n",
    "    global refImages, refNoises\n",
    "    image = None\n",
    "    #if False :\n",
    "    if True :\n",
    "        image, _,_ = trainSet.__getitem__(*examples[DCfg.gapW])\n",
    "        #image, _,_ = trainSet.__getitem__(27643, 240)\n",
    "    else :\n",
    "        while True:\n",
    "            image, index, idxs = trainSet[random.randint(0,len(trainSet)-1)]\n",
    "            if image.mean() > 0 and image.min() < -0.1 :\n",
    "                print (f\"{index}, {idxs}\")\n",
    "                break\n",
    "    image = image.squeeze()\n",
    "    tensorStat(image)\n",
    "    plotImage(image)\n",
    "    image = image.to(TCfg.device)\n",
    "    refImages = torch.stack((image,image)).to(TCfg.device)\n",
    "    refNoises = torch.randn((2,TCfg.latentDim)).to(TCfg.device)\n",
    "\n",
    "showMe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## <font style=\"color:lightblue\">Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Lower resolution generators</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736151/3157356196.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generator2(\n",
       "  (noise2latent): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=700, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Unflatten(dim=1, unflattened_size=(7, 10, 10))\n",
       "  )\n",
       "  (encode): Sequential(\n",
       "    (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (link): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Unflatten(dim=1, unflattened_size=torch.Size([64, 4, 4]))\n",
       "  )\n",
       "  (decode): Sequential(\n",
       "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Conv2d(64, 1, kernel_size=(1, 3), stride=(1, 1))\n",
       "    (7): Tanh()\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): LeakyReLU(negative_slope=0.2)\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "      (3): Unflatten(dim=1, unflattened_size=torch.Size([64, 4, 4]))\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "      (2): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "      (3): LeakyReLU(negative_slope=0.2)\n",
       "      (4): ConvTranspose2d(64, 64, kernel_size=(3, 1), stride=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.2)\n",
       "      (6): Conv2d(64, 1, kernel_size=(1, 3), stride=(1, 1))\n",
       "      (7): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Generator2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator2, self).__init__()\n",
    "\n",
    "        self.gapW = 2\n",
    "        self.sinoSh = (5*self.gapW,5*self.gapW) # 10,10\n",
    "        self.sinoSize = math.prod(self.sinoSh)\n",
    "        self.gapSh = (self.sinoSh[0],self.gapW)\n",
    "        self.gapSize = math.prod(self.gapSh)\n",
    "        self.gapRngX = np.s_[ self.sinoSh[1]//2 - self.gapW//2 : self.sinoSh[1]//2 + self.gapW//2 ]\n",
    "        self.gapRng = np.s_[ : , self.gapRngX ]\n",
    "\n",
    "        latentChannels = 7\n",
    "        self.noise2latent = nn.Sequential(\n",
    "            nn.Linear(TCfg.latentDim, self.sinoSize*latentChannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten( 1, (latentChannels,) + self.sinoSh )\n",
    "        )\n",
    "        fillWheights(self.noise2latent)\n",
    "\n",
    "        baseChannels = 64\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(latentChannels+1, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.encode)\n",
    "\n",
    "        encSh = self.encode(torch.zeros((1,latentChannels+1,*self.sinoSh))).shape\n",
    "        linChannels = math.prod(encSh)\n",
    "        self.link = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(linChannels, linChannels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, encSh[1:]),\n",
    "        )\n",
    "        fillWheights(self.link)\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, 1, (1,3)),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "        fillWheights(self.decode)\n",
    "\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            self.encode,\n",
    "            self.link,\n",
    "            self.decode\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        images, noises = input\n",
    "        images, orgDims = unsqeeze4dim(images)\n",
    "        latent = self.noise2latent(noises)\n",
    "        modelIn = torch.cat((images,latent),dim=1)\n",
    "        mIn = modelIn[:,0,*self.gapRng]\n",
    "        mIn[()] = self.preProc(images[:,0,:,:])\n",
    "        patches = self.body(modelIn)\n",
    "        mIn = mIn.unsqueeze(1)\n",
    "        #patches = mIn + torch.where( patches < 0 , patches * mIn , patches ) # no normalization\n",
    "        patches = mIn + patches * torch.where( patches < 0 , mIn+0.5 , 1 ) # normalization\n",
    "        return squeezeOrg(patches, orgDims)\n",
    "\n",
    "\n",
    "    def preProc(self, images) :\n",
    "        images = images.unsqueeze(0) # for the 2D case\n",
    "        res = torch.zeros(images[...,*self.gapRng].shape, device=images.device)\n",
    "        res[...,0] += 2*images[...,self.gapRngX.start-1] + images[...,self.gapRngX.stop]\n",
    "        res[...,1] += 2*images[...,self.gapRngX.stop] + images[...,self.gapRngX.start-1]\n",
    "        res = res.squeeze(0) # to compensate for the first squeeze\n",
    "        return res/3\n",
    "\n",
    "    def generatePatches(self, images, noises=None) :\n",
    "        if noises is None :\n",
    "            noises = torch.randn( 1 if images.dim() < 3 else images.shape[0], TCfg.latentDim).to(TCfg.device)\n",
    "        return self.forward((images,noises))\n",
    "\n",
    "\n",
    "    def fillImages(self, images, noises=None) :\n",
    "        images[...,*self.gapRng] = self.generatePatches(images, noises)\n",
    "        return images\n",
    "\n",
    "\n",
    "    def generateImages(self, images, noises=None) :\n",
    "        clone = images.clone()\n",
    "        return self.fillImages(clone, noises)\n",
    "\n",
    "\n",
    "generator2 = Generator2()\n",
    "generator2 = load_model(generator2, model_path=\"saves/gap2_cor.model_gen.pt\" )\n",
    "generator2.to(TCfg.device)\n",
    "generator2.eval()\n",
    "#model_summary = summary(generator2, input_data=[ [refImages, refNoises] ] ).__str__()\n",
    "#print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Generator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.gapW = DCfg.gapW\n",
    "        self.sinoSh = (5*self.gapW,5*self.gapW) # 20,20\n",
    "        self.sinoSize = math.prod(self.sinoSh)\n",
    "        self.gapSh = (self.sinoSh[0],self.gapW)\n",
    "        self.gapSize = math.prod(self.gapSh)\n",
    "        self.gapRngX = np.s_[ self.sinoSh[1]//2 - self.gapW//2 : self.sinoSh[1]//2 + self.gapW//2 ]\n",
    "        self.gapRng = np.s_[ : , self.gapRngX ]\n",
    "\n",
    "        latentChannels = 7\n",
    "        self.noise2latent = nn.Sequential(\n",
    "            nn.Linear(TCfg.latentDim, self.sinoSize*latentChannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten( 1, (latentChannels,) + self.sinoSh )\n",
    "        )\n",
    "        fillWheights(self.noise2latent)\n",
    "\n",
    "        baseChannels = 128\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(latentChannels+1, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3, stride=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.encode)\n",
    "\n",
    "\n",
    "        encSh = self.encode(torch.zeros((1,latentChannels+1,*self.sinoSh))).shape\n",
    "        linChannels = math.prod(encSh)\n",
    "        self.link = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(linChannels, linChannels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, encSh[1:]),\n",
    "        )\n",
    "        fillWheights(self.link)\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (4,1), stride=(2,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, 1, 1),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "        fillWheights(self.decode)\n",
    "\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            self.encode,\n",
    "            self.link,\n",
    "            self.decode\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        images, noises = input\n",
    "        images, orgDims = unsqeeze4dim(images)\n",
    "        latent = self.noise2latent(noises)\n",
    "        modelIn = torch.cat((images,latent),dim=1)\n",
    "        mIn = modelIn[:,0,*self.gapRng]\n",
    "        mIn[()] = self.preProc(images[:,0,:,:])\n",
    "        patches = self.body(modelIn)\n",
    "        mIn = mIn.unsqueeze(1)\n",
    "        #patches = mIn + torch.where( patches < 0 , patches * mIn , patches ) # no normalization\n",
    "        patches = mIn + patches * torch.where( patches < 0 , mIn+0.5 , 1 ) # normalization\n",
    "        return squeezeOrg(patches, orgDims)\n",
    "\n",
    "\n",
    "    def preProc(self, images) :\n",
    "        images, orgDims = unsqeeze4dim(images)\n",
    "        preImages = torch.nn.functional.interpolate(images, scale_factor=0.5, mode='area')\n",
    "        prePatches = generator2.generatePatches(preImages)\n",
    "        prePatches = torch.nn.functional.interpolate(prePatches, scale_factor=2, mode='bilinear')\n",
    "        return squeezeOrg(prePatches, orgDims)\n",
    "\n",
    "\n",
    "    def generatePatches(self, images, noises=None) :\n",
    "        if noises is None :\n",
    "            noises = torch.randn( 1 if images.dim() < 3 else images.shape[0], TCfg.latentDim).to(TCfg.device)\n",
    "        return self.forward((images,noises))\n",
    "\n",
    "\n",
    "    def fillImages(self, images, noises=None) :\n",
    "        images[...,*self.gapRng] = self.generatePatches(images, noises)\n",
    "        return images\n",
    "\n",
    "\n",
    "    def generateImages(self, images, noises=None) :\n",
    "        clone = images.clone()\n",
    "        return self.fillImages(clone, noises)\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "generator.to(TCfg.device)\n",
    "model_summary = summary(generator, input_data=[ [refImages, refNoises] ] ).__str__()\n",
    "#print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Discriminator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Discriminator                            [2, 1]                    --\n",
      "├─Sequential: 1-1                        [2, 128, 4, 4]            --\n",
      "│    └─Conv2d: 2-1                       [2, 128, 18, 18]          1,280\n",
      "│    └─LeakyReLU: 2-2                    [2, 128, 18, 18]          --\n",
      "│    └─Conv2d: 2-3                       [2, 128, 8, 8]            147,584\n",
      "│    └─LeakyReLU: 2-4                    [2, 128, 8, 8]            --\n",
      "│    └─Conv2d: 2-5                       [2, 128, 6, 6]            147,584\n",
      "│    └─LeakyReLU: 2-6                    [2, 128, 6, 6]            --\n",
      "│    └─Conv2d: 2-7                       [2, 128, 4, 4]            147,584\n",
      "│    └─LeakyReLU: 2-8                    [2, 128, 4, 4]            --\n",
      "├─Sequential: 1-2                        [2, 1]                    --\n",
      "│    └─Flatten: 2-9                      [2, 2048]                 --\n",
      "│    └─Dropout: 2-10                     [2, 2048]                 --\n",
      "│    └─Linear: 2-11                      [2, 128]                  262,272\n",
      "│    └─LeakyReLU: 2-12                   [2, 128]                  --\n",
      "│    └─Dropout: 2-13                     [2, 128]                  --\n",
      "│    └─Linear: 2-14                      [2, 1]                    129\n",
      "│    └─Sigmoid: 2-15                     [2, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 706,433\n",
      "Trainable params: 706,433\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 35.59\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.90\n",
      "Params size (MB): 2.83\n",
      "Estimated Total Size (MB): 3.73\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        baseChannels = 128\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3, stride=2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.body)\n",
    "\n",
    "        encSh = self.body(torch.zeros((1,1,*DCfg.sinoSh))).shape\n",
    "        linChannels = math.prod(encSh)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(linChannels, baseChannels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(baseChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        fillWheights(self.head)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        convRes = self.body(images)\n",
    "        res = self.head(convRes)\n",
    "        return res\n",
    "\n",
    "discriminator = Discriminator()\n",
    "discriminator = discriminator.to(TCfg.device)\n",
    "model_summary = summary(discriminator, input_data=refImages ).__str__()\n",
    "print(model_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Optimizers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_G = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=TCfg.learningRateG,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=TCfg.learningRateD,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Restore checkpoint</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736151/3157356196.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkPoint = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "savedCheckPoint = f\"checkPoint_{TCfg.exec}\"\n",
    "epoch = 0\n",
    "#if False :\n",
    "if True :\n",
    "    epoch = loadCheckPoint(savedCheckPoint+\".pth\", generator, discriminator,\n",
    "                           optimizer_G, optimizer_D, device=TCfg.device)\n",
    "    #epoch = loadCheckPoint(savedCheckPoint+\"_BeforeBest.pth\", generator, discriminator,\n",
    "    #                       optimizer_G, optimizer_D, device=TCfg.device)\n",
    "else :\n",
    "    try : os.remove(TCfg.historyHDF)\n",
    "    except : pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55400574 0.36430728 0.50920075]\n",
      "1.582e-03, 4.127e-02, -1.050e-01, 9.202e-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLIAAAI7CAYAAAD4c7LTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAACPVAAAj1QGQh3HaAAAY8UlEQVR4nO3aW3NUhbaG4dFJd9IkEQhICAdXlYdCESlQC4+l4P+/8HdYXuiFYk7dc1+ZvdaVrFl7DvL1fp4fML+k0wnD154NwzAUAAAAAFxxW2/7CwAAAACANyFkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAI8ykfPpvNpnx8+05V1TAMLTvL5bJlZ3t7u2Wn82c0n0/6tr60u7vbstP1nquqOj8/b9npej90vReq+n6XunR9P11/66qqdnZ2WnZWq1XLTuf7e7FYtOz8/PPPLTuM57Ybz2033qbddp3Ozs5adtx2V5/bbjy33XhT3XY+kQUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABAhPmUD9/a6ulkwzC07FRV7e7utuwsl8uWndls1rLT9V6oqlqtVi075+fnLTtdP6PurQ6d3896vW7Z2bSf0enpadvWX3/91bKzWCxadjrfC13/JnH1ue3Gc9uN57bL2Orgtrv63Hbjue3enE9kAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAgwnzKh29t9XSy3d3dlp2qquVy2bKzXq83auf09LRlp6pqZ2enZefs7KxlZ3t7u2Wnqmpvb69lZxiGlp2uv0FVfb9Ls9msZafr/d31/VT1/f3u+jdpf3+/Zaeq6ubNm21bXG1uu/HcduO57cZz243nthvHbTee2+7N+UQWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABHmUz782rVrUz7+0tZWX487OTlp2VmtVi07wzC07HS9F6r6XrsuXT+jqr7393w+6Z+eS7PZrGWnc6vr/bC7u9uy0/m3YblctuwcHBy07Ozt7bXsVFUtFou2La42t914brvx3Hbjbdpt1/m3wW03jttuPLfdm/OJLAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAizKd8+Onp6ZSPv7S11dfjVqtVy85sNmvZuXbtWstO1+tWVXV+ft6y0/Uz6nx/7+zstOys1+uWnWEYWnaqqhaLRcvO7u5uy87+/n7LzsHBQctO51bX72zX7xH8O7fdeG678dx243XdJ103V+e/fW67cdx247nt3pxPZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQYT7lw2ez2ZSPv7RarVp2Oi0Wi5adrtfu7OysZaeqar1et+zM55P++ly6du1ay05V1dZWT9ve2dlp2el87bq+p4ODg5adw8PDlp3d3d2Wnaqq09PTlp1hGFp2rl+/3rJTVfXw4cO2La42t914brvx3Hbjdd12Xa/d3t5ey05V3++s2248t9146bedT2QBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACDCfMqHn5+fT/n4t2I+n/QluzQMQ8tO18/o4uKiZaeq72d0fHzcstP1XqiqWiwWLTtdP6Pbt2+37FRVvfPOOy07y+WyZafrb8Pr169bdqqq9vb2WnYePHjQsvPs2bOWnaqqFy9etG1xtbntxnPbjbdpt916vW7Zqara2dlp2XHbjee2G89tN176becTWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEmE/58PV6PeXjL83nk34b/2F7e7tlZ7VatexcXFy07CwWi5adqqp79+617Gxt9XTgzvf3rVu3WnZu3LjRsrNcLlt2qvr+3v3xxx8tO12/s/fv32/Zqap69OhRy84PP/zQsvP48eOWnaqq3d3dti2uNrfdeG678TbtttvZ2WnZqdq8267z36NhGFp23Hbjue3GS7/tfCILAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAjzSR8+n/TxlxaLRctOp4uLi5adnZ2dlp379++37FT1fU/L5bJl5+DgoGWnc6vrb0PX71FV1fn5ecvO7du3W3YePnzYsvPVV1+17FRV/fjjjy07N27caNlZrVYtO/Dv3Hbjue3Gc9uNt7+/37LT9TvbdW9V9f0767Ybz233/5dPZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIMJ8yofv7OxM+fhLs9msZaeq6uLiomVnsVi07Dx48KBl5/DwsGWnqmq5XLbs3Lhxo2VnvV637FRVnZyctOycnp627HT9jKr6fpcePXrUsvPy5cuWnSdPnrTsdFqtVm/7S4DJuO3Gc9uN57Ybr+u2Ozs7a9npvO2Ojo5adtx2V5/b7urxiSwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIsynfPhisZjy8ZfW63XLTlXVfD7pS3bp3r17LTvvvfdey07Xe6Gqant7u2Xn9evXLTtbW329eblctuwcHBy07HzwwQctO1VV33zzTcvOy5cvW3a63gvDMLTsVPX+W9Gh87WbzWZtW1xtbrvx3Hbjue3Gc9uN57Ybx203ntvuzflEFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAAR5pM+fD7p4y9dXFy07FRV3b17t2Xnk08+adk5Pz9v2Tk7O2vZqar6888/W3b29vZadvb391t2qqqOjo5adp4+fdqy8/XXX7fsVFU9f/68Zafrd3a1WrXsbKJhGN72l/B/bhO/J8Zx243nthvPbTee2248tx1/28Q7KP178oksAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEmE/58NVqNeXjL929e7dlp6rqyZMnLTu///57y87Z2VnLzmKxaNmpqrp161bLzrvvvtuyc+fOnZadqqrPP/+8ZefVq1ctOzdu3GjZqao6PT1t2RmGoWVnNpu17DBe13uhyvuB/+W2G89tN57bbrznz5+37Pz0008tO2678fxbfvW57d6cT2QBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEGE+5cOPjo6mfPylp0+ftuxUVf3yyy8tO8MwtOwcHBy07BweHrbsdG7961//atl5+fJly05V3+/S7u5uy85qtWrZqer7nd20ndls1rLTqeu1g7fBbTee2+7qb3Xddq9evWrZqar67LPPWnbcdnb+5rajg09kAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBhPuXDnzx5MuXjL/36668tO1VV8/mkL9mlmzdvtuwcHh627Ny5c6dlp6rq/fffb9l59epVy85HH33UslNVdX5+3rJzenrasrO1tXmtfhiGlp3ZbNayA2Rx243nthtv0267Dz/8sGWnquri4qJlx203ntsO/nub95cAAAAAgI0kZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIgwn/Lhv/3225SPv7S/v9+yU1V18+bNjdo5Pj5u2Xn8+HHLTlXV999/37JzdHTUsrNarVp2qqouLi5admazWcvO+fl5y05V1dZWz/8X6HrtunY6beL31MVrx9/cdld/x203Xtdt13VvdW657cZz2423id9Tl/TXzieyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQYT7lw2/fvj3l4y9dv369Zaeq6tatWy07x8fHLTvPnj1r2fn2229bdqqqFotFy87JyUnLzvb2dstOVdUwDG1bUFU1m83e9pcA/BfcduO57cbbtNtuPp/0P8H+g9uObm47OvhEFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAAR5lM+/Pj4eMrHX7p161bLTlXV0dFRy84XX3zRsvPll1+27Mxms5adqqqzs7OWna2tng7c+dp1fU+b+Npx9Q3DsFE7ne/vru+Jq89tN57bbrxNu+06ue3YZG678dJvu837aw0AAADARhKyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARBCyAAAAAIggZAEAAAAQQcgCAAAAIIKQBQAAAEAEIQsAAACACEIWAAAAABGELAAAAAAiCFkAAAAARJhP+fD79+9P+fhLd+/ebdmpqnrx4kXLzqefftqys1qtNmpnE21vb7/tLyHWer1+218C/2AYho3b6vp71/nawd/cduO57fib2248t93V57Ybz2335nwiCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEeZTPvzevXtTPv7Sd99917JTVfXxxx+37JycnLTsDMPQsrOJZrPZ2/4S+AdbW32tfr1et2116Pp+Ol+31WrVstP1t6Hz/T2fT3ouEMRtN57b7upz2119brvx3Hbjue2uHp/IAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACCCkAUAAABABCELAAAAgAhCFgAAAAARhCwAAAAAIghZAAAAAEQQsgAAAACIIGQBAAAAEEHIAgAAACDCbBiG4W1/EQAAAADwT3wiCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABBByAIAAAAggpAFAAAAQAQhCwAAAIAIQhYAAAAAEYQsAAAAACIIWQAAAABEELIAAAAAiCBkAQAAABDhfwA5d/E1MoZ7lwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad() :\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    refImages[1,...,DCfg.gapRngX] = generator.preProc(refImages[1,...])\n",
    "    mgens = generator.generateImages(refImages,refNoises)\n",
    "    liks = discriminator(torch.cat((refImages, mgens[0,...].unsqueeze(0))))\n",
    "    print(liks.detach().cpu().squeeze().numpy())\n",
    "    tensorStat( mgens[1,*DCfg.disRng] - refImages[1,*DCfg.disRng] )\n",
    "    plotImages((refImages[0,...].detach().cpu().squeeze(),\n",
    "                refImages[1,...].detach().cpu().squeeze()))\n",
    "    plotImages((mgens[0,...].detach().cpu().squeeze(),\n",
    "                mgens[1,...].detach().cpu().squeeze()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Train</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE = nn.BCELoss(reduction='none')\n",
    "MSE = nn.MSELoss(reduction='none')\n",
    "\n",
    "\n",
    "def applyWeights(inp, weights):\n",
    "    inp = inp.squeeze()\n",
    "    sum = len(inp)\n",
    "    if not weights is None :\n",
    "        inp *= weights\n",
    "        sum = weights.sum()\n",
    "    return inp.sum()/sum\n",
    "\n",
    "def loss_Adv(y_true, y_pred, weights=None):\n",
    "    loss = BCE(y_pred, y_true)\n",
    "    return applyWeights(loss,weights)\n",
    "\n",
    "TrainInfoMaxDifIdx = 0\n",
    "def loss_Gen(y_true, y_pred, p_true, p_pred, weights=None):\n",
    "    global TrainInfoMaxDifIdx\n",
    "    lossAdv = loss_Adv(y_true, y_pred, weights)\n",
    "    lossesDif = MSE(p_pred, p_true).mean(dim=(1,2)) / (0.5+p_true.mean(dim=(1,2)))\n",
    "    TrainInfoMaxDifIdx = lossesDif.argmax().item()\n",
    "    lossDif = applyWeights(lossesDif, weights)\n",
    "    return lossAdv, lossDif\n",
    "\n",
    "lossDifCoef = 0\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "\n",
    "    D_losses, GA_fakeLosses, GD_fakeLosses, GA_prepLosses, GD_prepLosses = [], [], [], [], []\n",
    "    ratReal = ratPrep = ratFake = nofIm 0\n",
    "    with torch.no_grad():\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "        for it , data in tqdm.tqdm(enumerate(dataloader), total=int(len(dataloader))):\n",
    "            images = data[0].squeeze(1).to(TCfg.device)\n",
    "            nofIm = images.shape[0]\n",
    "            means = images.mean(dim=(1,2)).squeeze() + 0.5\n",
    "            #stdds = images.std(dim=(1,2)).squeeze()\n",
    "            absWeights = means + 1\n",
    "            absWeights /= absWeights.sum()\n",
    "            weights = absWeights.to(TCfg.device)\n",
    "            labelsTrue = torch.full((nofIm, 1),  1 - TCfg.labelSmoothFac,\n",
    "                                dtype=torch.float, device=TCfg.device)\n",
    "            nofIm += images.shape[0]\n",
    "\n",
    "            y_pred_real = discriminator(images)\n",
    "            D_loss_real = loss_Adv(labelsTrue, y_pred_real, weights)\n",
    "            D_losses.append(D_loss_real)\n",
    "            ratReal += torch.count_nonzero(y_pred_real > 0.5)\n",
    "\n",
    "            prepImages = images.clone()\n",
    "            prepImages[...,*DCfg.gapRng] = generator.preProc(images)\n",
    "            y_pred_prep = discriminator(prepImages)\n",
    "            GA_loss_prep, GD_loss_prep = \\\n",
    "                loss_Gen(labelsTrue, y_pred_prep,\n",
    "                         images[...,DCfg.gapRngX], prepImages[...,DCfg.gapRngX],\n",
    "                         weights)\n",
    "            GA_prepLosses.append(GA_loss_prep)\n",
    "            GD_prepLosses.append(GD_loss_prep)\n",
    "            ratPrep += torch.count_nonzero(y_pred_prep > 0.5)\n",
    "\n",
    "            fakeImages = generator.generateImages(images)\n",
    "            y_pred_fake = discriminator(fakeImages)\n",
    "            GA_loss_fake, GD_loss_fake = \\\n",
    "                loss_Gen(labelsTrue, y_pred_fake,\n",
    "                         images[...,DCfg.gapRngX], fakeImages[...,DCfg.gapRngX],\n",
    "                         weights)\n",
    "            GA_fakeLosses.append(GA_loss_fake)\n",
    "            GD_fakeLosses.append(GD_loss_fake)\n",
    "            ratFake += torch.count_nonzero(y_pred_fake > 0.5)\n",
    "\n",
    "    total_D_loss = statistics.fmean(D_losses)\n",
    "    total_GA_loss_prep = statistics.fmean(GA_prepLosses)\n",
    "    total_GD_loss_prep = statistics.fmean(GD_prepLosses)\n",
    "    total_GA_loss_fake = statistics.fmean(GA_fakeLosses)\n",
    "    total_GD_loss_fake = statistics.fmean(GD_fakeLosses)\n",
    "\n",
    "    print(f\"Dis {total_D_loss:.4e} ({ratReal/len(dataloader)}),\\n\"\n",
    "          f\"Pre {total_GA_loss_prep:.4e} + {total_GD_loss_prep:.4e} ({ratPrep/nofIm}),\\n\"\n",
    "          f\"Gen {total_GA_loss_fake:.4e} + {total_GD_loss_fake:.4e} ({ratFake/nofIm}).\"\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Train step</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class TrainInfo:\n",
    "    bestRealImage = None\n",
    "    bestRealPrediction = 0\n",
    "    bestRealIndex = 0\n",
    "    worstRealImage = None\n",
    "    worstRealPrediction = 0\n",
    "    worstRealIndex = 0\n",
    "    bestFakeImage = None\n",
    "    bestFakePrediction = 0\n",
    "    bestFakeIndex = 0\n",
    "    worstFakeImage = None\n",
    "    worstFakePrediction = 0\n",
    "    worstFakeIndex = 0\n",
    "    highestDifIndex = 0\n",
    "    ratReal = 0.0\n",
    "    ratFake = 0.0\n",
    "    totalImages = 0\n",
    "    iterations = 0\n",
    "    disPerformed = 0\n",
    "    genPerformed = 0\n",
    "\n",
    "\n",
    "trainDis = True\n",
    "trainGen = True\n",
    "def train_step(images):\n",
    "    global trainDis, trainGen\n",
    "    TrainInfo.iterations += 1\n",
    "\n",
    "    nofIm = images.shape[0]\n",
    "    images = images.squeeze(1).to(TCfg.device)\n",
    "    means = images.mean(dim=(1,2)).squeeze() + 0.5\n",
    "    #stdds = images.std(dim=(1,2)).squeeze()\n",
    "    absWeights = means + 1\n",
    "    absWeights /= absWeights.sum()\n",
    "    weights = absWeights.to(TCfg.device)\n",
    "    #disWeights = stdds / means\n",
    "    #disWeights /= disWeights.sum()\n",
    "    #weights = absWeights + disWeights\n",
    "    #weights /= weights.sum()\n",
    "    labelsTrue = torch.full((nofIm, 1),  1 - TCfg.labelSmoothFac,\n",
    "                        dtype=torch.float, device=TCfg.device)\n",
    "    labelsFalse = torch.full((nofIm, 1),  TCfg.labelSmoothFac,\n",
    "                        dtype=torch.float, device=TCfg.device)\n",
    "\n",
    "\n",
    "    fakeImages = None\n",
    "    y_pred_real = None\n",
    "    y_pred_fake = None\n",
    "    y_pred_both = None\n",
    "    labels = None\n",
    "    D_loss = None\n",
    "    G_loss = None\n",
    "    GA_loss = None\n",
    "    GD_loss = None\n",
    "    dif_losses = None\n",
    "\n",
    "    ratReal = 0\n",
    "    ratFake = 0\n",
    "\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "    counter = 0\n",
    "    if trainDis:\n",
    "        counter += 1\n",
    "        TrainInfo.disPerformed += 1\n",
    "        try :\n",
    "            optimizer_D.zero_grad()\n",
    "            y_pred_real = discriminator(images)\n",
    "            fakeImages = generator.generateImages(images)\n",
    "            y_pred_fake = discriminator(fakeImages)\n",
    "            y_pred_both = torch.cat((y_pred_real, y_pred_fake), dim=0)\n",
    "            labels = torch.cat( ( labelsTrue, labelsFalse), dim=0).to(TCfg.device)\n",
    "            D_loss = loss_Adv(labels, y_pred_both, torch.cat( (weights, weights) ) )\n",
    "            #preImages = images.clone()\n",
    "            #preImages[...,*DCfg.gapRng] = generator.preProc(images)\n",
    "            #y_pred_pre = discriminator(preImages)\n",
    "            #y_pred_both = torch.cat((y_pred_real, y_pred_real,\n",
    "            #                         y_pred_fake, y_pred_pre), dim=0)\n",
    "            #labels = torch.cat( (labelsTrue, labelsTrue,\n",
    "            #                     labelsFalse, labelsFalse), dim=0).to(TCfg.device)\n",
    "            #D_loss = loss_Adv(labels, y_pred_both, torch.cat( (weights, weights,\n",
    "            #                                                   weights, weights) ) )\n",
    "            D_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        except :\n",
    "            optimizer_D.zero_grad()\n",
    "            del fakeImages\n",
    "            del y_pred_real\n",
    "            del y_pred_fake\n",
    "            del y_pred_both\n",
    "            del labels\n",
    "            del D_loss\n",
    "            raise\n",
    "        ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        trainGen = True #ratReal > ratFake\n",
    "    else :\n",
    "        with torch.no_grad():\n",
    "            y_pred_real = discriminator(images)\n",
    "            D_loss = loss_Adv(labelsTrue, y_pred_real, weights)\n",
    "            ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.train()\n",
    "    counter = 0\n",
    "    #trainGen = False\n",
    "    if trainGen :\n",
    "        counter += 1\n",
    "        TrainInfo.genPerformed += 1\n",
    "        try :\n",
    "            optimizer_G.zero_grad()\n",
    "            fakeImages = generator.generateImages(images)\n",
    "            y_pred_fake = discriminator(fakeImages)\n",
    "            GA_loss, GD_loss = loss_Gen(labelsTrue, y_pred_fake,\n",
    "                                        images[...,DCfg.gapRngX], fakeImages[...,DCfg.gapRngX],\n",
    "                                        weights)\n",
    "            G_loss = GA_loss + lossDifCoef * GD_loss\n",
    "            G_loss.backward()\n",
    "            optimizer_G.step()\n",
    "        except :\n",
    "            optimizer_G.zero_grad()\n",
    "            del fakeImages\n",
    "            del y_pred_fake\n",
    "            del G_loss\n",
    "            del GA_loss\n",
    "            del GD_loss\n",
    "            del dif_losses\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        trainDis = True # ratFake > 0.1 or ratReal <= ratFake\n",
    "    else :\n",
    "        with torch.no_grad():\n",
    "            GA_loss, GD_loss = loss_Gen(labelsTrue, y_pred_fake,\n",
    "                                        images[...,DCfg.gapRngX], fakeImages[...,DCfg.gapRngX],\n",
    "                                        weights)\n",
    "            G_loss = GA_loss + lossDifCoef * GD_loss\n",
    "            ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "\n",
    "    #trainDis = trainGen = True\n",
    "\n",
    "\n",
    "    idx = y_pred_real.argmax()\n",
    "    TrainInfo.bestRealImage = images[idx,...].clone().detach()\n",
    "    TrainInfo.bestRealPrediction = y_pred_real[idx].item()\n",
    "    TrainInfo.bestRealIndex = idx\n",
    "\n",
    "    idx = y_pred_real.argmin()\n",
    "    TrainInfo.worstRealImage = images[idx,...].clone().detach()\n",
    "    TrainInfo.worstRealPrediction =  y_pred_real[idx].item()\n",
    "    TrainInfo.worstRealIndex = idx\n",
    "\n",
    "    idx = y_pred_fake.argmax()\n",
    "    TrainInfo.bestFakeImage = fakeImages[idx,...].clone().detach()\n",
    "    TrainInfo.bestFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.bestFakeIndex = idx\n",
    "\n",
    "    idx = y_pred_fake.argmin()\n",
    "    TrainInfo.worstFakeImage = fakeImages[idx,...].clone().detach()\n",
    "    TrainInfo.worstFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.worstFakeIndex = idx\n",
    "\n",
    "    TrainInfo.highestDifIndex = TrainInfoMaxDifIdx\n",
    "    TrainInfo.ratReal += ratReal * nofIm\n",
    "    TrainInfo.ratFake += ratFake * nofIm\n",
    "    TrainInfo.totalImages += nofIm\n",
    "\n",
    "    return D_loss, GA_loss, GD_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Trainer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "D_LOSS = []\n",
    "GA_LOSS = []\n",
    "GD_LOSS = []\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    global epoch\n",
    "\n",
    "    minGdLoss = -1.0\n",
    "    minDLoss = -1.0\n",
    "    minGEpoch = 0\n",
    "    minDEpoch = 0\n",
    "    discriminator.to(TCfg.device)\n",
    "    generator.to(TCfg.device)\n",
    "    refImages.to(TCfg.device)\n",
    "    refNoises.to(TCfg.device)\n",
    "    lastUpdateTime = time.time()\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        D_loss_list, GA_loss_list, GD_loss_list = [], [], []\n",
    "\n",
    "        for it , data in tqdm.tqdm(enumerate(dataloader), total=int(len(dataloader))):\n",
    "            images = data[0].to(TCfg.device)\n",
    "            D_loss, GA_loss, GD_loss = train_step(images)\n",
    "            #print(f\" Dis {D_loss.item():.3f}, Gen {GA_loss.item():.3f}, MSE {GD_loss.item():.3f}.\")\n",
    "            D_loss_list.append(D_loss)\n",
    "            GA_loss_list.append(GA_loss)\n",
    "            GD_loss_list.append(GD_loss)\n",
    "\n",
    "            #if True:\n",
    "            #if not it or it > len(dataloader)-2 or time.time() - lastUpdateTime > 60 :\n",
    "            if time.time() - lastUpdateTime > 60 :\n",
    "                lastUpdateTime = time.time()\n",
    "                IPython.display.clear_output(wait=True)\n",
    "                print(f\"Epoch: {epoch} ({minDEpoch},{minGEpoch}). Losses: \"\n",
    "                      f\" Dis: {D_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratReal/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.disPerformed/TrainInfo.iterations:.3f}),\"\n",
    "                      f\" Gen: {GA_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratFake/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.genPerformed/TrainInfo.iterations:.3f}),\"\n",
    "                      f\" MSE: {GD_loss:.3e} ({minGdLoss:.3e}).\"\n",
    "                      )\n",
    "                TrainInfo.iterations = 0\n",
    "                TrainInfo.totalImages = 0\n",
    "                TrainInfo.ratReal = 0\n",
    "                TrainInfo.ratFake = 0\n",
    "                TrainInfo.genPerformed = 0\n",
    "                TrainInfo.disPerformed = 0\n",
    "\n",
    "                iM = trainSet[random.randint(0,len(trainSet)-1)][0].clone().detach().to(TCfg.device)\n",
    "                #iM = images[TrainInfo.highestDifIndex,0,...].clone().detach().unsqueeze(0)\n",
    "                #iM = refImages[0,...].unsqueeze(0)\n",
    "                hdIms = torch.repeat_interleave(iM,3,dim=0)\n",
    "                dif = torch.zeros(DCfg.sinoSh)\n",
    "                liks = None\n",
    "                with torch.no_grad():\n",
    "                    hGap = DCfg.gapW // 2\n",
    "                    generator.eval()\n",
    "                    org = hdIms[0,...]\n",
    "                    pre = generator.preProc(org)\n",
    "                    hdIms[2,*DCfg.gapRng] = pre\n",
    "                    gen = generator.generatePatches(org)\n",
    "                    hdIms[1,*DCfg.gapRng] = gen\n",
    "                    discriminator.eval()\n",
    "                    liks = discriminator(hdIms)\n",
    "                    dif[DCfg.gapRng] = gen - pre\n",
    "                    dif[:,hGap:hGap+DCfg.gapW] = org[DCfg.gapRng] - pre\n",
    "                    dif[:,-DCfg.gapW-hGap:-hGap] = gen - org[DCfg.gapRng]\n",
    "                tensorStat(dif[DCfg.disRng])\n",
    "                if ( cof := max(-dif.min(),dif.max()) ) != 0 :\n",
    "                    dif /= 2*cof\n",
    "\n",
    "                fourImages = np.ones( (2*DCfg.sinoSh[1] + DCfg.gapW ,\n",
    "                                       4*DCfg.sinoSh[0] + 3*DCfg.gapW), dtype=np.float32  )\n",
    "                def addImage(clmn, row, img) :\n",
    "                    fourImages[ row * ( DCfg.sinoSh[1]+DCfg.gapW) : (row+1) * DCfg.sinoSh[1] + row*DCfg.gapW ,\n",
    "                                clmn * ( DCfg.sinoSh[0]+DCfg.gapW) : (clmn+1) * DCfg.sinoSh[0] + clmn*DCfg.gapW ] = \\\n",
    "                        img.squeeze().detach().cpu().numpy()\n",
    "                addImage(0,0,TrainInfo.bestRealImage)\n",
    "                addImage(0,1,TrainInfo.worstRealImage)\n",
    "                addImage(1,0,TrainInfo.bestFakeImage)\n",
    "                addImage(1,1,TrainInfo.worstFakeImage)\n",
    "                addImage(2,0,hdIms[1,...])\n",
    "                addImage(2,1,hdIms[0,...])\n",
    "                addImage(3,0,hdIms[2,...])\n",
    "                addImage(3,1,dif)\n",
    "\n",
    "                print (f\"TT: {TrainInfo.bestRealPrediction:.4e} ({data[1][TrainInfo.bestRealIndex]},{data[2][TrainInfo.bestRealIndex]}),  \"\n",
    "                       f\"FT: {TrainInfo.bestFakePrediction:.4e} ({data[1][TrainInfo.bestFakeIndex]},{data[2][TrainInfo.bestFakeIndex]}),  \"\n",
    "                       f\"GP: {liks[1].item():.5f}, {liks[2].item():.5f} \" )\n",
    "                print (f\"TF: {TrainInfo.worstRealPrediction:.4e} ({data[1][TrainInfo.worstRealIndex]},{data[2][TrainInfo.worstRealIndex]}),  \"\n",
    "                       f\"FF: {TrainInfo.worstFakePrediction:.4e} ({data[1][TrainInfo.worstFakeIndex]},{data[2][TrainInfo.worstFakeIndex]}),  \"\n",
    "                       f\"R : {liks[0].item():.5f} ({data[1][TrainInfo.highestDifIndex]},{data[2][TrainInfo.highestDifIndex]})\" )\n",
    "                try :\n",
    "                    #tifffile.imwrite(f\"tmp_{TCfg.exec}.tif\", fourImages)\n",
    "                    addToHDF(TCfg.historyHDF, \"data\", fourImages)\n",
    "                except :\n",
    "                    eprint(\"Failed to save.\")\n",
    "                plotImage(fourImages)\n",
    "\n",
    "        epoch_D_loss = sum(D_loss_list)/len(D_loss_list)\n",
    "        epoch_GA_loss = sum(GA_loss_list)/len(GA_loss_list)\n",
    "        epoch_GD_loss = sum(GD_loss_list)/len(GD_loss_list)\n",
    "        D_LOSS.append(epoch_D_loss.detach().cpu())\n",
    "        GA_LOSS.append(epoch_GA_loss.detach().cpu())\n",
    "        GD_LOSS.append(epoch_GD_loss.detach().cpu())\n",
    "        if minGdLoss < 0.0 or epoch_GD_loss < minGdLoss :\n",
    "            minGdLoss = epoch_GD_loss\n",
    "            minGEpoch = epoch\n",
    "            saveCheckPoint(savedCheckPoint+\"_BEST.pth\", epoch,\n",
    "                           generator, discriminator,\n",
    "                           optimizer_G, optimizer_D,\n",
    "                           device=TCfg.device)\n",
    "            os.system(f\"cp {savedCheckPoint}.pth {savedCheckPoint}_BeforeBest.pth\")\n",
    "            os.system(f\"cp {savedCheckPoint}_BEST.pth {savedCheckPoint}.pth\")\n",
    "        else :\n",
    "            saveCheckPoint(savedCheckPoint+\".pth\", epoch,\n",
    "                           generator, discriminator,\n",
    "                           optimizer_G, optimizer_D,\n",
    "                           device=TCfg.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Execute</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1703 (0,1702). Losses:  Dis: 0.691 (0.458 / 1.000), Gen: 0.719 (0.326 / 1.000), MSE: 1.837e-04 (4.155e-04).\n",
      "1.079e-03, 2.671e-03, -2.899e-03, 6.898e-03\n",
      "TT: 9.9336e-01 (584524,535),  FT: 7.4507e-01 (582325,253),  GP: 0.49266, 0.49193 \n",
      "TF: 2.2879e-01 (585036,687),  FF: 1.7995e-01 (578165,1218),  R : 0.49402 (577567,580)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:46<00:00,  2.13s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    dataset=trainSet,\n",
    "    batch_size=TCfg.batchSize,\n",
    "    shuffle=False,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "# weight of the MSE loss in generator metrics. Initially set to 0 to repel training from trivial solution of\n",
    "# zero correction,and later set to higher number when pure adversarial loss makes generator to produce results\n",
    "# superior to zero correction.\n",
    "lossDifCoef = 1000\n",
    "trainGen=True\n",
    "trainDis=True\n",
    "try :\n",
    "    #evaluate(trainLoader)\n",
    "    train(trainLoader)\n",
    "except :\n",
    "    del trainLoader\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Post</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160378 496\n",
      "0.00011075045767938718\n",
      "[0.54391104 0.49613807 0.22777216]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "def testMe() :\n",
    "    with torch.no_grad() :\n",
    "\n",
    "        testImage, index, idxs = trainSet[random.randint(0,len(trainSet)-1)]\n",
    "        print(index, idxs)\n",
    "        testImage = testImage.to(TCfg.device)\n",
    "        preImage = testImage.clone()\n",
    "        patch = generator.preProc(testImage)\n",
    "        preImage[:,*DCfg.gapRng] = patch\n",
    "        genImage = generator.generateImages(testImage)\n",
    "        mse = MSE(testImage, genImage).mean().item()\n",
    "        print(mse)\n",
    "\n",
    "        toShow = np.zeros( ( DCfg.sinoSh[1], 4*DCfg.sinoSh[0] + 3*DCfg.gapW), dtype=np.float32  )\n",
    "        def addImage(clmn, img) :\n",
    "            toShow[ :  , clmn * ( DCfg.sinoSh[0]+DCfg.gapW) : (clmn+1) * DCfg.sinoSh[0] + clmn*DCfg.gapW ] = \\\n",
    "                img.squeeze().detach().cpu().numpy()\n",
    "\n",
    "        mn = testImage.mean().item()\n",
    "        addImage(0, testImage - mn)\n",
    "        addImage(1, genImage - mn)\n",
    "        addImage(2, preImage - mn)\n",
    "        dif = genImage - preImage\n",
    "        hGap = DCfg.gapW // 2\n",
    "        dif[:,:,hGap:hGap+DCfg.gapW] = (testImage[:,*DCfg.gapRng] - preImage[:,*DCfg.gapRng])\n",
    "        dif[:,:,-DCfg.gapW-hGap:-hGap] = (genImage[:,*DCfg.gapRng] - testImage[:,*DCfg.gapRng])\n",
    "        #if ( cof := max(-dif.min(),dif.max()) ) != 0 :\n",
    "        #    dif /= 2*cof\n",
    "        addImage(3, dif)\n",
    "        disIn = torch.cat(( testImage, genImage, preImage), dim=0)\n",
    "        liks = discriminator(disIn)\n",
    "        print(liks.squeeze().cpu().numpy())\n",
    "        plotImage(toShow)\n",
    "\n",
    "\n",
    "testMe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True :\n",
    "    save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "    save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
