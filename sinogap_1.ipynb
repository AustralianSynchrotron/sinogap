{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Header</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread, imsave\n",
    "import h5py\n",
    "import tifffile\n",
    "import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "\n",
    "def plotData(dataY, rangeY=None, dataYR=None, rangeYR=None,\n",
    "             dataX=None, rangeX=None, rangeP=None,\n",
    "             figsize=(16,8), saveTo=None, show=True):\n",
    "\n",
    "    if type(dataY) is np.ndarray :\n",
    "        plotData((dataY,), rangeY=rangeY, dataYR=dataYR, rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataYR) is np.ndarray :\n",
    "        plotData(dataY, rangeY=rangeY, dataYR=(dataYR,), rangeYR=rangeYR,\n",
    "             dataX=dataX, rangeX=rangeX, rangeP=rangeP,\n",
    "             figsize=figsize, saveTo=saveTo, show=show)\n",
    "        return\n",
    "    if type(dataY) is not tuple :\n",
    "        eprint(f\"Unknown data type to plot: {type(dataY)}.\")\n",
    "        return\n",
    "    if type(dataYR) is not tuple and dataYR is not None:\n",
    "        eprint(f\"Unknown data type to plot: {type(dataYR)}.\")\n",
    "        return\n",
    "\n",
    "    last = min( len(data) for data in dataY )\n",
    "    if dataYR is not None:\n",
    "        last = min( last,  min( len(data) for data in dataYR ) )\n",
    "    if dataX is not None:\n",
    "        last = min(last, len(dataX))\n",
    "    if rangeP is None :\n",
    "        rangeP = (0,last)\n",
    "    elif type(rangeP) is int :\n",
    "        rangeP = (0,rangeP) if rangeP > 0 else (-rangeP,last)\n",
    "    elif type(rangeP) is tuple :\n",
    "        rangeP = ( 0    if rangeP[0] is None else rangeP[0],\n",
    "                   last if rangeP[1] is None else rangeP[1],)\n",
    "    else :\n",
    "        eprint(f\"Bad data type on plotData input rangeP: {type(rangeP)}\")\n",
    "        raise Exception(f\"Bug in the code.\")\n",
    "    rangeP = np.s_[ max(0, rangeP[0]) : min(last, rangeP[1]) ]\n",
    "    if dataX is None :\n",
    "        dataX = np.arange(rangeP.start, rangeP.stop)\n",
    "\n",
    "    plt.style.use('default')\n",
    "    plt.style.use('dark_background')\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax1.xaxis.grid(True, 'both', linestyle='dotted')\n",
    "    if rangeX is not None :\n",
    "        ax1.set_xlim(rangeX)\n",
    "    else :\n",
    "        ax1.set_xlim(rangeP.start,rangeP.stop-1)\n",
    "\n",
    "    ax1.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "    nofPlots = len(dataY)\n",
    "    if rangeY is not None:\n",
    "        ax1.set_ylim(rangeY)\n",
    "    colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "    for idx , data in enumerate(dataY):\n",
    "        ax1.plot(dataX, data[rangeP], linestyle='-',  color=colors[idx])\n",
    "\n",
    "    if dataYR is not None : # right Y axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.yaxis.grid(True, 'both', linestyle='dotted')\n",
    "        nofPlots = len(dataYR)\n",
    "        if rangeYR is not None:\n",
    "            ax2.set_ylim(rangeYR)\n",
    "        colors = [ matplotlib.colors.hsv_to_rgb((hv/nofPlots, 1, 1)) for hv in range(nofPlots) ]\n",
    "        for idx , data in enumerate(dataYR):\n",
    "            ax2.plot(dataX, data[rangeP], linestyle='dashed',  color=colors[idx])\n",
    "\n",
    "    if saveTo:\n",
    "        fig.savefig(saveTo)\n",
    "    if not show:\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def plotImage(image) :\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotImages(images) :\n",
    "    for i, img in enumerate(images) :\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sliceShape(shape, sl) :\n",
    "    if type(shape) is int :\n",
    "        shape = torch.Size([shape])\n",
    "    if type(sl) is tuple :\n",
    "        if len(shape) != len(sl) :\n",
    "            raise Exception(f\"Different sizes of shape {shape} and sl {sl}\")\n",
    "        out = []\n",
    "        for i in range(0, len(shape)) :\n",
    "            indeces = sl[i].indices(shape[i])\n",
    "            out.append(indeces[1]-indeces[0])\n",
    "        return out\n",
    "    elif type(sl) is slice :\n",
    "        indeces = sl.indices(shape[0])\n",
    "        return indeces[1]-indeces[0]\n",
    "    else :\n",
    "        raise Exception(f\"Incompatible object {sl}\")\n",
    "\n",
    "\n",
    "def tensorStat(stat) :\n",
    "    print(stat.mean().item(), stat.std().item(), (stat.std()/stat.mean()).item(), stat.min().item(), stat.max().item())\n",
    "\n",
    "\n",
    "def fillWheights(seq) :\n",
    "    for wh in seq :\n",
    "        if hasattr(wh, 'weight') :\n",
    "            torch.nn.init.xavier_uniform_(wh.weight)\n",
    "\n",
    "\n",
    "def unsqeeze4dim(tens):\n",
    "    orgDims = tens.dim()\n",
    "    if tens.dim() == 2 :\n",
    "        tens = tens.unsqueeze(0)\n",
    "    if tens.dim() == 3 :\n",
    "        tens = tens.unsqueeze(1)\n",
    "    return tens, orgDims\n",
    "\n",
    "\n",
    "def squeezeOrg(tens, orgDims):\n",
    "    if orgDims == tens.dim():\n",
    "        return tens\n",
    "    if tens.dim() != 4 or orgDims > 4 or orgDims < 2:\n",
    "        raise Exception(f\"Unexpected dimensions to squeeze: {tens.dim()} {orgDims}.\")\n",
    "    if orgDims < 4 :\n",
    "        if tens.shape[1] > 1:\n",
    "            raise Exception(f\"Cant squeeze dimension 1 in: {tens.shape}.\")\n",
    "        tens = tens.squeeze(1)\n",
    "    if orgDims < 3 :\n",
    "        if tens.shape[0] > 1:\n",
    "            raise Exception(f\"Cant squeeze dimension 0 in: {tens.shape}.\")\n",
    "        tens = tens.squeeze(0)\n",
    "    return tens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Configs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED_VALUE):\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed(SEED_VALUE)\n",
    "    torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "\n",
    "seed = 7\n",
    "set_seed(seed)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TCfg:\n",
    "    exec = 1\n",
    "    device: torch.device = f\"cuda:{exec}\"\n",
    "    nofEpochs: int = 2048\n",
    "    latentDim: int = 64\n",
    "    batchSize: int = 65536\n",
    "    labelSmoothFac: float = 1.0 # For Real labels (or set to 1.0 for no smoothing).\n",
    "    learningRateD: float = 0.00002\n",
    "    learningRateG: float = 0.00002\n",
    "    historyHDF = f\"train_{exec}.hdf\"\n",
    "\n",
    "class DCfg:\n",
    "    gapW = 2\n",
    "    sinoSh = (5*gapW,5*gapW) # 10,10\n",
    "    readSh = (8*sinoSh[0], 8*sinoSh[0]) # 80,80\n",
    "    sinoSize = math.prod(sinoSh)\n",
    "    gapSh = (sinoSh[0],gapW)\n",
    "    gapSize = math.prod(gapSh)\n",
    "    gapRngX = np.s_[ sinoSh[1]//2 - gapW//2 : sinoSh[1]//2 + gapW//2 ]\n",
    "    gapRng = np.s_[ : , gapRngX ]\n",
    "    disRng = np.s_[ gapW:-gapW , gapRngX ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Save/Load model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_path):\n",
    "    if not device == 'cpu':\n",
    "        model.to('cpu')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    if not device == 'cpu':\n",
    "        model.to(device)\n",
    "    return\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def addToHDF(filename, containername, data) :\n",
    "    if len(data.shape) == 2 :\n",
    "        data=np.expand_dims(data, 0)\n",
    "    if len(data.shape) != 3 :\n",
    "        raise Exception(f\"Not appropriate input array size {data.shape}.\")\n",
    "\n",
    "    with h5py.File(filename,'a') as file :\n",
    "\n",
    "        if  containername not in file.keys():\n",
    "            dset = file.create_dataset(containername, data.shape,\n",
    "                                       maxshape=(None,data.shape[1],data.shape[2]),\n",
    "                                       dtype='f')\n",
    "            dset[()] = data\n",
    "            return\n",
    "\n",
    "        dset = file[containername]\n",
    "        csh = dset.shape\n",
    "        if csh[1] != data.shape[1] or csh[2] != data.shape[2] :\n",
    "            raise Exception(f\"Shape mismatch: input {data.shape}, file {dset.shape}.\")\n",
    "        msh = dset.maxshape\n",
    "        newLen = csh[0] + data.shape[0]\n",
    "        if msh[0] is None or msh[0] >= newLen :\n",
    "            dset.resize(newLen, axis=0)\n",
    "        else :\n",
    "            raise Exception(f\"Insufficient maximum shape {msh} to add data\"\n",
    "                            f\" {data.shape} to current volume {dset.shape}.\")\n",
    "        dset[csh[0]:newLen,...] = data\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Raw Read</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StripesFromHDF :\n",
    "\n",
    "    def __init__(self, sampleName, maskName, bgName=None, dfName=None, loadToMem=True):\n",
    "\n",
    "        sampleHDF = sampleName.split(':')\n",
    "        if len(sampleHDF) != 2 :\n",
    "            raise Exception(f\"String \\\"{sampleName}\\\" does not represent an HDF5 format.\")\n",
    "        with h5py.File(sampleHDF[0],'r') as trgH5F:\n",
    "            if  sampleHDF[1] not in trgH5F.keys():\n",
    "                raise Exception(f\"No dataset '{sampleHDF[1]}' in input file {sampleHDF[0]}.\")\n",
    "            self.data = trgH5F[sampleHDF[1]]\n",
    "            if not self.data.size :\n",
    "                raise Exception(f\"Container \\\"{sampleName}\\\" is zero size.\")\n",
    "            self.sh = self.data.shape\n",
    "            if len(self.sh) != 3 :\n",
    "                raise Exception(f\"Dimensions of the container \\\"{sampleName}\\\" is not 3 {self.sh}.\")\n",
    "            self.fsh = self.sh[1:3]\n",
    "            self.volume = None\n",
    "            if loadToMem :\n",
    "                self.volume = np.empty(self.sh, dtype=np.float32)\n",
    "                self.data.read_direct(self.volume)\n",
    "                trgH5F.close()\n",
    "\n",
    "            def loadImage(imageName) :\n",
    "                if not imageName:\n",
    "                    return None\n",
    "                imdata = imread(imageName).astype(np.float32)\n",
    "                if len(imdata.shape) == 3 :\n",
    "                    imdata = np.mean(imdata[:,:,0:3], 2)\n",
    "                if imdata.shape != self.fsh :\n",
    "                    raise Exception(f\"Dimensions of the input image \\\"{imageName}\\\" {imdata.shape} \"\n",
    "                                    f\"do not match the face of the container \\\"{sampleName}\\\" {self.fsh}.\")\n",
    "                return imdata\n",
    "\n",
    "            self.mask = loadImage(maskName)\n",
    "            if self.mask is None :\n",
    "                self.mask = np.ones(self.fsh, dtype=np.uint8)\n",
    "            self.mask = self.mask.astype(bool)\n",
    "            self.bg = loadImage(bgName)\n",
    "            self.df = loadImage(dfName)\n",
    "            if self.bg is not None :\n",
    "                if self.df is not None:\n",
    "                    self.bg -= self.df\n",
    "                self.mask  &=  self.bg > 0.0\n",
    "\n",
    "            self.allIndices = []\n",
    "            for yCr in range(0,self.fsh[0]) :\n",
    "                for xCr in range(0,self.fsh[1]) :\n",
    "                    idx = np.s_[yCr,xCr]\n",
    "                    if self.mask[idx] :\n",
    "                        if self.volume is not None :\n",
    "                            if self.df is not None :\n",
    "                                self.volume[:,*idx] -= self.df[idx]\n",
    "                            if self.bg is not None :\n",
    "                                self.volume[:,*idx] /= self.bg[idx]\n",
    "                        if  xCr + DCfg.readSh[1] < self.fsh[1] \\\n",
    "                        and np.all( self.mask[yCr,xCr+1:xCr+DCfg.readSh[1]] ) :\n",
    "                            self.allIndices.append(idx)\n",
    "\n",
    "    def get_dataset(self, transform=None) :\n",
    "\n",
    "        class Sinos(torch.utils.data.Dataset) :\n",
    "\n",
    "            def __init__(self, root, transform=None):\n",
    "                self.container = root\n",
    "                self.transform = transforms.Compose([transforms.ToTensor(), transform]) \\\n",
    "                    if transform else transforms.ToTensor()\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.container.allIndices)\n",
    "\n",
    "            def __getitem__(self, index, idxs=None):\n",
    "                idx = self.container.allIndices[index]\n",
    "                if idxs is None :\n",
    "                    idxs = random.randint(0,self.container.sh[0]-DCfg.readSh[0]-1)\n",
    "                xyrng=np.s_[ idx[0], idx[1]:idx[1]+DCfg.readSh[1] ]\n",
    "                if self.container.volume is not None :\n",
    "                    data = self.container.volume[idxs:idxs+DCfg.readSh[0], *xyrng]\n",
    "                else :\n",
    "                    data = self.container.data[idxs:idxs+DCfg.readSh[0], *xyrng]\n",
    "                    if self.container.df is not None :\n",
    "                        data -= self.container.df[None,*xyrng]\n",
    "                    if self.container.bg is not None :\n",
    "                        data /= self.container.bg[None,*xyrng]\n",
    "                if self.transform :\n",
    "                    data = self.transform(data)\n",
    "                return data\n",
    "\n",
    "        return Sinos(self, transform)\n",
    "\n",
    "#sinoRoot = StripesFromHDF(\"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/SAMPLE_Y0_BG.hdf:/data\",\n",
    "#                          \"/mnt/ssdData/4176862R_Eig_Threshold-4keV/output/maskc.tif\",\n",
    "#                          None, None) # 229247,541 @ gap2\n",
    "\n",
    "sinoRoot = StripesFromHDF(\"/mnt/hddData/Linda_18515/output/Lamb1_Eiger_7m_45keV_360Scan/for_sinogap.hdf:/data\",\n",
    "                          \"/mnt/hddData/Linda_18515/output/Lamb1_Eiger_7m_45keV_360Scan/for_sinogap_mask.tif\",\n",
    "                          None, None )  # 34008,1620 @ gap2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Transform</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTransform =  transforms.Compose([\n",
    "    transforms.Resize(DCfg.sinoSh),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=(0.5), std=(1))\n",
    "])\n",
    "trainSet = sinoRoot.get_dataset(dataTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Show</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.048245951533317566 0.09211321175098419 -1.9092422723770142 -0.2303464114665985 0.10024410486221313\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "refImages = None\n",
    "refNoises = None\n",
    "\n",
    "def showMe() :\n",
    "    global refImages, refNoises\n",
    "    image = None\n",
    "    while True:\n",
    "        image = trainSet.__getitem__(34008,1620).squeeze()\n",
    "        #image = trainSet[random.randint(0,len(trainSet)-1)].squeeze()\n",
    "        if image.mean() > 0  and image.std() > 0.1 :\n",
    "            break\n",
    "        break\n",
    "    tensorStat(image)\n",
    "    image = image.to(TCfg.device)\n",
    "    plotImage(image.cpu())\n",
    "    refImages = torch.stack((image,image)).to(TCfg.device)\n",
    "    refNoises = torch.randn((2,TCfg.latentDim)).to(TCfg.device)\n",
    "\n",
    "showMe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## <font style=\"color:lightblue\">Models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">etc</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillWheights(seq) :\n",
    "    for wh in seq :\n",
    "        if hasattr(wh, 'weight') :\n",
    "            torch.nn.init.xavier_uniform_(wh.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Generator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3726559/3525890942.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Generator                                [2, 10, 2]                --\n",
      "├─Sequential: 1-1                        [2, 7, 10, 10]            --\n",
      "│    └─Linear: 2-1                       [2, 700]                  45,500\n",
      "│    └─ReLU: 2-2                         [2, 700]                  --\n",
      "│    └─Unflatten: 2-3                    [2, 7, 10, 10]            --\n",
      "├─Sequential: 1-2                        [2, 1, 10, 2]             --\n",
      "│    └─Sequential: 2-4                   [2, 64, 4, 4]             --\n",
      "│    │    └─Conv2d: 3-1                  [2, 64, 8, 8]             4,672\n",
      "│    │    └─LeakyReLU: 3-2               [2, 64, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-3                  [2, 64, 6, 6]             36,928\n",
      "│    │    └─LeakyReLU: 3-4               [2, 64, 6, 6]             --\n",
      "│    │    └─Conv2d: 3-5                  [2, 64, 4, 4]             36,928\n",
      "│    │    └─LeakyReLU: 3-6               [2, 64, 4, 4]             --\n",
      "│    └─Sequential: 2-5                   [2, 64, 4, 4]             --\n",
      "│    │    └─Flatten: 3-7                 [2, 1024]                 --\n",
      "│    │    └─Linear: 3-8                  [2, 1024]                 1,049,600\n",
      "│    │    └─LeakyReLU: 3-9               [2, 1024]                 --\n",
      "│    │    └─Unflatten: 3-10              [2, 64, 4, 4]             --\n",
      "│    └─Sequential: 2-6                   [2, 1, 10, 2]             --\n",
      "│    │    └─ConvTranspose2d: 3-11        [2, 64, 6, 4]             12,352\n",
      "│    │    └─LeakyReLU: 3-12              [2, 64, 6, 4]             --\n",
      "│    │    └─ConvTranspose2d: 3-13        [2, 64, 8, 4]             12,352\n",
      "│    │    └─LeakyReLU: 3-14              [2, 64, 8, 4]             --\n",
      "│    │    └─ConvTranspose2d: 3-15        [2, 64, 10, 4]            12,352\n",
      "│    │    └─LeakyReLU: 3-16              [2, 64, 10, 4]            --\n",
      "│    │    └─Conv2d: 3-17                 [2, 1, 10, 2]             193\n",
      "│    │    └─Tanh: 3-18                   [2, 1, 10, 2]             --\n",
      "==========================================================================================\n",
      "Total params: 1,210,877\n",
      "Trainable params: 1,210,877\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 9.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 4.84\n",
      "Estimated Total Size (MB): 5.09\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        latentChannels = 7\n",
    "        self.noise2latent = nn.Sequential(\n",
    "            nn.Linear(TCfg.latentDim, DCfg.sinoSize*latentChannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten( 1, (latentChannels,) + DCfg.sinoSh )\n",
    "        )\n",
    "        fillWheights(self.noise2latent)\n",
    "\n",
    "        baseChannels = 64\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(latentChannels+1, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.encode)\n",
    "\n",
    "\n",
    "        self.link = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (baseChannels, 4, 4)),\n",
    "        )\n",
    "        fillWheights(self.link)\n",
    "\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(baseChannels, baseChannels, (3,1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, 1, (1,3)),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "        fillWheights(self.decode)\n",
    "\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            self.encode,\n",
    "            self.link,\n",
    "            self.decode\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        images, noises = input\n",
    "        images, orgDims = unsqeeze4dim(images)\n",
    "        latent = self.noise2latent(noises)\n",
    "        modelIn = torch.cat((images,latent),dim=1)\n",
    "        mIn = modelIn[:,0,*DCfg.gapRng]\n",
    "        mIn[()] = self.preProc(images[:,0,:,:])\n",
    "        patches = self.body(modelIn) - 0.5\n",
    "        mIn = mIn.unsqueeze(1)\n",
    "        patches = mIn + torch.where( patches < 0 , patches * (mIn+0.5) , patches )\n",
    "        return squeezeOrg(patches, orgDims)\n",
    "\n",
    "\n",
    "    def preProc(self, images) :\n",
    "        res = torch.zeros(images[...,*DCfg.gapRng].shape, device=images.device)\n",
    "        res[...,0] += 2*images[...,DCfg.gapRngX.start-1] + images[...,DCfg.gapRngX.stop]\n",
    "        res[...,1] += 2*images[...,DCfg.gapRngX.stop] + images[...,DCfg.gapRngX.start-1]\n",
    "        return res/3\n",
    "\n",
    "    def generatePatches(self, images, noises=None) :\n",
    "        if noises is None :\n",
    "            noises = torch.randn(images.shape[0], TCfg.latentDim).to(TCfg.device)\n",
    "        return self.forward((images,noises))\n",
    "\n",
    "\n",
    "    def fillImages(self, images, noises=None) :\n",
    "        images[...,*DCfg.gapRng] = self.generatePatches(images, noises)\n",
    "        return images\n",
    "\n",
    "\n",
    "    def generateImages(self, images, noises=None) :\n",
    "        clone = images.clone()\n",
    "        return self.fillImages(clone)\n",
    "\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "if True :\n",
    "    generator = load_model(generator, model_path=f\"model_{TCfg.exec}_gen.pt\" )\n",
    "else :\n",
    "    try : os.remove(TCfg.historyHDF)\n",
    "    except : pass\n",
    "generator.to(TCfg.device)\n",
    "model_summary = summary(generator, input_data=[ [refImages, refNoises] ] ).__str__()\n",
    "print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Discriminator</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Discriminator                            [2, 1]                    --\n",
      "├─Sequential: 1-1                        [2, 64, 4, 4]             --\n",
      "│    └─Conv2d: 2-1                       [2, 64, 8, 8]             640\n",
      "│    └─LeakyReLU: 2-2                    [2, 64, 8, 8]             --\n",
      "│    └─Conv2d: 2-3                       [2, 64, 6, 6]             36,928\n",
      "│    └─LeakyReLU: 2-4                    [2, 64, 6, 6]             --\n",
      "│    └─Conv2d: 2-5                       [2, 64, 4, 4]             36,928\n",
      "│    └─LeakyReLU: 2-6                    [2, 64, 4, 4]             --\n",
      "├─Sequential: 1-2                        [2, 1]                    --\n",
      "│    └─Flatten: 2-7                      [2, 1024]                 --\n",
      "│    └─Dropout: 2-8                      [2, 1024]                 --\n",
      "│    └─Linear: 2-9                       [2, 128]                  131,200\n",
      "│    └─Linear: 2-10                      [2, 1]                    129\n",
      "│    └─Sigmoid: 2-11                     [2, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 205,825\n",
      "Trainable params: 205,825\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 4.19\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 0.82\n",
      "Estimated Total Size (MB): 0.95\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3726559/3525890942.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        baseChannels = 64\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(baseChannels, baseChannels, 3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        fillWheights(self.body)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        fillWheights(self.head)\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        if images.dim() == 3:\n",
    "            images = images.unsqueeze(1)\n",
    "        convRes = self.body(images)\n",
    "        res = self.head(convRes)\n",
    "        return res\n",
    "\n",
    "discriminator = Discriminator()\n",
    "discriminator = load_model(discriminator, model_path=f\"model_{TCfg.exec}_dis.pt\")\n",
    "discriminator.to(TCfg.device)\n",
    "model_summary = summary(discriminator, input_data=refImages ).__str__()\n",
    "print(model_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4244197 0.4221485]\n",
      "0.0014207506319507957 0.039805907756090164 28.017518997192383 -0.048343025147914886 0.06465709209442139\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.eval()\n",
    "discriminator.eval()\n",
    "refImages[1,...,DCfg.gapRngX] = generator.preProc(refImages[1,...])\n",
    "mgens = generator.generateImages(refImages,refNoises)\n",
    "liks = discriminator(mgens)\n",
    "print(liks.detach().cpu().squeeze().numpy())\n",
    "tensorStat( mgens[1,*DCfg.disRng] - refImages[1,*DCfg.disRng] )\n",
    "plotImages((refImages[0,...].detach().cpu().squeeze(),\n",
    "            refImages[1,...].detach().cpu().squeeze()))\n",
    "plotImages((mgens[0,...].detach().cpu().squeeze(),\n",
    "            mgens[1,...].detach().cpu().squeeze()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Train</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE = nn.BCELoss(reduction='none')\n",
    "MSE = nn.MSELoss(reduction='none')\n",
    "\n",
    "\n",
    "def applyWeights(inp, weights):\n",
    "    inp = inp.squeeze()\n",
    "    sum = len(inp)\n",
    "    if not weights is None :\n",
    "        inp *= weights\n",
    "        sum = weights.sum()\n",
    "    return inp.sum()/sum\n",
    "\n",
    "def loss_Adv(y_true, y_pred, weights=None):\n",
    "    loss = BCE(y_pred, y_true)\n",
    "    return applyWeights(loss,weights)\n",
    "\n",
    "def loss_Gen(y_true, y_pred, p_true, p_pred, weights=None):\n",
    "    lossAdv = loss_Adv(y_true, y_pred, weights)\n",
    "    lossesDif = MSE(p_pred, p_true).mean(dim=(1,2)) / (0.5+p_true.mean(dim=(1,2)))\n",
    "    lossDif = applyWeights(lossesDif, weights)\n",
    "    return lossAdv + 2000 * lossDif, lossAdv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Optimizers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_G = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=TCfg.learningRateG,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=TCfg.learningRateD,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Train step</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class TrainInfo:\n",
    "    bestRealImage = None\n",
    "    bestRealPrediction = 0\n",
    "    bestRealWght = 0\n",
    "    worstRealImage = None\n",
    "    worstRealPrediction = 0\n",
    "    worstRealWght = 0\n",
    "    bestFakeImage = None\n",
    "    bestFakePrediction = 0\n",
    "    bestFakeWght = 0\n",
    "    worstFakeImage = None\n",
    "    worstFakePrediction = 0\n",
    "    worstFakeWght = 0\n",
    "    ratReal = 0.0\n",
    "    ratFake = 0.0\n",
    "    totalImages = 0\n",
    "    iterations = 0\n",
    "    disPerformed = 0\n",
    "    genPerformed = 0\n",
    "    gaLoss = None\n",
    "\n",
    "\n",
    "trainDis = True\n",
    "trainGen = True\n",
    "def train_step(images):\n",
    "    global trainDis, trainGen\n",
    "    TrainInfo.iterations += 1\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    nofIm = images.shape[0]\n",
    "    images = images.squeeze(1).to(TCfg.device)\n",
    "    means = images.mean(dim=(1,2)).squeeze() + 0.5\n",
    "    #stdds = images.std(dim=(1,2)).squeeze()\n",
    "    absWeights = means + 1\n",
    "    absWeights /= absWeights.sum()\n",
    "    weights = absWeights.to(TCfg.device)\n",
    "    #disWeights = stdds / means\n",
    "    #disWeights /= disWeights.sum()\n",
    "    #weights = absWeights + disWeights\n",
    "    #weights /= weights.sum()\n",
    "\n",
    "    fakeImages = generator.generateImages(images)\n",
    "    y_pred_real = None\n",
    "    y_pred_fake = None\n",
    "    ratReal = 0\n",
    "    ratFake = 0\n",
    "\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "    counter = 0\n",
    "    while trainDis:\n",
    "        counter += 1\n",
    "        TrainInfo.disPerformed += 1\n",
    "        optimizer_D.zero_grad()\n",
    "        y_pred_real = discriminator(images)\n",
    "        y_pred_fake = discriminator(fakeImages)\n",
    "        y_pred_both = torch.cat((y_pred_real, y_pred_fake), dim=0)\n",
    "        labels = torch.cat( (\n",
    "            torch.full((nofIm, 1),  TCfg.labelSmoothFac),\n",
    "            torch.zeros(nofIm, 1) ),\n",
    "            dim=0\n",
    "        ).to(TCfg.device)\n",
    "        D_loss = loss_Adv(labels, y_pred_both, torch.cat( (weights, weights) ) )\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        if ratReal > ratFake:\n",
    "            trainGen = True\n",
    "            break\n",
    "        elif counter > 1:\n",
    "            trainGen = ratFake < 0.1 or ratReal > 0.6\n",
    "            break\n",
    "        else:\n",
    "            images = images.detach()\n",
    "            fakeImages = fakeImages.detach()\n",
    "    if not trainDis :\n",
    "        with torch.no_grad():\n",
    "            y_pred_real = discriminator(images)\n",
    "            labels = torch.full((nofIm, 1),  TCfg.labelSmoothFac,\n",
    "                                dtype=torch.float, device=TCfg.device)\n",
    "            D_loss = loss_Adv(labels, y_pred_real, weights)\n",
    "            ratReal = torch.count_nonzero(y_pred_real > 0.5)/nofIm\n",
    "\n",
    "\n",
    "    discriminator.eval()\n",
    "    generator.train()\n",
    "    counter = 0\n",
    "    while trainGen :\n",
    "        counter += 1\n",
    "        TrainInfo.genPerformed += 1\n",
    "        optimizer_G.zero_grad()\n",
    "        fakeImages = generator.generateImages(images)\n",
    "        y_pred_fake = discriminator(fakeImages)\n",
    "        labels = torch.ones(nofIm, 1).to(TCfg.device)\n",
    "        G_loss, GA_loss = loss_Gen(labels, y_pred_fake,\n",
    "                                   images[...,DCfg.gapRngX], fakeImages[...,DCfg.gapRngX],\n",
    "                                   weights)\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "        if ratFake > 0.1 :\n",
    "            trainDis = True\n",
    "            break\n",
    "        elif counter > 1:\n",
    "            trainDis = ratReal < 0.1 or ratFake > 0.6\n",
    "            break\n",
    "        else :\n",
    "            images = images.detach()\n",
    "            fakeImages = fakeImages.detach()\n",
    "    if not trainGen :\n",
    "        with torch.no_grad():\n",
    "            labels = torch.ones(nofIm, 1, dtype=torch.float, device=TCfg.device)\n",
    "            G_loss, GA_loss = loss_Gen(labels, y_pred_fake,\n",
    "                                       images[...,DCfg.gapRngX], fakeImages[...,DCfg.gapRngX],\n",
    "                                       weights)\n",
    "            ratFake = torch.count_nonzero(y_pred_fake > 0.5)/nofIm\n",
    "\n",
    "    trainDis = trainGen = True\n",
    "\n",
    "\n",
    "    idx = y_pred_real.argmax()\n",
    "    TrainInfo.bestRealImage = images[idx,...]\n",
    "    TrainInfo.bestRealPrediction = y_pred_real[idx].item()\n",
    "    TrainInfo.bestRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_real.argmin()\n",
    "    TrainInfo.worstRealImage = images[idx,...]\n",
    "    TrainInfo.worstRealPrediction =  y_pred_real[idx].item()\n",
    "    TrainInfo.worstRealWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmax()\n",
    "    TrainInfo.bestFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.bestFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.bestFakeWght = nofIm*weights[idx]\n",
    "    idx = y_pred_fake.argmin()\n",
    "    TrainInfo.worstFakeImage = fakeImages[idx,...]\n",
    "    TrainInfo.worstFakePrediction = y_pred_fake[idx].item()\n",
    "    TrainInfo.worstFakeWght = nofIm*weights[idx]\n",
    "    TrainInfo.ratReal += ratReal * nofIm\n",
    "    TrainInfo.ratFake += ratFake * nofIm\n",
    "    TrainInfo.totalImages += nofIm\n",
    "    TrainInfo.gaLoss = GA_loss\n",
    "\n",
    "    return D_loss, G_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:lightblue\">Trainer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "G_LOSS = []\n",
    "D_LOSS = []\n",
    "\n",
    "def train(dataloader):\n",
    "\n",
    "    discriminator.to(TCfg.device)\n",
    "    generator.to(TCfg.device)\n",
    "    refImages.to(TCfg.device)\n",
    "    refNoises.to(TCfg.device)\n",
    "    lastUpdateTime = time.time()\n",
    "\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        epoch += 1\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        start = time.time()\n",
    "        D_loss_list, G_loss_list = [], []\n",
    "\n",
    "        for it , images in tqdm.tqdm(enumerate(dataloader), total=int(len(dataloader))):\n",
    "            images = images.to(TCfg.device)\n",
    "            D_loss, G_loss = train_step(images)\n",
    "            D_loss_list.append(D_loss)\n",
    "            G_loss_list.append(G_loss)\n",
    "\n",
    "            #if not it or it > len(dataloader)-2 or time.time() - lastUpdateTime > 60 :\n",
    "            if time.time() - lastUpdateTime > 60 :\n",
    "                lastUpdateTime = time.time()\n",
    "                IPython.display.clear_output(wait=True)\n",
    "                print(f\"Epoch: {epoch}.\"\n",
    "                      f\" Dis loss: {D_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratReal/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.disPerformed/TrainInfo.iterations:.3f}), \"\n",
    "                      f\" Gen loss: {G_loss:.3f} \"\n",
    "                      f\"({TrainInfo.ratFake/TrainInfo.totalImages:.3f} / \"\n",
    "                      f\"{TrainInfo.genPerformed/TrainInfo.iterations:.3f}) \"\n",
    "                      f\" GA loss: {TrainInfo.gaLoss:.3f}\"\n",
    "                      )\n",
    "\n",
    "                TrainInfo.iterations = 0\n",
    "                TrainInfo.totalImages = 0\n",
    "                TrainInfo.ratReal = 0\n",
    "                TrainInfo.ratFake = 0\n",
    "                TrainInfo.genPerformed = 0\n",
    "                TrainInfo.disPerformed = 0\n",
    "                fourImages = np.ones( (2*DCfg.sinoSh[1] + DCfg.gapW ,\n",
    "                                       4*DCfg.sinoSh[0] + 3*DCfg.gapW), dtype=np.float32  )\n",
    "                def addImage(clmn, row, img) :\n",
    "                    fourImages[ row * ( DCfg.sinoSh[1]+DCfg.gapW) : (row+1) * DCfg.sinoSh[1] + row*DCfg.gapW ,\n",
    "                                clmn * ( DCfg.sinoSh[0]+DCfg.gapW) : (clmn+1) * DCfg.sinoSh[0] + clmn*DCfg.gapW ] = \\\n",
    "                        img.squeeze().detach().cpu().numpy()\n",
    "                refNoises[0,:] = torch.randn(TCfg.latentDim)\n",
    "                gens = generator.generateImages(refImages,refNoises)\n",
    "                dif = gens[0,...] - refImages[1,...]\n",
    "                tensorStat(dif[DCfg.disRng])\n",
    "                hGap = DCfg.gapW // 2\n",
    "                dif[:,hGap:hGap+DCfg.gapW] = (refImages[0,*DCfg.gapRng] - refImages[1,*DCfg.gapRng])\n",
    "                dif[:,-DCfg.gapW-hGap:-hGap] = (gens[0,*DCfg.gapRng] - refImages[0,*DCfg.gapRng])\n",
    "                if ( cof := max(-dif.min(),dif.max()) ) != 0 :\n",
    "                    dif /= 2*cof\n",
    "                disIn = torch.cat(( gens[0,...].unsqueeze(0), refImages), dim=0)\n",
    "                liks = discriminator(disIn)\n",
    "                addImage(0,0,TrainInfo.bestRealImage)\n",
    "                addImage(0,1,TrainInfo.worstRealImage)\n",
    "                addImage(1,0,TrainInfo.bestFakeImage)\n",
    "                addImage(1,1,TrainInfo.worstFakeImage)\n",
    "                addImage(2,0,gens[0,...])\n",
    "                addImage(2,1,refImages[0,...])\n",
    "                addImage(3,0,refImages[1,...])\n",
    "                addImage(3,1,dif)\n",
    "\n",
    "                print (f\"TT: {TrainInfo.bestRealPrediction:.4e} ({TrainInfo.bestRealWght:.3f}),  \"\n",
    "                       f\"FT: {TrainInfo.bestFakePrediction:.4e} ({TrainInfo.bestFakeWght:.3f}),  \"\n",
    "                       f\"GI: {liks[0].item():.5f}, {liks[2].item():.5f} \" )\n",
    "                print (f\"TF: {TrainInfo.worstRealPrediction:.4e} ({TrainInfo.worstRealWght:.3f}),  \"\n",
    "                       f\"FF: {TrainInfo.worstFakePrediction:.4e} ({TrainInfo.worstFakeWght:.3f}),  \"\n",
    "                       f\"RP: {liks[1].item():.5f}\" )\n",
    "                try :\n",
    "                    #tifffile.imwrite(f\"tmp_{TCfg.exec}.tif\", fourImages)\n",
    "                    addToHDF(TCfg.historyHDF, \"data\", fourImages)\n",
    "                except :\n",
    "                    eprint(\"Failed to save.\")\n",
    "                plt.imshow(fourImages, cmap='gray')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "        epoch_D_loss = sum(D_loss_list)/len(D_loss_list)\n",
    "        epoch_G_loss = sum(G_loss_list)/len(G_loss_list)\n",
    "        D_LOSS.append(epoch_D_loss.detach().cpu())\n",
    "        G_LOSS.append(epoch_G_loss.detach().cpu())\n",
    "        print('\\n')\n",
    "        print(f\"Time for epoch {epoch + 1} is {time.time()-start} sec\")\n",
    "        print(f\"Discriminator loss: {epoch_D_loss:.3f}, Generator loss: {epoch_G_loss:.3f}.\")\n",
    "        print('\\n')\n",
    "        save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "        save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Execute</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247. Dis loss: 0.673 (0.519 / 1.045),  Gen loss: 1.210 (0.282 / 1.045)  GA loss: 0.728\n",
      "0.010103954933583736 0.06327229738235474 6.262131690979004 -0.07819823175668716 0.08348363637924194\n",
      "TT: 1.0000e+00 (1.039),  FT: 9.9165e-01 (1.030),  GI: 0.57467, 0.18057 \n",
      "TF: 8.5995e-03 (0.974),  FF: 1.8193e-02 (0.974),  RP: 0.73958\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1491.2x1118.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:57<00:00,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time for epoch 248 is 63.32173538208008 sec\n",
      "Discriminator loss: 0.672, Generator loss: 1.219.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    dataset=trainSet,\n",
    "    batch_size=TCfg.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "trainGen=True\n",
    "trainDis=True\n",
    "#fillWheights(generator.body)\n",
    "#fillWheights(discriminator.body)\n",
    "#fillWheights(discriminator.head)\n",
    "train(trainLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:lightblue\">Post</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randIdx = random.randint(0,len(testSet)-1)\n",
    "image = testSet[randIdx]\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#image = image.to(TCfg.device)\n",
    "#fake_image = generate_images(image)\n",
    "#plt.imshow(fake_image.detach().squeeze(), cmap='gray')\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "addToHDF(f\"test_{TCfg.exec}.hdf\", \"data\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(generator, TCfg.device, model_path=f\"model_{TCfg.exec}_gen.pt\")\n",
    "save_model(discriminator, TCfg.device, model_path=f\"model_{TCfg.exec}_dis.pt\")\n",
    "#save_model(generator, TCfg.device, model_path=\"saves/work_1.GEN.pt\")\n",
    "#save_model(discriminator, TCfg.device, model_path=\"saves/work_1.DIS.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
